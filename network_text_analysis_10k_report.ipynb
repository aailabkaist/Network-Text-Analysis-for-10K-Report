{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb13bd11",
   "metadata": {},
   "source": [
    "# <div align=center> Analyzing the Supply Chain Network </div>\n",
    "# <div align=center> Created from the Conventional Relation Extraction Model and Chat-GPT: </div>\n",
    "# <div align=center> Focusing On S&P 500 Companies </div>\n",
    "###  <div align=center> Jeong Yeo(faiiry9@kaist.ac.kr); Jaejun Kim(jaejun.kim305@gmail.com); Ilju Park(dlfwnqkr12@gmail.com); Il-chul Moon(icmoon@kaist.ac.kr)<br/> </div>\n",
    "본 코드는 ChatGPT와 SSAN 모델을 활용하여 S&P 500 기업들의 10K 리포트로부터 relation extraction한 데이터를 활용하여 supply chain network를 구축하는 과정 및 결과를 보여줍니다.\n",
    "### SSAN Model Relation Extraction 과정\n",
    "1) 문서내의 개체간의 종속성을 추출합니다.\n",
    "2) 종속성을 모델링하기 위해 각 층에 self-attention을 활용합니다.\n",
    "3) transformation module을 이용하여 attentive bias를 생성합니다.\n",
    "* Binary relation prediction:\n",
    "$$ P(r|e_s, e_o) = \\text{sigmoid}(e_s^T W_r e_o), \\text{ where } e_s \\in \\mathbb{R}^{d_e}, W_r \\in \\mathbb{R}^{d_e d_e} $$\n",
    "\n",
    "* Cross Entropy Loss:\n",
    "$$ L = \\sum_{<s,o>}\\sum_{r} \\text{CrossEntropy}(P(r|e_s, e_o), y_r(e_s, e_o))\\text{, where y = target label}$$\n",
    "\n",
    "#### SSAN Architecture\n",
    "<img src = 'https://raw.githubusercontent.com/BenfengXu/SSAN/master/SSAN.png' alt = 'SSAN' width = '800'/>\n",
    "\n",
    "Xu, B., Wang, Q., Lyu, Y., Zhu, Y., & Mao, Z. (2021). Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction. Proceedin gs of the AAAI Conference on Artificial Intelligence, 35(16), 14149–14157. doi:10.1609/aaai.v35i16.17665\n",
    "\n",
    "### GPT Relation Extraction 과정\n",
    "1) 문서 전체를 처리하여 문맥, 객체 및 관계를 이해합니다.\n",
    "2) 객체와 관계의 언어적 패턴을 식별합니다.\n",
    "3) 사전 학습된 지식을 바탕으로 관계를 추론합니다.\n",
    "\n",
    "# 0. Initial Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91580fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib_venn import venn3, venn2\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import seaborn as sns\n",
    "from community import community_louvain\n",
    "import dataframe_image as dfi\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062aa53",
   "metadata": {},
   "source": [
    "# 1. Dataload\n",
    "본 섹션에서는 SSAN과 GPT로 추출한 entity와 relation의 데이터프레임을 불러오고 관계 그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_DF(csv_name, company):\n",
    "    df = pd.read_csv('{}.csv'.format(csv_name))#.iloc[:,1:]\n",
    "    #print(df.isna().sum())\n",
    "\n",
    "    df = df.dropna(subset=['num_para'], how='any', axis=0)\n",
    "    #print(df.isna().sum())\n",
    "\n",
    "    gpt_rel = df[df['relations'].notnull()]\n",
    "    gpt_rel = gpt_rel.reset_index(drop=True)\n",
    "\n",
    "    #print('=======DATA LOAD DONE========')\n",
    "\n",
    "\n",
    "    # 한 문단 당 relations들 한 리스트로 만들기 i.e.['1. Company @ strategic alliance @ None', '2. Company @ Caterpillar Inc. @ strategic alliance', ..]\n",
    "    rel_li = []\n",
    "    rel_index_li = []\n",
    "    for idx, rel in enumerate(gpt_rel['relations']):\n",
    "        if '\\n' in rel:\n",
    "            rel_li.append(rel.split('\\n'))\n",
    "            #print(rel.split('\\n'))\n",
    "            for i in range(len(rel.split('\\n'))):\n",
    "                rel_index_li.append(idx)\n",
    "                #print(idx)\n",
    "            #print('='*20)\n",
    "        else:\n",
    "            if '1. (' in rel:\n",
    "                rel_li.append(rel)\n",
    "                #print(rel)\n",
    "                #print('='*20)\n",
    "                rel_index_li.append(idx)\n",
    "\n",
    "    #print('=======RELs to a LIST DONE========')\n",
    "\n",
    "\n",
    "    # 한 리스트로 만들기\n",
    "    import itertools\n",
    "\n",
    "    def from_iterable(iterables):\n",
    "        for it in iterables:\n",
    "            for element in it:\n",
    "                yield element\n",
    "\n",
    "    rel_li = list(itertools.chain(*rel_li)) \n",
    "\n",
    "    #print('=======MAKING ONE REL LIST DONE========')\n",
    "\n",
    "    # 넘버링 떼기\n",
    "    rels_li = []\n",
    "    idx_li = []\n",
    "    for idx, rel in enumerate(rel_li):\n",
    "        if rel[1:3]== '. ':\n",
    "            if rel[3]=='(':\n",
    "                rels_li.append(rel[4:-1])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "            else:\n",
    "                rels_li.append(rel[3:])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "        elif rel[2:4]== '. ':\n",
    "            if rel[4]=='(':\n",
    "                rels_li.append(rel[5:-1])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "            else:\n",
    "                rels_li.append(rel[4:])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "    print('len of idx and rel:', len(idx_li), len(rels_li))\n",
    "    #print('=======REMOVING NUMBERING DONE========')\n",
    "\n",
    "    # @ 기준으로 h,t,r로 나누기\n",
    "    r_li = []\n",
    "    para_li = []\n",
    "    for rel, idx in zip(rels_li,idx_li):\n",
    "        if len(rel.split(' @ '))==3:\n",
    "            if 'entity' not in rel.split(' @ ')[0] and 'entity' not in rel.split(' @ ')[1]:\n",
    "                r_li.append(rel.split(' @ '))\n",
    "                para_li.append(gpt_rel['para'].iloc[idx][:-1])\n",
    "        # else:\n",
    "            # print(rel.split(' @ '))\n",
    "    #print('=======DEVIDE h,t,r DONE========')\n",
    "\n",
    "\n",
    "\n",
    "    #================================================\n",
    "\n",
    "    # df로 만들기\n",
    "    kg_df = pd.DataFrame(r_li, columns = ['source','target','edge'])\n",
    "    kg_df['para'] = para_li\n",
    "    kg_df = kg_df.drop_duplicates()#keep='last')\n",
    "\n",
    "    #print('=======MAKING DF DONE========')\n",
    "\n",
    "    # edge = none인 행 제거\n",
    "    nem_df = kg_df.dropna(subset=['edge'], how='any', axis=0)\n",
    "    # print(nem_df.isna().sum())\n",
    "\n",
    "    none_row = nem_df[nem_df['source']=='None'].index\n",
    "    NEM_df = nem_df.drop(none_row, inplace=False)\n",
    "    nem_df  = NEM_df.reset_index(drop=True)\n",
    "\n",
    "    none_row = nem_df[nem_df['target']=='None'].index\n",
    "    NEM_df = nem_df.drop(none_row, inplace=False)\n",
    "    nem_df  = NEM_df.reset_index(drop=True)\n",
    "\n",
    "    none_row = nem_df[nem_df['edge']=='None'].index\n",
    "    NEM_df = nem_df.drop(none_row, inplace=False)\n",
    "    NEM_df  = NEM_df.reset_index(drop=True)\n",
    "\n",
    "    #print('=======REMOVING ROW: None in h,t,r DONE========')\n",
    "\n",
    "    # # entity라고 나온거 제거\n",
    "    # given_rel = [\"head of government\", \"country\", \"place of birth\", \"place of death\", \"father\", \"mother\", \"spouse\", \"country of citizenship\", \"continent\", \"instance of\", \"head of state\", \"capital\", \"official language\", \"position held\", \"child\", \"author\", \"member of sports team\", \"director\", \"screenwriter\", \"educated at\", \"composer\", \"member of political party\", \"employer\", \"founded by\", \"league\", \"publisher\", \"owned by\", \"located in the administrative territorial entity\", \"genre\", \"operator\", \"religion\", \"contains administrative territorial entity\", \"follows\", \"followed by\", \"headquarters location\", \"cast member\", \"producer\", \"award received\", \"creator\", \"parent taxon\", \"ethnic group\", \"performer\", \"manufacturer\", \"developer\", \"series\", \"sister city\", \"legislative body\", \"basin country\", \"located in or next to body of water\", \"military branch\", \"record label\", \"production company\", \"location\", \"subclass of\", \"subsidiary\", \"part of\", \"original language of work\", \"platform\", \"mouth of the watercourse\", \"original network\", \"member of\", \"chairperson\", \"country of origin\", \"has part\", \"residence\", \"date of birth\", \"date of death\", \"inception\", \"dissolved, abolished or demolished\", \"publication date\", \"start time\", \"end time\", \"point in time\", \"conflict\", \"characters\", \"lyrics by\", \"located on terrain feature\", \"participant\", \"influenced by\", \"location of formation\", \"parent organization\", \"notable work\", \"separated from\", \"narrative location\", \"work location\", \"applies to jurisdiction\", \"product or material produced\", \"unemployment rate\", \"territory claimed by\", \"participant of\", \"replaces\", \"replaced by\", \"capital of\", \"languages spoken, written or signed\", \"present in work\", \"sibling\"]\n",
    "    # given_rel_row = []\n",
    "\n",
    "    # for i in range(len(NEM_df)):\n",
    "    #     if any(g_rel == NEM_df['edge'][i] for g_rel in given_rel):\n",
    "    #         given_rel_row.append([NEM_df['source'][i],NEM_df['target'][i],NEM_df['edge'][i]])\n",
    "    # #print('=======REMOVING ROW: None in h,t,r DONE========')\n",
    "\n",
    "    # given rel에 존재하는 데이터만 저장\n",
    "    given_rel = [\"head of government\", \"country\", \"place of birth\", \"place of death\", \"father\", \"mother\", \"spouse\", \"country of citizenship\", \"continent\", \"instance of\", \"head of state\", \"capital\", \"official language\", \"position held\", \"child\", \"author\", \"member of sports team\", \"director\", \"screenwriter\", \"educated at\", \"composer\", \"member of political party\", \"employer\", \"founded by\", \"league\", \"publisher\", \"owned by\", \"located in the administrative territorial entity\", \"genre\", \"operator\", \"religion\", \"contains administrative territorial entity\", \"follows\", \"followed by\", \"headquarters location\", \"cast member\", \"producer\", \"award received\", \"creator\", \"parent taxon\", \"ethnic group\", \"performer\", \"manufacturer\", \"developer\", \"series\", \"sister city\", \"legislative body\", \"basin country\", \"located in or next to body of water\", \"military branch\", \"record label\", \"production company\", \"location\", \"subclass of\", \"subsidiary\", \"part of\", \"original language of work\", \"platform\", \"mouth of the watercourse\", \"original network\", \"member of\", \"chairperson\", \"country of origin\", \"has part\", \"residence\", \"date of birth\", \"date of death\", \"inception\", \"dissolved, abolished or demolished\", \"publication date\", \"start time\", \"end time\", \"point in time\", \"conflict\", \"characters\", \"lyrics by\", \"located on terrain feature\", \"participant\", \"influenced by\", \"location of formation\", \"parent organization\", \"notable work\", \"separated from\", \"narrative location\", \"work location\", \"applies to jurisdiction\", \"product or material produced\", \"unemployment rate\", \"territory claimed by\", \"participant of\", \"replaces\", \"replaced by\", \"capital of\", \"languages spoken, written or signed\", \"present in work\", \"sibling\"]\n",
    "    given_rel_row = []\n",
    "\n",
    "    for i in range(len(NEM_df)):\n",
    "        if any(g_rel == NEM_df['edge'][i] for g_rel in given_rel):\n",
    "            given_rel_row.append([NEM_df['para'][i],NEM_df['source'][i],NEM_df['target'][i],NEM_df['edge'][i]])\n",
    "\n",
    "    final_DF2 = pd.DataFrame(given_rel_row, columns = ['para','source','target','relation'])\n",
    "\n",
    "    final_DF = final_DF2[['source','target','relation']]\n",
    "\n",
    "\n",
    "    #print('=======EXTRACTING JUST GIVEN REL DONE========')\n",
    "\n",
    "    # 저장 \n",
    "    final_DF.to_csv('./{}_DF1.csv'.format(company))\n",
    "    #print('=======SAVE AS DF DONE========')\n",
    "    final_DF2.to_excel(excel_writer='./{}.xlsx'.format(company)) \n",
    "    #dfi.export(final_DF, './RESULT/{}_RE_By_{}.png'.format(company, 'GPT'), max_cols=-1, max_rows=-1)\n",
    "\n",
    "    return final_DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P500 기업들의 이름과 티커를 불러옵니다.\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "table = pd.read_html(url)\n",
    "sp500_df = table[0]\n",
    "\n",
    "# 티커와 회사명을 딕셔너리로 매핑\n",
    "ticker_to_name = pd.Series(sp500_df.Security.values, index=sp500_df.Symbol).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: first 10 companies\n",
    "for ticker, name in list(ticker_to_name.items())[:10]:\n",
    "    print(f'{ticker}: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 entity와 관계의 데이터프레임을 불러옵니다.\n",
    "gpt_tag = pd.read_csv('GPT_SP_DF_TAG.csv')\n",
    "gpt_tag.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575e5fd",
   "metadata": {},
   "source": [
    "* para = pragraph\n",
    "* source = entity\n",
    "* target = entity\n",
    "* relation = relation\n",
    "* source_ner, target_ner = ner\n",
    "* sorted_pair\n",
    "* sorted_pair = entity pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d22f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of relations\n",
    "len(gpt_tag['relation'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe68e63",
   "metadata": {},
   "source": [
    "총 96개의 관계가 있습니다.\n",
    "### 각 관계 수 별 bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of each relations\n",
    "edge_counts = gpt_tag['relation'].value_counts()\n",
    "\n",
    "# bar chart of relation type counts\n",
    "plt.figure(figsize=(20, 18))  # Adjust the size as needed\n",
    "plt.barh(edge_counts.index, edge_counts.values, color='slategray') \n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Relation Type')\n",
    "plt.title('Relation Type Counts in Descending Order')\n",
    "plt.gca().invert_yaxis()  # To display the highest count at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 불러옵니다.\n",
    "gpt_just = pd.read_csv('GPT_Just_S_T.csv')\n",
    "gpt_just.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b4417",
   "metadata": {},
   "source": [
    "* para = paragraph\n",
    "* source = entity\n",
    "* target = entity\n",
    "* relation = relation\n",
    "* source_ner, target_ner = ner\n",
    "* company = company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Adding edges from the DataFrame\n",
    "for _, row in gpt_just.iterrows():\n",
    "    G.add_edge(row['source'], row['target'], edge=row['relation'])\n",
    "    \n",
    "    # Calculate centrality measures (e.g., degree centrality)\n",
    "centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Preparing to categorize nodes by their NER tags\n",
    "node_categories = defaultdict(list)\n",
    "\n",
    "# source와 target을 딕셔너리로 변환\n",
    "source_dict = gpt_just.set_index('source')['source_ner'].to_dict()\n",
    "target_dict = gpt_just.set_index('target')['target_ner'].to_dict()\n",
    "\n",
    "for node in G.nodes:\n",
    "    # 딕셔너리를 사용하여 빠르게 노드 카테고리 찾기\n",
    "    if node in source_dict:\n",
    "        category = source_dict[node]\n",
    "    elif node in target_dict:\n",
    "        category = target_dict[node]\n",
    "    else:\n",
    "        category = 'Unknown'\n",
    "    \n",
    "    node_categories[category].append(node)\n",
    "    \n",
    "# # Sorting nodes in each category by centrality\n",
    "top_centrality_by_category = {category: sorted(nodes, key=lambda x: centrality[x], reverse=True)[:10]\n",
    "                              for category, nodes in node_categories.items()}\n",
    "\n",
    "# top_centrality_by_category\n",
    "# Converting the top_centrality_by_category dictionary to a DataFrame\n",
    "df_top_centrality = pd.DataFrame([{\"Category\": category, \"Node\": node, \"Centrality\": centrality[node]}\n",
    "                                  for category, nodes in top_centrality_by_category.items()\n",
    "                                  for node in nodes])\n",
    "\n",
    "df_top_centrality.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e72e0",
   "metadata": {},
   "source": [
    "### source_node별 subgraph\n",
    "* source_node = 'Tesla, Inc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the shortest path to all nodes reachable from the source_node\n",
    "# (node: length)\n",
    "source_node = 'Tesla, Inc.'\n",
    "reachable_nodes = nx.single_source_shortest_path_length(G, source_node)\n",
    "list(reachable_nodes.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1282f",
   "metadata": {},
   "source": [
    "source_node를 'Tesla, Inc.'로 설정했을때 첫 10개의 reachable_nodes입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(reachable_nodes.keys())))\n",
    "# first 100 reachable nodes\n",
    "first_100_reachable_nodes = dict(list(reachable_nodes.items())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422fc9b",
   "metadata": {},
   "source": [
    "'Tesla, Inc.'에는 190269개의 reachable nodes가 있고, 그래프를 그리기 위해 100개의 노드만 선택했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67965474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the subgraph of reachable nodes\n",
    "subgraph = G.subgraph(first_100_reachable_nodes.keys())\n",
    "\n",
    "# Use the Kamada-Kawai layout for a hierarchical structure\n",
    "pos = nx.kamada_kawai_layout(subgraph)\n",
    "\n",
    "# Draw the subgraph with Kamada-Kawai layout\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw(subgraph, pos, with_labels=True, node_size=20, font_size=8, alpha=0.7, arrows=True)\n",
    "plt.title(f\"Hierarchical View of Reachable Nodes from {source_node}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82705d",
   "metadata": {},
   "source": [
    "아래의 코드는 전체 reachable_nodes에 대한 그래프 입니다. 연산량이 많아 주석처리 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb53cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# heavy computation\n",
    "# source_node can be replaced with other nodes\n",
    "source_node = 'Tesla, Inc.'\n",
    "reachable_nodes = nx.single_source_shortest_path_length(G, source_node)\n",
    "\n",
    "# Extract the subgraph of reachable nodes\n",
    "subgraph = G.subgraph(reachable_nodes.keys())\n",
    "\n",
    "# Use the Kamada-Kawai layout for a hierarchical structure\n",
    "pos = nx.kamada_kawai_layout(subgraph)\n",
    "\n",
    "# Draw the subgraph with Kamada-Kawai layout\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw(subgraph, pos, with_labels=True, node_size=20, font_size=8, alpha=0.7, arrows=True)\n",
    "plt.title(f\"Hierarchical View of Reachable Nodes from {source_node}\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59216a",
   "metadata": {},
   "source": [
    "연산량이 많아 100개의 노드만 선택하여 그래프를 구축하였습니다.\n",
    "* source_node = 'lithium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff73073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with lithium\n",
    "source_node = 'lithium'\n",
    "reachable_nodes = nx.single_source_shortest_path_length(G, source_node)\n",
    "# first 100 reachable nodes\n",
    "first_100_reachable_nodes = dict(list(reachable_nodes.items())[:100])\n",
    "\n",
    "# Extract the subgraph of reachable nodes\n",
    "subgraph = G.subgraph(first_100_reachable_nodes.keys())\n",
    "\n",
    "# Use the Kamada-Kawai layout for a hierarchical structure\n",
    "pos = nx.kamada_kawai_layout(subgraph)\n",
    "\n",
    "# Draw the subgraph with Kamada-Kawai layout\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw(subgraph, pos, with_labels=True, node_size=20, font_size=8, alpha=0.7, arrows=True)\n",
    "plt.title(f\"Hierarchical View of Reachable Nodes from {source_node}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39eca0",
   "metadata": {},
   "source": [
    "#### source_node와 target_node별 최단 path를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0324c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소스노드와 타겟노드 사이의 최단 path를 보여줍니다.\n",
    "# find and display the shortest path from one specific node to another in a network\n",
    "source_node = 'lithium'\n",
    "# computes the shortest path to all reachable nodes\n",
    "shortest_paths = nx.single_source_shortest_path(G, source_node)\n",
    "\n",
    "# can be replaced with other target nodes\n",
    "target_node = 'Tesla, Inc.'\n",
    "# target_node is in the keys of the shortest_paths, \n",
    "if target_node in shortest_paths:\n",
    "    path = shortest_paths[target_node]\n",
    "    print(f\"The shortest path from {source_node} to {target_node} is:\")\n",
    "    print(path)\n",
    "else:\n",
    "    print(f\"There is no path from {source_node} to {target_node}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the target_node is replaced from Tesla, Inc. to Tesla\n",
    "target_node = 'Tesla'\n",
    "if target_node in shortest_paths:\n",
    "    path = shortest_paths[target_node]\n",
    "    print(f\"The shortest path from {source_node} to {target_node} is:\")\n",
    "    print(path)\n",
    "else:\n",
    "    print(f\"There is no path from {source_node} to {target_node}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da286c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undirected graph\n",
    "bi_G = nx.Graph()\n",
    "\n",
    "for _, row in gpt_just.iterrows():\n",
    "    bi_G.add_edge(row['source'], row['target'], edge=row['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_node = 'lithium'\n",
    "# compute the shortest path length from the source node to other nodes\n",
    "reachable_nodes = nx.single_source_shortest_path_length(bi_G, source_node)\n",
    "# shortest path key = target nodes, valeus = shortest path from the source node to target nodes\n",
    "shortest_paths = nx.single_source_shortest_path(bi_G, source_node)\n",
    "\n",
    "# target_node can be replaced with other nodes\n",
    "target_node = 'Tesla'\n",
    "# if target_node is present in shortest_paths\n",
    "if target_node in shortest_paths:\n",
    "    # retrieves the shortest path from source_node to target_node\n",
    "    path = shortest_paths[target_node]\n",
    "    print(f\"The shortest path from {source_node} to {target_node} is:\")\n",
    "    print(path)\n",
    "else:\n",
    "    print(f\"There is no path from {source_node} to {target_node}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = 'Tesla, Inc.'  # 'target'을 원하는 목표 노드의 실제 이름으로 바꾸세요.\n",
    "if target_node in shortest_paths:\n",
    "    path = shortest_paths[target_node]\n",
    "    print(f\"The shortest path from {source_node} to {target_node} is:\")\n",
    "    print(path)\n",
    "else:\n",
    "    print(f\"There is no path from {source_node} to {target_node}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0429c03",
   "metadata": {},
   "source": [
    "Undirected Graph를 통해 시각화합니다.\n",
    "* source_node = 'lithium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6568d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_node = 'lithium'\n",
    "# compute the shortest path length from the source node to other nodes\n",
    "reachable_nodes = nx.single_source_shortest_path_length(bi_G, source_node)\n",
    "print(len(list(reachable_nodes.keys())))\n",
    "# first 100 reachable nodes\n",
    "first_100_reachable_nodes = dict(list(reachable_nodes.items())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744cdb2",
   "metadata": {},
   "source": [
    "총 268602개의 reachable_nodes가 있고 연산량이 많아 100개의 노드만을 선택하여 그래프를 구축하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec64a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  도달 가능한 노드만을 포함하는 서브그래프를 추출합니다.\n",
    "# replaced G with bi_G\n",
    "subgraph = bi_G.subgraph(first_100_reachable_nodes.keys())\n",
    "\n",
    "# Kamada-Kawai 레이아웃을 사용하여 서브그래프의 노드 위치를 계산합니다.\n",
    "pos = nx.kamada_kawai_layout(subgraph)\n",
    "\n",
    "# 서브그래프를 시각화합니다.\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw(subgraph, pos, with_labels=True, node_size=50, font_size=8, alpha=0.7, arrows=True)\n",
    "plt.title(f\"Graph Visualization from '{source_node}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee2d84",
   "metadata": {},
   "source": [
    "아래의 코드는 전체 reachable_nodes에 대한 그래프 입니다. 연산량이 많아 주석처리 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9667d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# heavy computation\n",
    "#  도달 가능한 노드만을 포함하는 서브그래프를 추출합니다.\n",
    "subgraph = G.subgraph(reachable_nodes.keys())\n",
    "\n",
    "# Kamada-Kawai 레이아웃을 사용하여 서브그래프의 노드 위치를 계산합니다.\n",
    "pos = nx.kamada_kawai_layout(subgraph)\n",
    "\n",
    "# 서브그래프를 시각화합니다.\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw(subgraph, pos, with_labels=True, node_size=50, font_size=8, alpha=0.7, arrows=True)\n",
    "plt.title(f\"Graph Visualization from '{source_node}'\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496be48",
   "metadata": {},
   "source": [
    "# 2. Scoring\n",
    "본 섹션에서는 정의된 함수로 데이터프레임을 전처리하고 성능지표를 계산합니다.\n",
    "## preprocessing Dataframes with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35272be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CC(csv_name, loading_file_name):\n",
    "    file_name = loading_file_name\n",
    "\n",
    "    df = pd.read_csv('{}.csv'.format(csv_name))#.iloc[:,1:]\n",
    "    #print(df.isna().sum())\n",
    "\n",
    "    df = df.dropna(subset=['num_para'], how='any', axis=0)\n",
    "    #print(df.isna().sum())\n",
    "\n",
    "    gpt_rel = df[df['relations'].notnull()]\n",
    "    gpt_rel = gpt_rel.reset_index(drop=True)\n",
    "        \n",
    "    # 각 문단당 relations, para, num_para를 딕셔너리로 저장\n",
    "    data_list = []\n",
    "    given_rel = [\"head of government\", \"country\", \"place of birth\", \"place of death\", \"father\", \"mother\", \"spouse\", \"country of citizenship\", \"continent\", \"instance of\", \"head of state\", \"capital\", \"official language\", \"position held\", \"child\", \"author\", \"member of sports team\", \"director\", \"screenwriter\", \"educated at\", \"composer\", \"member of political party\", \"employer\", \"founded by\", \"league\", \"publisher\", \"owned by\", \"located in the administrative territorial entity\", \"genre\", \"operator\", \"religion\", \"contains administrative territorial entity\", \"follows\", \"followed by\", \"headquarters location\", \"cast member\", \"producer\", \"award received\", \"creator\", \"parent taxon\", \"ethnic group\", \"performer\", \"manufacturer\", \"developer\", \"series\", \"sister city\", \"legislative body\", \"basin country\", \"located in or next to body of water\", \"military branch\", \"record label\", \"production company\", \"location\", \"subclass of\", \"subsidiary\", \"part of\", \"original language of work\", \"platform\", \"mouth of the watercourse\", \"original network\", \"member of\", \"chairperson\", \"country of origin\", \"has part\", \"residence\", \"date of birth\", \"date of death\", \"inception\", \"dissolved, abolished or demolished\", \"publication date\", \"start time\", \"end time\", \"point in time\", \"conflict\", \"characters\", \"lyrics by\", \"located on terrain feature\", \"participant\", \"influenced by\", \"location of formation\", \"parent organization\", \"notable work\", \"separated from\", \"narrative location\", \"work location\", \"applies to jurisdiction\", \"product or material produced\", \"unemployment rate\", \"territory claimed by\", \"participant of\", \"replaces\", \"replaced by\", \"capital of\", \"languages spoken, written or signed\", \"present in work\", \"sibling\"]\n",
    "\n",
    "    for index, row in gpt_rel.iterrows():\n",
    "        relations = row['relations'].split('\\n')\n",
    "        relations = [relation.strip() for relation in relations if relation.strip()]\n",
    "        para = row['para'][:-1]\n",
    "        num_para = row['num_para']\n",
    "        \n",
    "        for relation in relations:\n",
    "            if (\"None\" not in relation) and (\"none\" not in relation):\n",
    "                # 넘버링을 제거하고 source, target, relation 분리\n",
    "                parts = relation.split('@')\n",
    "                if len(parts) == 3:\n",
    "                    source = parts[0].split(' ')[1].strip()\n",
    "                    target = parts[1].strip()\n",
    "                    relation = parts[2].strip()\n",
    "\n",
    "                    if relation in given_rel:\n",
    "                        # source = source.replace(\"we\", real_com_name).replace(\"We\", real_com_name)\\\n",
    "                        #             .replace(\"Company\", real_com_name).replace(\"company\", real_com_name)\n",
    "                        # target = target.replace(\"we\", real_com_name).replace(\"We\", real_com_name)\\\n",
    "                        #             .replace(\"Company\", real_com_name).replace(\"company\", real_com_name)\n",
    "\n",
    "                        relation_dict = {\n",
    "                            'num_para': num_para,\n",
    "                            'para': para,\n",
    "                            'source': source,\n",
    "                            'target': target,\n",
    "                            'relation': relation\n",
    "                        }\n",
    "                        data_list.append(relation_dict)\n",
    "\n",
    "\n",
    "    # 위에서 생성한 data_list 사용\n",
    "    data_df = pd.DataFrame(data_list)\n",
    "    #data_df['company'] = csv_name[23:]\n",
    "    data_df = data_df.drop_duplicates(subset=['source', 'target', 'relation'])\n",
    "\n",
    "    DF = data_df\n",
    "\n",
    "    # 저장 \n",
    "    data_df.to_csv('.{}.csv'.format(file_name))\n",
    "\n",
    "    # 'source' 또는 'target' 열의 값이 'para' 열에 속하는 경우 'o' 할당\n",
    "\n",
    "    for i, row in data_df.iterrows():\n",
    "        if pd.notna(row['para']):\n",
    "            source_in_para = row['source'] in row['para']\n",
    "            target_in_para = row['target'] in row['para']\n",
    "            \n",
    "            if source_in_para and target_in_para:\n",
    "                data_df.loc[i, 'entity exist?'] = 'o'\n",
    "            else:\n",
    "                data_df.loc[i, 'entity exist?'] = 'x'\n",
    "\n",
    "    data_df.to_excel(excel_writer='{}.xlsx'.format(file_name), index=False)  \n",
    " \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy를 이용해 relation을 추출한 데이터를 불러옵니다.\n",
    "after_spacy = pd.read_csv('f1_after_spacy_gpt.csv')\n",
    "after_spacy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70340576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 불러옵니다.\n",
    "preprocessed = pd.read_excel('F1_a_s_preprocessed.xlsx')\n",
    "preprocessed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_SP_DF1 = CC('f1_after_spacy_gpt', 'F1_a_s_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4271f",
   "metadata": {},
   "source": [
    "CC 함수를 이용해 'f1_after_spacy_gpt' dataframe과 'F1_a_s_preprocessed' dataframe을 처리하여 GPT_SP_DF1에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_DF(csv_name, company):\n",
    "    df = pd.read_csv('{}.csv'.format(csv_name))#.iloc[:,1:]\n",
    "\n",
    "    df = df.dropna(subset=['num_para'], how='any', axis=0)\n",
    "\n",
    "    gpt_rel = df[df['relations'].notnull()]\n",
    "    gpt_rel = gpt_rel.reset_index(drop=True)\n",
    "\n",
    "    # 한 문단 당 relations들 한 리스트로 만들기 i.e.['1. Company @ strategic alliance @ None', '2. Company @ Caterpillar Inc. @ strategic alliance', ..]\n",
    "    rel_li = []\n",
    "    rel_index_li = []\n",
    "    for idx, rel in enumerate(gpt_rel['relations']):\n",
    "        if '\\n' in rel:\n",
    "            rel_li.append(rel.split('\\n'))\n",
    "            #print(rel.split('\\n'))\n",
    "            for i in range(len(rel.split('\\n'))):\n",
    "                rel_index_li.append(idx)\n",
    "                #print(idx)\n",
    "            #print('='*20)\n",
    "        else:\n",
    "            if '1. (' in rel:\n",
    "                rel_li.append(rel)\n",
    "                #print(rel)\n",
    "                #print('='*20)\n",
    "                rel_index_li.append(idx)\n",
    "\n",
    "    # 한 리스트로 만들기\n",
    "    import itertools\n",
    "\n",
    "    def from_iterable(iterables):\n",
    "        for it in iterables:\n",
    "            for element in it:\n",
    "                yield element\n",
    "\n",
    "    rel_li = list(itertools.chain(*rel_li)) \n",
    "\n",
    "    # 넘버링 떼기\n",
    "    rels_li = []\n",
    "    idx_li = []\n",
    "    for idx, rel in enumerate(rel_li):\n",
    "        if rel[1:3]== '. ':\n",
    "            if rel[3]=='(':\n",
    "                rels_li.append(rel[4:-1])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "            else:\n",
    "                rels_li.append(rel[3:])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "        elif rel[2:4]== '. ':\n",
    "            if rel[4]=='(':\n",
    "                rels_li.append(rel[5:-1])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "            else:\n",
    "                rels_li.append(rel[4:])\n",
    "                idx_li.append(rel_index_li[idx])\n",
    "    print('len of idx and rel:', len(idx_li), len(rels_li))\n",
    "\n",
    "    # @ 기준으로 h,t,r로 나누기\n",
    "    r_li = []\n",
    "    para_li = []\n",
    "    for rel, idx in zip(rels_li,idx_li):\n",
    "        if len(rel.split(' @ '))==3:\n",
    "            if 'entity' not in rel.split(' @ ')[0] and 'entity' not in rel.split(' @ ')[1]:\n",
    "                r_li.append(rel.split(' @ '))\n",
    "                para_li.append(gpt_rel['para'].iloc[idx][:-1])\n",
    "        # else:\n",
    "            # print(rel.split(' @ '))\n",
    "\n",
    "    # df로 만들기\n",
    "    kg_df = pd.DataFrame(r_li, columns = ['source','target','edge'])\n",
    "    kg_df['para'] = para_li\n",
    "    kg_df = kg_df.drop_duplicates()#keep='last')\n",
    "\n",
    "\n",
    "    # edge = none인 행 제거\n",
    "    nem_df = kg_df.dropna(subset=['edge'], how='any', axis=0)\n",
    "\n",
    "    none_row = nem_df[nem_df['source']=='None'].index\n",
    "    NEM_df = nem_df.drop(none_row, inplace=False)\n",
    "    nem_df  = NEM_df.reset_index(drop=True)\n",
    "\n",
    "    none_row = nem_df[nem_df['target']=='None'].index\n",
    "    NEM_df = nem_df.drop(none_row, inplace=False)\n",
    "    nem_df  = NEM_df.reset_index(drop=True)\n",
    "\n",
    "    none_row = nem_df[nem_df['edge']=='None'].index\n",
    "    NEM_df = nem_df.drop(none_row, inplace=False)\n",
    "    NEM_df  = NEM_df.reset_index(drop=True)\n",
    "\n",
    "    # given rel에 존재하는 데이터만 저장\n",
    "    given_rel = [\"head of government\", \"country\", \"place of birth\", \"place of death\", \"father\", \"mother\", \"spouse\", \"country of citizenship\", \"continent\", \"instance of\", \"head of state\", \"capital\", \"official language\", \"position held\", \"child\", \"author\", \"member of sports team\", \"director\", \"screenwriter\", \"educated at\", \"composer\", \"member of political party\", \"employer\", \"founded by\", \"league\", \"publisher\", \"owned by\", \"located in the administrative territorial entity\", \"genre\", \"operator\", \"religion\", \"contains administrative territorial entity\", \"follows\", \"followed by\", \"headquarters location\", \"cast member\", \"producer\", \"award received\", \"creator\", \"parent taxon\", \"ethnic group\", \"performer\", \"manufacturer\", \"developer\", \"series\", \"sister city\", \"legislative body\", \"basin country\", \"located in or next to body of water\", \"military branch\", \"record label\", \"production company\", \"location\", \"subclass of\", \"subsidiary\", \"part of\", \"original language of work\", \"platform\", \"mouth of the watercourse\", \"original network\", \"member of\", \"chairperson\", \"country of origin\", \"has part\", \"residence\", \"date of birth\", \"date of death\", \"inception\", \"dissolved, abolished or demolished\", \"publication date\", \"start time\", \"end time\", \"point in time\", \"conflict\", \"characters\", \"lyrics by\", \"located on terrain feature\", \"participant\", \"influenced by\", \"location of formation\", \"parent organization\", \"notable work\", \"separated from\", \"narrative location\", \"work location\", \"applies to jurisdiction\", \"product or material produced\", \"unemployment rate\", \"territory claimed by\", \"participant of\", \"replaces\", \"replaced by\", \"capital of\", \"languages spoken, written or signed\", \"present in work\", \"sibling\"]\n",
    "    given_rel_row = []\n",
    "\n",
    "    for i in range(len(NEM_df)):\n",
    "        if any(g_rel == NEM_df['edge'][i] for g_rel in given_rel):\n",
    "            given_rel_row.append([NEM_df['para'][i],NEM_df['source'][i],NEM_df['target'][i],NEM_df['edge'][i]])\n",
    "\n",
    "    final_DF2 = pd.DataFrame(given_rel_row, columns = ['para','source','target','relation'])\n",
    "\n",
    "    final_DF = final_DF2[['source','target','relation']]\n",
    "\n",
    "    # 저장 \n",
    "    final_DF.to_csv('./{}_DF1.csv'.format(company))\n",
    "    #print('=======SAVE AS DF DONE========')\n",
    "    final_DF2.to_excel(excel_writer='./{}.xlsx'.format(company)) \n",
    "    #dfi.export(final_DF, './RESULT/{}_RE_By_{}.png'.format(company, 'GPT'), max_cols=-1, max_rows=-1)\n",
    "\n",
    "    return final_DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"CSV 파일을 안전하게 읽어옵니다.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"파일이 존재하지 않습니다: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"파일 읽기 오류: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def extract_relations(gpt_rel):\n",
    "    \"\"\"'relations' 열에서 관계를 추출합니다.\"\"\"\n",
    "    rel_li, rel_index_li = [], []\n",
    "    for idx, rel in enumerate(gpt_rel['relations']):\n",
    "        # 'relations' 열의 형식 검증 및 정제\n",
    "        if not isinstance(rel, str):\n",
    "            continue\n",
    "        rel = rel.strip()\n",
    "        if '\\n' in rel:\n",
    "            parts = rel.split('\\n')\n",
    "        else:\n",
    "            parts = [rel]\n",
    "        for part in parts:\n",
    "            if '.' in part and '@' in part:\n",
    "                rel_li.append(part)\n",
    "                rel_index_li.append(idx)\n",
    "\n",
    "    # 숫자 제거 및 트리플렛 추출\n",
    "    triples = []\n",
    "    for idx, rel in enumerate(rel_li):\n",
    "        parts = rel.split('@')\n",
    "        if len(parts) == 3:\n",
    "            source, target, relation = [x.strip() for x in parts]\n",
    "            source = source[source.find('.') + 1:].strip()  # 숫자 제거\n",
    "            triples.append((source, target, relation, gpt_rel['para'].iloc[rel_index_li[idx]]))\n",
    "\n",
    "    return triples\n",
    "\n",
    "def create_dataframe(triples, given_rel):\n",
    "    \"\"\"트리플렛을 포함하는 새로운 DataFrame을 생성합니다.\"\"\"\n",
    "    df = pd.DataFrame(triples, columns=['source', 'target', 'relation', 'para'])\n",
    "    # 주어진 관계에 해당하는 데이터만 필터링\n",
    "    df = df[df['relation'].isin(given_rel)]\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def process(csv_name, company):\n",
    "    file_path = f'{csv_name}.csv'\n",
    "    df = load_data(file_path)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 필요한 열 선택\n",
    "    df = df.dropna(subset=['num_para'])\n",
    "    gpt_rel = df[df['relations'].notnull()].reset_index(drop=True)\n",
    "\n",
    "    # 관계 추출\n",
    "    triples = extract_relations(gpt_rel)\n",
    "\n",
    "    # 주어진 관계 목록\n",
    "    given_rel = [\"head of government\", \"country\", \"place of birth\", \"place of death\", \"father\", \"mother\", \"spouse\", \"country of citizenship\", \"continent\", \"instance of\", \"head of state\", \"capital\", \"official language\", \"position held\", \"child\", \"author\", \"member of sports team\", \"director\", \"screenwriter\", \"educated at\", \"composer\", \"member of political party\", \"employer\", \"founded by\", \"league\", \"publisher\", \"owned by\", \"located in the administrative territorial entity\", \"genre\", \"operator\", \"religion\", \"contains administrative territorial entity\", \"follows\", \"followed by\", \"headquarters location\", \"cast member\", \"producer\", \"award received\", \"creator\", \"parent taxon\", \"ethnic group\", \"performer\", \"manufacturer\", \"developer\", \"series\", \"sister city\", \"legislative body\", \"basin country\", \"located in or next to body of water\", \"military branch\", \"record label\", \"production company\", \"location\", \"subclass of\", \"subsidiary\", \"part of\", \"original language of work\", \"platform\", \"mouth of the watercourse\", \"original network\", \"member of\", \"chairperson\", \"country of origin\", \"has part\", \"residence\", \"date of birth\", \"date of death\", \"inception\", \"dissolved, abolished or demolished\", \"publication date\", \"start time\", \"end time\", \"point in time\", \"conflict\", \"characters\", \"lyrics by\", \"located on terrain feature\", \"participant\", \"influenced by\", \"location of formation\", \"parent organization\", \"notable work\", \"separated from\", \"narrative location\", \"work location\", \"applies to jurisdiction\", \"product or material produced\", \"unemployment rate\", \"territory claimed by\", \"participant of\", \"replaces\", \"replaced by\", \"capital of\", \"languages spoken, written or signed\", \"present in work\", \"sibling\"]\n",
    "\n",
    "    # 새로운 DataFrame 생성\n",
    "    final_df = create_dataframe(triples, given_rel)\n",
    "\n",
    "    # 결과 저장\n",
    "    # final_df.to_csv(f'./{company}_DF1.csv', index=False)\n",
    "    # final_df.to_excel(f'./{company}.xlsx', index=False)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# 사용 예시\n",
    "result_df2 = process('f1_after_spacy_gpt','F1_a_s_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2 = process('f1_after_spacy_gpt','F1_a_s_preprocessed')\n",
    "result_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cb9ae",
   "metadata": {},
   "source": [
    "process 함수를 이용하여 'f1_after_spacy_gpt' dataframe과 'F1_a_s_preprocessed' dataframe을 처리하여 result_df2에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf99d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "name_type_dict = {}\n",
    "\n",
    "with open('output_text_file.json', 'r', encoding='utf-8') as file:\n",
    "    inputs_json = json.load(file)\n",
    "\n",
    "for item in inputs_json:\n",
    "    if item['vertexSet']:  # vertexSet이 비어있지 않은 경우에만 작업을 수행\n",
    "        for vertex in item['vertexSet']:\n",
    "            for v in vertex:\n",
    "                name = v['name']\n",
    "                type_ = v['type']\n",
    "                name_type_dict[name] = type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(name_type_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed95e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임의 각 행에 대해 단어를 가져와서 NER 태그를 찾습니다.\n",
    "## source 의 단어가 name_type_dict에 있으면 ner 태크 가져오기\n",
    "\n",
    "# iterate through the 'source' column and retrieve the NER tag if the word is in name_type_dict and generate a lsit\n",
    "ner_tags = [name_type_dict.get(word, 'unknown') for word in result_df2['source']]\n",
    "\n",
    "# 이제 ner_tags 리스트는 데이터프레임의 행 수와 일치합니다.\n",
    "## source_ner_2에 태그 저장\n",
    "# add a new column source_ner_2 and store ner_tags\n",
    "result_df2['source_ner_2'] = ner_tags\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 단어를 가져와서 NER 태그를 찾습니다.\n",
    "## target과 name_type_dict가 겹치면 ner 태그 가져오기\n",
    "\n",
    "# iterate through the 'target' column and retrieve the NER tag if the word is in name_type_dict and generate a lsit\n",
    "ner_tags = [name_type_dict.get(word, 'unknown') for word in result_df2['target']]\n",
    "\n",
    "# 이제 ner_tags 리스트는 데이터프레임의 행 수와 일치합니다.\n",
    "## target_ner_2에 태그 저장\n",
    "# add a new column target_ner_2 and store ner_tags\n",
    "result_df2['target_ner_2'] = ner_tags\n",
    "\n",
    "# drop 'unknown' values\n",
    "result_df2 = result_df2[(result_df2['source_ner_2']!='unknown')&(result_df2['target_ner_2']!='unknown')]\n",
    "\n",
    "result_df2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56426876",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_SP_DF = making_DF('f1_after_spacy_gpt','F1_a_s_preprocessed')\n",
    "\n",
    "## check if word is in para\n",
    "def is_word_in_para(word, para):\n",
    "    return word in para\n",
    "\n",
    "# Apply the function to each row\n",
    "## checks if the source and target values are in the para column\n",
    "GPT_SP_DF['source_in_para'] = GPT_SP_DF.apply(lambda row: is_word_in_para(row['source'], row['para']), axis=1)\n",
    "GPT_SP_DF['target_in_para'] = GPT_SP_DF.apply(lambda row: is_word_in_para(row['target'], row['para']), axis=1)\n",
    "\n",
    "## GPT_SP_DF2 is same as GPT_SP_DF\n",
    "GPT_SP_DF2= GPT_SP_DF.copy()\n",
    "## GPT_DF2 only includes rows that have True values in the source_in_para and target_in_para columns\n",
    "GPT_DF2 = GPT_SP_DF[(GPT_SP_DF['source_in_para'] == True) & (GPT_SP_DF['target_in_para'] == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45baf8bd",
   "metadata": {},
   "source": [
    "making_DF 함수를 이용하여 'f1_after_spacy_gpt' dataframe과 'F1_a_s_preprocessed' dataframe을 처리하여 GPT_SP_DF에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4b7d2",
   "metadata": {},
   "source": [
    "## Preprocessing Dataframes without spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f821460",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_SP_DF3 = CC('f1_gpt', 'F1_preprocessed')\n",
    "# only include rows wheere the 'entity exist' column == 'o'\n",
    "GPT_SP_DF3 = GPT_SP_DF3[GPT_SP_DF3['entity exist?']=='o']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82704e",
   "metadata": {},
   "source": [
    "CC 함수를 이용해 'f1_gpt' dataframe과 'F1_preprocessed' dataframe을 처리하여 GPT_SP_DF3에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_SP_DF3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63e688",
   "metadata": {},
   "source": [
    "GPT_SP_DF3에는 기존 문장, 추출된 entity, 관계, 그리고 추출된 entity가 기존 문장에 있는지 여부 데이터가 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different from the previous GPT_SP_DF\n",
    "# previous input: ('f1_after_spacy_gpt','F1_a_s_preprocessed')\n",
    "GPT_SP_DF = making_DF('f1_gpt', 'F1_preprocessed')\n",
    "# function that checks if word is in para\n",
    "def is_word_in_para(word, para):\n",
    "    return word in para\n",
    "\n",
    "# apply the function to each row\n",
    "GPT_SP_DF['source_in_para'] = GPT_SP_DF.apply(lambda row: is_word_in_para(row['source'], row['para']), axis=1)\n",
    "GPT_SP_DF['target_in_para'] = GPT_SP_DF.apply(lambda row: is_word_in_para(row['target'], row['para']), axis=1)\n",
    "\n",
    "GPT_SP_DF4 = GPT_SP_DF.copy()\n",
    "\n",
    "GPT_DF4 = GPT_SP_DF[(GPT_SP_DF['source_in_para'] == True) & (GPT_SP_DF['target_in_para'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "result_df4 = process('f1_gpt','F1_preprocessed')\n",
    "result_df4.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04433475",
   "metadata": {},
   "source": [
    "procss 함수로 'f1_gpt' dataframe과 F1_preprocessed dataframe을 처리한 후 result_df4에 저장했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8ee82",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e52bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docred dataset의 validation 세트를 로드합니다.\n",
    "validation_set = load_dataset(\"docred\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 통계 변수 초기화\n",
    "total_documents = len(validation_set)\n",
    "total_entities = 0\n",
    "total_sentences = 0\n",
    "# count occurrences of different relation types\n",
    "relation_types = Counter()\n",
    "\n",
    "# 각 문서에 대한 정보 파악\n",
    "for document in validation_set:\n",
    "    # 엔티티 수 계산\n",
    "    total_entities += sum(len(vertex) for vertex in document['vertexSet'])\n",
    "\n",
    "    # 문장 수 계산\n",
    "    total_sentences += len(document['sents'])\n",
    "\n",
    "    # 관계 유형 계산\n",
    "    for relation_type in document['labels']:\n",
    "        relation_types[relation_type] += 1\n",
    "\n",
    "# 평균 엔티티 수와 문서 길이 계산\n",
    "average_entities_per_doc = total_entities / total_documents\n",
    "average_sentences_per_doc = total_sentences / total_documents\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Total Documents: {total_documents}\")\n",
    "print(f\"Average Entities per Document: {average_entities_per_doc}\")\n",
    "print(f\"Average Sentences per Document: {average_sentences_per_doc}\")\n",
    "print(f\"Relation Types: {relation_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서의 엔티티 수와 문장 수를 저장할 리스트를 초기화합니다.\n",
    "entity_counts = []\n",
    "sentence_counts = []\n",
    "\n",
    "# 각 문서에 대한 엔티티 수와 문장 수를 계산합니다.\n",
    "for document in validation_set:\n",
    "    entity_counts.append(len(document['vertexSet']))\n",
    "    sentence_counts.append(len(document['sents']))\n",
    "\n",
    "# 엔티티 수와 문장 수의 최대와 최소를 계산합니다.\n",
    "max_entities = max(entity_counts)\n",
    "min_entities = min(entity_counts)\n",
    "max_sentences = max(sentence_counts)\n",
    "min_sentences = min(sentence_counts)\n",
    "\n",
    "print(f'maxiumum number of entities: {max_entities}, minimum number of entities: {min_entities}, maximum number of sentences: {max_sentences}, minimum number of sentences: {min_sentences}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 통계 변수 초기화\n",
    "# count the occurrence of different relation texts\n",
    "relation_texts = Counter()\n",
    "# count the occurrence of entity pairs\n",
    "entity_pair_relations = Counter()\n",
    "\n",
    "# 각 문서에 대한 정보 파악\n",
    "for document in validation_set:\n",
    "    # 각 관계에 대한 정보 추출\n",
    "    relations = document['labels']\n",
    "    relation_texts_list = relations['relation_text']\n",
    "    heads = relations['head']\n",
    "    tails = relations['tail']\n",
    "\n",
    "    # 관계 유형과 라벨 분포 계산\n",
    "    for relation_text in relation_texts_list:\n",
    "        relation_texts[relation_text] += 1\n",
    "\n",
    "    # 엔티티 쌍 관계 분포 계산\n",
    "    for head, tail in zip(heads, tails):\n",
    "        entity_pair_relations[(head, tail)] += 1\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Relation Texts: {relation_texts}\")\n",
    "print(f\"Entity Pair Relations: {entity_pair_relations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1dd48",
   "metadata": {},
   "source": [
    "각 관계별 count수와 entity별 unique pair의 갯수를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서에서 관계의 개수를 저장할 리스트 초기화\n",
    "relation_counts_per_doc = []\n",
    "\n",
    "# 각 문서를 순회하며 관계의 개수를 계산\n",
    "for document in validation_set:\n",
    "    # 현재 문서에서 관계의 개수를 세고 리스트에 추가\n",
    "    relation_counts_per_doc.append(len(document['labels']['relation_id']))\n",
    "    # print(document['labels']['relation_id'])\n",
    "\n",
    "# 평균, 최대, 최소 관계 개수 계산\n",
    "average_relations = sum(relation_counts_per_doc) / len(relation_counts_per_doc)\n",
    "max_relations = max(relation_counts_per_doc)\n",
    "min_relations = min(relation_counts_per_doc)\n",
    "\n",
    "print(f'average number of relations per document: {average_relations}, maximum number relations in a document: {max_relations}, mininum number of relations in a document: {min_relations}') #, relation_counts_per_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b16f93",
   "metadata": {},
   "source": [
    "### Relation Type별 Count Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b17716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정된 데이터에서 relation_texts Counter 객체를 생성합니다.\n",
    "# 실제 데이터로 대체해야합니다.\n",
    "\n",
    "# 데이터를 준비합니다: 관계 유형과 해당 카운트\n",
    "labels, values = zip(*relation_texts.items())\n",
    "\n",
    "# 관계 유형을 기준으로 내림차순 정렬합니다.\n",
    "labels, values = zip(*sorted(zip(labels, values), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# 히스토그램을 생성합니다.\n",
    "plt.figure(figsize=(15, 20))  # 그래프의 크기를 설정합니다.\n",
    "plt.barh(labels, values, color='slategray')  # 수평 막대 그래프 생성\n",
    "plt.xlabel('Count')  # x축 라벨\n",
    "plt.ylabel('Relation Type')  # y축 라벨\n",
    "plt.title('Relation Type Counts in Descending Order', fontsize=20)  # 그래프 제목\n",
    "plt.gca().invert_yaxis()  # y축의 순서를 뒤집어 내림차순으로 만듭니다.\n",
    "plt.show()  # 그래프를 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SSAN_result_exist_relation1.p', 'rb') as f:\n",
    "    ssan_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45240187",
   "metadata": {},
   "source": [
    "### 성능 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582949a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score calculation\n",
    "def calculate_f1_score(predicted_relations, validation_relations):\n",
    "    # True Positives, False Positives, False Negatives 초기화\n",
    "    TP = sum(pred in validation_relations for pred in predicted_relations)\n",
    "    FP = sum(pred not in validation_relations for pred in predicted_relations)\n",
    "    FN = sum(true_rel not in predicted_relations for true_rel in validation_relations)\n",
    "\n",
    "    # Precision, Recall 계산\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "# extracts and returns tuples of relations from GPT_DF and validation_set\n",
    "def pre_true_DataSet(GPT_DF,validation_set):\n",
    "    # initialize list for predicted relations\n",
    "    predicted_relations = []\n",
    "    # extract relations from GPT_DF\n",
    "    # create a tuple containing the 'source', 'target', and 'relation'.\n",
    "    for i, row in GPT_DF.iterrows():\n",
    "        predicted_relations.append((row['source'], row['target'], row['relation'])) #, row['relation'], set([row['source'], row['target']]\n",
    "    \n",
    "    # initialize list for validation relations\n",
    "    validation_relations = []\n",
    "    # extract relations from validation set\n",
    "    for item in validation_set:\n",
    "        # 각 문장에 대한 정점 집합(vertexSet)과 레이블을 추출\n",
    "        vertex_set = item['vertexSet']\n",
    "        labels = item['labels']\n",
    "\n",
    "        # 각 관계에 대하여\n",
    "        # iterate labels\n",
    "        for head, tail, relation in zip(labels['head'], labels['tail'], labels['relation_text']):\n",
    "            # 'head'와 'tail' 인덱스를 사용하여 각 관계의 개체 이름을 얻음\n",
    "            # extract name of the head entity from 'vertex_set'\n",
    "            head_entity = vertex_set[head][0]['name']  # 첫 번째 인스턴스를 사용\n",
    "            # extract name of the tail entity\n",
    "            tail_entity = vertex_set[tail][0]['name']  # 첫 번째 인스턴스를 사용\n",
    "\n",
    "            # 관계를 튜플 형태로 리스트에 추가\n",
    "            validation_relations.append((head_entity, tail_entity, relation)) # , relation, set([head_entity, tail_entity])\n",
    "\n",
    "    # predicted_relations contains tuples of relations extracted from 'GPT_DF'\n",
    "    # validation_relations contains tuples of relations extracted from 'validation_set'\n",
    "    return predicted_relations, validation_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75409fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_true_DataSet_set(GPT_DF,validation_set):\n",
    "    predicted_relations = []\n",
    "    for i, row in GPT_DF.iterrows():\n",
    "        predicted_relations.append(set([row['source'], row['target']])) #, row['relation'], set([row['source'], row['target']]\n",
    "        predicted_relations = [tuple(s) for s in predicted_relations]\n",
    "        predicted_relations = list(set(predicted_relations))\n",
    "\n",
    "    validation_relations = []\n",
    "    for item in validation_set:\n",
    "        # 각 문장에 대한 정점 집합(vertexSet)과 레이블을 추출\n",
    "        vertex_set = item['vertexSet']\n",
    "        labels = item['labels']\n",
    "\n",
    "        # 각 관계에 대하여\n",
    "        for head, tail, relation in zip(labels['head'], labels['tail'], labels['relation_text']):\n",
    "            # 'head'와 'tail' 인덱스를 사용하여 각 관계의 개체 이름을 얻음\n",
    "            head_entity = vertex_set[head][0]['name']  # 첫 번째 인스턴스를 사용\n",
    "            tail_entity = vertex_set[tail][0]['name']  # 첫 번째 인스턴스를 사용\n",
    "\n",
    "            # 관계를 튜플 형태로 리스트에 추가\n",
    "            validation_relations.append(set([head_entity, tail_entity])) # , relation, set([head_entity, tail_entity])\n",
    "            validation_relations = [tuple(s) for s in validation_relations]\n",
    "            validation_relations = list(set(validation_relations))\n",
    "    return predicted_relations, validation_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f334ee",
   "metadata": {},
   "source": [
    "* GPT_DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_DF2\n",
    "predicted_relations_gpt_small, validation_relations = pre_true_DataSet(GPT_DF2,validation_set)\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_relations_gpt_small, validation_relations)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e52692",
   "metadata": {},
   "source": [
    "* GPT_DF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_DF4\n",
    "predicted_relations_gpt_larger, validation_relations = pre_true_DataSet(GPT_DF4,validation_set)\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_relations_gpt_larger, validation_relations)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c86d0",
   "metadata": {},
   "source": [
    "* extracting predicted relations from result_df2 dataframe and result_df4 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# result_df2\n",
    "predicted_relations, validation_relations = pre_true_DataSet(result_df2,validation_set)\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_relations, validation_relations)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predicted_relations, validation_relations = pre_true_DataSet(result_df4,validation_set)\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_relations, validation_relations)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf88f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding NER columns, source_ner_2 and target_ner_2 to result_df4\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 단어를 가져와서 NER 태그를 찾습니다.\n",
    "ner_tags = [name_type_dict.get(word, 'unknown') for word in result_df4['source']]\n",
    "\n",
    "# 이제 ner_tags 리스트는 데이터프레임의 행 수와 일치합니다.\n",
    "result_df4['source_ner_2'] = ner_tags\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 단어를 가져와서 NER 태그를 찾습니다.\n",
    "ner_tags = [name_type_dict.get(word, 'unknown') for word in result_df4['target']]\n",
    "\n",
    "# 이제 ner_tags 리스트는 데이터프레임의 행 수와 일치합니다.\n",
    "result_df4['target_ner_2'] = ner_tags\n",
    "\n",
    "result_df4 = result_df4[(result_df4['source_ner_2']!='unknown')&(result_df4['target_ner_2']!='unknown')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023e48d",
   "metadata": {},
   "source": [
    "ner 값이 'unknown'인 row를 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_relations, validation_relations = pre_true_DataSet(result_df4,validation_set)\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_relations, validation_relations)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4211e2f",
   "metadata": {},
   "source": [
    "ner값이 'unknown'인 row를 삭제 후 precision과 F1 Score는 증가, recall은 감소했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaabeed",
   "metadata": {},
   "source": [
    "### SSAN with relation 성능지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415fae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_relations= []\n",
    "for i in range(len(ssan_result)):\n",
    "    predicted_relations.append(ssan_result[i]['relation'])\n",
    "predicted_relations =  [item for sublist in predicted_relations  for item in sublist]\n",
    "_, validation_relations = pre_true_DataSet(GPT_DF4,validation_set)\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_relations, validation_relations)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d92c",
   "metadata": {},
   "source": [
    "### SSAN without relation 성능지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7174d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 값보다 precision, recall, f1 score 값이 낮습니다.\n",
    "# list of sets\n",
    "predicted_relations= []\n",
    "for i in range(len(ssan_result)):\n",
    "    predicted_relations.append(ssan_result[i]['relation'])\n",
    "predicted_relations =  [item for sublist in predicted_relations  for item in sublist]\n",
    "\n",
    "# transformation: only contains source entity, target entity, and relation entity.\n",
    "predicted_without_relations = [set([s,t]) for s,t,r in predicted_relations]\n",
    "predicted_without_relations = [tuple(sorted(relation)) for relation in predicted_without_relations]\n",
    "\n",
    "_, validation_relations = pre_true_DataSet(GPT_DF4,validation_set)\n",
    "validation_without_relations = [(source, target) for source, target, _ in validation_relations]\n",
    "# F1 점수 계산\n",
    "precision, recall ,f1_score = calculate_f1_score(predicted_without_relations, validation_without_relations)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b519f10",
   "metadata": {},
   "source": [
    "### ChatGPT with relation 성능지표\n",
    "'GPT_DF2 dataframe'과 'GPT_DF4' dataframe에서 추출한 predicted_relations를 합친 후 precision, recall, f1_score를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4841446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'GPT_DF2' dataframe 과 'GPT_DF4' dataframe에서 추출한 predicted_relations를 합칩니다.\n",
    "pr = list(predicted_relations_gpt_larger+predicted_relations)\n",
    "\n",
    "precision, recall ,f1_score = calculate_f1_score(pr, validation_relations)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e3aa2",
   "metadata": {},
   "source": [
    "### ChatGPT without relation 성능지표\n",
    "'GPT_DF2 dataframe'과 'GPT_DF4' dataframe에서 추출한 predicted_relations를 합친 후 relation을 제외한 precision, recall, f1_score를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT with relation 성능지표\n",
    "pr = list(predicted_relations_gpt_larger+predicted_relations)\n",
    "pr_without_relations = [(entity_1, entity_2) for entity_1, entity_2, _ in pr]\n",
    "valdiation_without_relations = [(entity_1, entity_2) for entity_1, entity_2, _ in validation_relations]\n",
    "\n",
    "precision, recall ,f1_score = calculate_f1_score(pr_without_relations, valdiation_without_relations)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045a5da",
   "metadata": {},
   "source": [
    "relation을 제외하고 entity 쌍들만 비교를 합니다. precision, recall, F1 score는 predicted entity pair가 validation set에 얼만큼 일치하는지 여부에 따라 계산되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3993792",
   "metadata": {},
   "source": [
    "### SSAN Relations and Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities\n",
    "# ssan_result[i]['relation']\n",
    "# 각 문서에서 관계의 개수를 저장할 리스트 초기화\n",
    "entity_counts_per_doc = []\n",
    "\n",
    "# 각 문서를 순회하며 관계의 개수를 계산\n",
    "for document in ssan_result:\n",
    "    # 현재 문서에서 관계의 개수를 세고 리스트에 추가\n",
    "    entity_counts_per_doc.append(len(document['entities']))\n",
    "    # print(document['labels']['relation_id'])\n",
    "\n",
    "# 평균, 최대, 최소 관계 개수 계산\n",
    "average_entities = sum(entity_counts_per_doc) / len(entity_counts_per_doc)\n",
    "max_entities = max(entity_counts_per_doc)\n",
    "min_entities = min(entity_counts_per_doc)\n",
    "\n",
    "print(f'average number of entities per doc: {average_entities}, maximum number of entites: {max_entities}, minimum entities: {min_entities}') #, relation_counts_per_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af219f8",
   "metadata": {},
   "source": [
    "각 문서별 entity와 relation의 갯수를 count 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relations\n",
    "# ssan_result[i]['relation']\n",
    "# 각 문서에서 관계의 개수를 저장할 리스트 초기화\n",
    "relation_counts_per_doc = []\n",
    "\n",
    "# 각 문서를 순회하며 관계의 개수를 계산\n",
    "for document in ssan_result:\n",
    "    # 현재 문서에서 관계의 개수를 세고 리스트에 추가\n",
    "    relation_counts_per_doc.append(len(document['relation']))\n",
    "    # print(document['labels']['relation_id'])\n",
    "\n",
    "# 평균, 최대, 최소 관계 개수 계산\n",
    "average_relations = sum(relation_counts_per_doc) / len(relation_counts_per_doc)\n",
    "max_relations = max(relation_counts_per_doc)\n",
    "min_relations = min(relation_counts_per_doc)\n",
    "\n",
    "print(f'average number of relations per doc: {average_relations}, maximum number of relations: {max_relations}, minimum number of relations: {min_relations}') #, relation_counts_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5747f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 통계 변수 초기화\n",
    "relation_texts = Counter()\n",
    "#entity_pair_relations = Counter()\n",
    "\n",
    "# 각 문서에 대한 정보 파악\n",
    "# loop through each 'document' in the 'ssan_result'\n",
    "for document in ssan_result:\n",
    "    # 각 관계에 대한 정보 추출\n",
    "    relations = document['relation']\n",
    "\n",
    "    for i in relations:\n",
    "        # counts the occurence of each type of relation\n",
    "        relation_texts[i[2]] += 1\n",
    " \n",
    "\n",
    "# 카운트 순으로 정렬합니다.\n",
    "sorted_relations = relation_texts.most_common()\n",
    "\n",
    "# 정렬된 결과를 출력합니다.\n",
    "sorted_relations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275e5d9",
   "metadata": {},
   "source": [
    "SSAN 모델에서 가장 많은 relations입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1216aa0",
   "metadata": {},
   "source": [
    "### GPT Relations and Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ad3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터 프레임을 생성합니다. 실제로는 파일에서 읽어오거나 다른 소스에서 가져올 수 있습니다.\n",
    "df = GPT_DF4.copy()\n",
    "\n",
    "# 문서별로 엔티티 수와 관계 수를 계산합니다.\n",
    "doc_entities = defaultdict(set)\n",
    "doc_relations = defaultdict(int)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    para_id = hash(tuple(row['para']))  # 문단을 해싱하여 고유한 문서 ID 생성\n",
    "    doc_entities[para_id].update([row['source'], row['target']])\n",
    "    doc_relations[para_id] += 1\n",
    "\n",
    "# 엔티티와 관계 수에 대한 통계를 계산합니다.\n",
    "entity_counts = [len(entities) for entities in doc_entities.values()]\n",
    "relation_counts = list(doc_relations.values())\n",
    "\n",
    "# 엔티티에 대한 통계\n",
    "average_entities = sum(entity_counts) / len(entity_counts)\n",
    "max_entities = max(entity_counts)\n",
    "min_entities = min(entity_counts)\n",
    "\n",
    "# 관계에 대한 통계\n",
    "average_relations = sum(relation_counts) / len(relation_counts)\n",
    "max_relations = max(relation_counts)\n",
    "min_relations = min(relation_counts)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'average number of unique entities: {average_entities}, maximum number of unique entities: {max_entities}, minimum number of unique entities: {min_entities}, average number of relations: {average_relations}, maximum number of relations: {max_relations}, minimum relations: {min_relations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 관계 유형의 분포를 계산합니다.\n",
    "relation_distribution = Counter(df['relation'])\n",
    "\n",
    "# 가장 많이 나타나는 상위 3개의 관계 유형을 찾습니다.\n",
    "most_common_relations = relation_distribution.most_common(3)\n",
    "\n",
    "# 결과를 출력하는 문자열을 포맷합니다.\n",
    "most_common_relations_report = \"데이터셋은 다음과 같은 상위 3개의 관계 유형을 포함합니다: \"\n",
    "most_common_relations_report += \", \".join([f\"'{relation[0]}'({relation[1]}회)\" for relation in most_common_relations]) + \".\"\n",
    "\n",
    "most_common_relations_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f8da0",
   "metadata": {},
   "source": [
    "### 벤다이어그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted_relations into a set\n",
    "setA = set(predicted_relations)\n",
    "# convert predicted_relations into a set\n",
    "setB = set(predicted_relations_gpt_small)\n",
    "# convert predicted_relations into a set\n",
    "setC = set(predicted_relations_gpt_larger)\n",
    "\n",
    "# 벤 다이어그램 그리기\n",
    "venn3([setA, setB, setC], ('SSAN', 'ChatGPT-small spacy', 'Set C-large spacy'))\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865b72d",
   "metadata": {},
   "source": [
    "* predicted_relations_gpt_small 벤 다이어그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted_relations into a set\n",
    "setA = set(predicted_relations)\n",
    "# convert predicted_relations into a set\n",
    "setB = set(predicted_relations_gpt_small)\n",
    "# convert predicted_relations into a set\n",
    "setC = set(predicted_relations_gpt_larger)\n",
    "# convert predicted_relations into a set\n",
    "setD = set(validation_relations)\n",
    "\n",
    "# 각 집합의 라벨에 전체 개수 포함\n",
    "labels = ['True label\\n(Total: {})'.format(len(setD)),\n",
    "          'SSAN\\n(Total: {})'.format(len(setA)),\n",
    "          'RE ChatGPT\\n(Total: {})'.format(len(setB))]\n",
    "# 벤 다이어그램 그리기\n",
    "venn3([setD, setA, setB], set_labels=labels)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175ca83",
   "metadata": {},
   "source": [
    "* predicted_relations_gpt_larger 벤 다이어그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted_relations into a set\n",
    "setA = set(predicted_relations)\n",
    "# convert predicted_relations into a set\n",
    "setB = set(predicted_relations_gpt_small)\n",
    "# convert predicted_relations into a set\n",
    "setC = set(predicted_relations_gpt_larger)\n",
    "# convert predicted_relations into a set\n",
    "setD = set(validation_relations)\n",
    "\n",
    "# 각 집합의 라벨에 전체 개수 포함\n",
    "labels = ['True label\\n(Total: {})'.format(len(setD)),\n",
    "          'SSAN\\n(Total: {})'.format(len(setA)),\n",
    "          'RE ChatGPT\\n(Total: {})'.format(len(setC))]\n",
    "# 벤 다이어그램 그리기\n",
    "venn3([setD, setA, setC], set_labels=labels)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e8ed8",
   "metadata": {},
   "source": [
    "* Venn Diagram With Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd9083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted_relations into a set\n",
    "setA = set(predicted_relations)\n",
    "# convert predicted_relations into a set\n",
    "pr = list(predicted_relations_gpt_larger+predicted_relations)\n",
    "setC = set(pr)\n",
    "# convert predicted_relations into a set\n",
    "setD = set(validation_relations)\n",
    "\n",
    "# 각 집합의 라벨에 전체 개수 포함\n",
    "labels = ['True label\\n(Total: {})'.format(len(setD)),\n",
    "          'SSAN\\n(Total: {})'.format(len(setA)),\n",
    "          'SSAN + ChatGPT\\n(Total: {})'.format(len(setC))]\n",
    "# 벤 다이어그램 그리기\n",
    "venn3([setD, setA, setC], set_labels=labels)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1d538",
   "metadata": {},
   "source": [
    "* 두 모델을 합쳤을때 recall은 증가하지만 precision은 감소합니다.\n",
    "* ChatGPT 모델은 SSAN 모델보다 224개의 triplet을 더 추출했습니다.\n",
    "* SSAN과 ChatGPT를 혼합하면 보완하는 relation extraction 모델이 될수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca6277",
   "metadata": {},
   "source": [
    "* Venn Diagram Without Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벤 다이어그램의 모양이 슬라이드와 다릅니다\n",
    "# convert predicted_relations into a set\n",
    "setA_r = set(predicted_without_relations)\n",
    "# convert predicted_relations into a set\n",
    "pr = list(predicted_relations_gpt_larger+predicted_relations)\n",
    "pr_without_relations = [(entity_1, entity_2) for entity_1, entity_2, _ in pr]\n",
    "setC_r = set(pr_without_relations)\n",
    "# convert predicted_relations into a set\n",
    "setD_r = set(validation_without_relations)\n",
    "\n",
    "# 각 집합의 라벨에 전체 개수 포함\n",
    "labels = ['True label\\n(Total: {})'.format(len(setD_r)),\n",
    "          'SSAN\\n(Total: {})'.format(len(setA_r)),\n",
    "          'SSAN + ChatGPT\\n(Total: {})'.format(len(setC_r))]\n",
    "# 벤 다이어그램 그리기\n",
    "venn3([setD_r, setA_r, setC_r], set_labels=labels)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba4bf0",
   "metadata": {},
   "source": [
    "* setB에 있는 triplet을 제외한 후 setA와 setC의 triplet에 대한 그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new counter object\n",
    "relation_texts = Counter()\n",
    "# setA & setC - setB = elements that are common to both setA and setC\n",
    "for triplet in list(setA & setC - setB):\n",
    "    # count the third item in each 'triplet' tuple\n",
    "    relation_texts[triplet[2]] += 1\n",
    "list(relation_texts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정된 데이터에서 relation_texts Counter 객체를 생성합니다.\n",
    "# 실제 데이터로 대체해야합니다.\n",
    "\n",
    "\n",
    "# 데이터를 준비합니다: 관계 유형과 해당 카운트\n",
    "# extract extract labels and counter values\n",
    "labels, values = zip(*relation_texts.items())\n",
    "\n",
    "# 관계 유형을 기준으로 내림차순 정렬합니다.\n",
    "# sorting the relations in descending order\n",
    "labels, values = zip(*sorted(zip(labels, values), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# 히스토그램을 생성합니다.\n",
    "plt.figure(figsize=(15, 20))  # 그래프의 크기를 설정합니다.\n",
    "plt.barh(labels, values, color='slategray')  # 수평 막대 그래프 생성\n",
    "plt.xlabel('Count')  # x축 라벨\n",
    "plt.ylabel('Relation Type')  # y축 라벨\n",
    "plt.title('Relation Type Counts in Descending Order', fontsize=20)  # 그래프 제목\n",
    "plt.gca().invert_yaxis()  # y축의 순서를 뒤집어 내림차순으로 만듭니다.\n",
    "plt.show()  # 그래프를 표시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490e028",
   "metadata": {},
   "source": [
    "* setA, setB, setC에 있는 모든 triplet 포함한 그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05af7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new counter object\n",
    "relation_texts = Counter()\n",
    "# setA & setC & setB = elements that are common to setA, setC, and setB\n",
    "for triplet in list(setA & setC & setB):\n",
    "    # count the third item in each 'triplet' tuple\n",
    "    relation_texts[triplet[2]] += 1\n",
    "\n",
    "# 가정된 데이터에서 relation_texts Counter 객체를 생성합니다.\n",
    "# 실제 데이터로 대체해야합니다.\n",
    "\n",
    "\n",
    "# 데이터를 준비합니다: 관계 유형과 해당 카운트\n",
    "# extract extract labels and counter values\n",
    "labels, values = zip(*relation_texts.items())\n",
    "\n",
    "# 관계 유형을 기준으로 내림차순 정렬합니다.\n",
    "# sorting the relations in descending order\n",
    "labels, values = zip(*sorted(zip(labels, values), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# 히스토그램을 생성합니다.\n",
    "plt.figure(figsize=(15, 20))  # 그래프의 크기를 설정합니다.\n",
    "plt.barh(labels, values, color='slategray')  # 수평 막대 그래프 생성\n",
    "plt.xlabel('Count')  # x축 라벨\n",
    "plt.ylabel('Relation Type')  # y축 라벨\n",
    "plt.title('Relation Type Counts in Descending Order', fontsize=20)  # 그래프 제목\n",
    "plt.gca().invert_yaxis()  # y축의 순서를 뒤집어 내림차순으로 만듭니다.\n",
    "plt.show()  # 그래프를 표시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312ebc8",
   "metadata": {},
   "source": [
    "# 3. Grouping\n",
    "본 섹션에서는 데이터프레임내의 entity pair들의 similarity를 계산한후, 각 임계값 별 단어 grouping의 변화를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity dataframe\n",
    "sim_DF = pd.read_csv('SIM_df_all.csv')\n",
    "sim_DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41935a2e",
   "metadata": {},
   "source": [
    "* entity 1 = entity 1\n",
    "* entity 2 = entity 2\n",
    "* similarity_percent = similarity percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e37597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity1과 entity2 쌍에 대해 similarity_percent가 가장 높은 행만 남깁니다.\n",
    "# for each unique pair of 'entity1' and 'entity2', it forms a group\n",
    "# finding indexes of rows with maximum simlarity percent\n",
    "# finds the index of the row with the maximum value in the column 'similarity_percent'\n",
    "idx = sim_DF.groupby(['entity1', 'entity2'])['similarity_percent'].idxmax()\n",
    "\n",
    "# 위에서 얻은 인덱스를 사용하여 중복 없는 데이터 프레임을 얻습니다.\n",
    "# create a new dataframe with unique highest similarity rows\n",
    "sim_df = sim_DF.loc[idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAN_SP_DF = pd.read_csv('SSAN_SP_DF_TAG.csv')\n",
    "SSAN_SP_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_SP_DF = pd.read_csv('GPT_SP_DF_TAG.csv')\n",
    "GPT_SP_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with 'Unkown' values\n",
    "GPT_SP_DF = GPT_SP_DF[(GPT_SP_DF['source_ner']!='Unknown')&(GPT_SP_DF['target_ner']!='Unknown')]\n",
    "GPT_SP_DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for high similiarity pairs\n",
    "def create_representative_df(SP_DF, sim_df, similarity_threshold):\n",
    "    # 유사도가 주어진 임계값 이상인 쌍만 필터링\n",
    "    high_similarity_pairs = sim_df[sim_df['similarity_percent'] >= similarity_threshold]\n",
    "\n",
    "    # 그래프 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 엔티티 쌍을 그래프의 엣지로 추가\n",
    "    for index, row in high_similarity_pairs.iterrows():\n",
    "        G.add_edge(row['entity1'], row['entity2'], weight=row['similarity_percent'])\n",
    "\n",
    "    # 연결된 컴포넌트 리스트\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "\n",
    "    # 각 연결된 컴포넌트에 대한 PageRank 계산\n",
    "    subgraph_representatives = {}\n",
    "    for component in connected_components:\n",
    "        subgraph = G.subgraph(component)\n",
    "        pagerank = nx.pagerank(subgraph, weight='weight')\n",
    "        # 가장 중요한 노드를 대표 엔티티로 선택\n",
    "        representative = max(pagerank, key=pagerank.get)\n",
    "        for node in component:\n",
    "            subgraph_representatives[node] = representative\n",
    "    \n",
    "    source_normalized = 'source_normalized_{}'.format(similarity_threshold)\n",
    "    target_normalized = 'target_normalized_{}'.format(similarity_threshold)\n",
    "\n",
    "    # 원본 데이터에서 source와 target을 대표 엔티티로 매핑\n",
    "    SP_DF[source_normalized] = SP_DF['source'].apply(lambda x: subgraph_representatives.get(x, x))\n",
    "    SP_DF[target_normalized] = SP_DF['target'].apply(lambda x: subgraph_representatives.get(x, x))\n",
    "\n",
    "    # 딕셔너리를 데이터 프레임으로 변환\n",
    "    representative_df = pd.DataFrame(list(subgraph_representatives.items()), columns=['Word', 'Representative'])\n",
    "\n",
    "    return representative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a65377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with SSAN\n",
    "representative_df_95 = create_representative_df(SSAN_SP_DF, sim_df, 95)\n",
    "representative_df_90 = create_representative_df(SSAN_SP_DF, sim_df, 90)\n",
    "representative_df_85 = create_representative_df(SSAN_SP_DF, sim_df, 85)\n",
    "representative_df_80 = create_representative_df(SSAN_SP_DF, sim_df, 80)\n",
    "representative_df_75 = create_representative_df(SSAN_SP_DF, sim_df, 75)\n",
    "representative_df_70 = create_representative_df(SSAN_SP_DF, sim_df, 70)\n",
    "representative_df_65 = create_representative_df(SSAN_SP_DF, sim_df, 65)\n",
    "representative_df_60 = create_representative_df(SSAN_SP_DF, sim_df, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with GPT\n",
    "g_representative_df_95 = create_representative_df(GPT_SP_DF, sim_df, 95)\n",
    "g_representative_df_90 = create_representative_df(GPT_SP_DF, sim_df, 90)\n",
    "g_representative_df_85 = create_representative_df(GPT_SP_DF, sim_df, 85)\n",
    "g_representative_df_80 = create_representative_df(GPT_SP_DF, sim_df, 80)\n",
    "g_representative_df_75 = create_representative_df(GPT_SP_DF, sim_df, 75)\n",
    "g_representative_df_70 = create_representative_df(GPT_SP_DF, sim_df, 70)\n",
    "g_representative_df_65 = create_representative_df(GPT_SP_DF, sim_df, 65)\n",
    "g_representative_df_60 = create_representative_df(GPT_SP_DF, sim_df, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038746c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a 'sorted_pair' column\n",
    "# creating tuples of 'source' and 'target' and sort alphabetically\n",
    "GPT_SP_DF['sorted_pair'] = GPT_SP_DF.apply(lambda row: tuple(sorted([row['source'], row['target']], key=lambda x: (isinstance(x, str), x))), axis=1)\n",
    "# remove duplicates and drop the 'sorted_pair' column\n",
    "GPT_SP_DF_sorted = GPT_SP_DF.drop_duplicates(subset='sorted_pair').drop('sorted_pair', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb5f31",
   "metadata": {},
   "source": [
    "데이터프레임내의 entity pair를 알파벳순으로 정렬 후 중복되는 entity pair를 drop합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6807e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with 'Unknown' values in the 'source_ner' and 'target_ner' columns\n",
    "GPT_SP_DF_sorted = GPT_SP_DF_sorted[(GPT_SP_DF_sorted['source_ner']!='Unknown') & (GPT_SP_DF_sorted['target_ner']!='Unknown')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a30b3",
   "metadata": {},
   "source": [
    "'source_ner' 과 'target_ner' 값이 'Unknown'인 row를 드랍합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 데이터를 생성합니다. 실제 데이터로 대체해야 합니다.\n",
    "# 이 데이터는 임시 데이터로, 실제 그래프를 생성할 때는 실제 데이터셋을 사용해야 합니다.\n",
    "\n",
    "def generate_representative_df(sim_df, similarity_threshold):\n",
    "    # 유사도가 주어진 임계값 이상인 쌍만 필터링\n",
    "    high_similarity_pairs = sim_df[sim_df['similarity_percent'] >= similarity_threshold]\n",
    "\n",
    "    # 그래프 생성\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 엔티티 쌍을 그래프의 엣지로 추가\n",
    "    for index, row in high_similarity_pairs.iterrows():\n",
    "        G.add_edge(row['entity1'], row['entity2'], weight=row['similarity_percent'])\n",
    "\n",
    "    # 연결된 컴포넌트 리스트\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "\n",
    "    # 각 연결된 컴포넌트에 대한 PageRank 계산\n",
    "    subgraph_representatives = {}\n",
    "    for component in connected_components:\n",
    "        subgraph = G.subgraph(component)\n",
    "        pagerank = nx.pagerank(subgraph, weight='weight')\n",
    "        # 가장 중요한 노드를 대표 엔티티로 선택\n",
    "        representative = max(pagerank, key=pagerank.get)\n",
    "        for node in component:\n",
    "            subgraph_representatives[node] = representative\n",
    "\n",
    "    # 대표 엔티티를 데이터 프레임으로 변환\n",
    "    representative_df = pd.DataFrame.from_dict(subgraph_representatives, orient='index', columns=['Representative'])\n",
    "    representative_df.reset_index(inplace=True)\n",
    "    representative_df.rename(columns={'index': 'Word'}, inplace=True)\n",
    "\n",
    "    return representative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d584ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 유사도 임계값에 대한 대표 단어 데이터프레임 생성\n",
    "thresholds = [95, 90, 85, 80, 75, 70, 65, 60]\n",
    "representative_dfs = {t: generate_representative_df(sim_df, t) for t in thresholds}\n",
    "\n",
    "# 겹치지 않게 모든 데이터프레임 시각화\n",
    "fig, axs = plt.subplots(len(thresholds), 1, figsize=(15, 40))  # 그래프 크기 조정\n",
    "\n",
    "for i, t in enumerate(thresholds):\n",
    "    df = representative_dfs[t]\n",
    "    \n",
    "    # 데이터 포인트 사이의 겹침을 줄이기 위해, 각 대표 단어에 대한 y 위치를 조정합니다.\n",
    "    unique_representatives = df['Representative'].unique()\n",
    "    y_positions = pd.Series(range(len(unique_representatives)), index=unique_representatives)\n",
    "    df['y_position'] = df['Representative'].map(y_positions)\n",
    "    \n",
    "    # 각 임계값에 대한 대표 단어의 분포를 시각화\n",
    "    axs[i].scatter(df.index, df['y_position'], label=f'{t}% Similarity Threshold',s=3)\n",
    "    axs[i].set_yticks(y_positions)\n",
    "    axs[i].set_yticklabels(unique_representatives, fontsize=3)\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Representative Words at {t}% Similarity Threshold')\n",
    "    axs[i].set_xlabel('Words')\n",
    "    axs[i].set_ylabel('Representative Words (Groups)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c73d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_df는 유사도 데이터를 포함하는 데이터프레임입니다.\n",
    "# thresholds는 다양한 유사도 임계값을 나타냅니다.\n",
    "thresholds = [95, 90, 85, 80, 75, 70, 65, 60]\n",
    "\n",
    "# 각 임계값에 대한 데이터프레임을 저장하는 딕셔너리\n",
    "representative_dfs = {t: generate_representative_df(sim_df, t) for t in thresholds}\n",
    "\n",
    "# 모든 엔티티와 대표 엔티티의 유니크한 리스트를 생성합니다.\n",
    "all_entities = pd.unique(sim_df[['entity1', 'entity2']].values.ravel('K'))\n",
    "all_representatives = pd.unique(pd.concat([df['Representative'] for df in representative_dfs.values()]))\n",
    "\n",
    "# 엔티티와 대표 엔티티에 대해 인덱스를 생성합니다.\n",
    "entity_to_index = {entity: i for i, entity in enumerate(all_entities)}\n",
    "representative_to_index = {rep: i for i, rep in enumerate(all_representatives)}\n",
    "\n",
    "# 색상 맵 생성\n",
    "colors = plt.cm.get_cmap('viridis', len(thresholds))\n",
    "\n",
    "# 그래프 생성\n",
    "plt.figure(figsize=(300, 300))\n",
    "\n",
    "# 각 임계값에 대해 데이터 포인트를 그래프에 추가\n",
    "for i, (threshold, df) in enumerate(representative_dfs.items()):\n",
    "    # 대표 엔티티의 인덱스를 얻습니다.\n",
    "    df['Representative_Index'] = df['Representative'].map(representative_to_index)\n",
    "    \n",
    "    # 원래 엔티티의 인덱스를 얻습니다.\n",
    "    df['Word_Index'] = df['Word'].map(entity_to_index)\n",
    "    \n",
    "    # 데이터 포인트를 그래프에 추가\n",
    "    plt.scatter(df['Word_Index'], df['Representative_Index'], color=colors(i), label=f'Threshold {threshold}%')\n",
    "\n",
    "# 레이블과 타이틀 추가\n",
    "plt.xlabel('Entities')\n",
    "plt.ylabel('Representative Entities')\n",
    "plt.title('Entity vs Representative Entity at Different Similarity Thresholds')\n",
    "plt.xticks(np.arange(len(all_entities)), all_entities, rotation=90)\n",
    "plt.yticks(np.arange(len(all_representatives)), all_representatives)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_DF = pd.read_csv('SIM_df_all-mpnet-base-v2_2.csv')\n",
    "# convert the 'similarity_percent' column to percent\n",
    "sim_DF['similarity_percent'] = sim_DF['similarity_percent'] * 100\n",
    "\n",
    "# entity1과 entity2 쌍에 대해 similarity_percent가 가장 높은 행만 남깁니다.\n",
    "# groupby와 idxmax를 사용하여 가장 높은 similarity_percent를 가진 인덱스를 얻습니다.\n",
    "idx = sim_DF.groupby(['entity1', 'entity2'])['similarity_percent'].idxmax()\n",
    "\n",
    "# 위에서 얻은 인덱스를 사용하여 중복 없는 데이터 프레임을 얻습니다.\n",
    "sim_df = sim_DF.loc[idx].reset_index(drop=True)\n",
    "\n",
    "gpt_sim_DF = pd.read_csv('GPT_SIM_df_all-mpnet-base-v2.csv')\n",
    "\n",
    "# entity1과 entity2 쌍에 대해 similarity_percent가 가장 높은 행만 남깁니다.\n",
    "# groupby와 idxmax를 사용하여 가장 높은 similarity_percent를 가진 인덱스를 얻습니다.\n",
    "gpt_idx = gpt_sim_DF.groupby(['entity1', 'entity2'])['similarity_percent'].idxmax()\n",
    "\n",
    "# 위에서 얻은 인덱스를 사용하여 중복 없는 데이터 프레임을 얻습니다.\n",
    "gpt_sim_df = gpt_sim_DF.loc[gpt_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec2f51",
   "metadata": {},
   "source": [
    "'sim_DF' 데이터프레임과 'gpt_sim_DF' 데이터프레임에서 groupby와 idxmax를 사용하여 가장 높은 similarity_percent를 가진 인덱스를 얻습니다. 이후 해당 인덱스로 중복 없는 데이터 프레임을 얻고 sim_df와 gpt_sim_df에 각각 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제로 사용할 SSAN_SP_DF 데이터프레임 생성 (이것은 실제 데이터로 대체되어야 합니다)\n",
    "thresholds = [95, 90, 85, 80, 75, 70, 65, 60]\n",
    "\n",
    "# 유사도 임계값별로 SSAN_SP_DF의 source와 target을 대표 단어로 매핑\n",
    "for threshold in thresholds:\n",
    "    representative_df = create_representative_df(SSAN_SP_DF, sim_df, threshold)\n",
    "    # create a dictionary from the representative_df for mapping\n",
    "    rep_dict = dict(zip(representative_df['Word'], representative_df['Representative']))\n",
    "    # map the 'source' column to its representative words, keeping orignial words where no mapping exists\n",
    "    SSAN_SP_DF[f'source_rep_{threshold}'] = SSAN_SP_DF['source'].map(rep_dict).fillna(SSAN_SP_DF['source'])\n",
    "    # map the 'target' column to its representative words, keeping orignial words where no mapping exists\n",
    "    SSAN_SP_DF[f'target_rep_{threshold}'] = SSAN_SP_DF['target'].map(rep_dict).fillna(SSAN_SP_DF['target'])\n",
    "\n",
    "    representative_df = create_representative_df(GPT_SP_DF, gpt_sim_df, threshold)\n",
    "    # create a dictionary from the representative_df for mapping\n",
    "    rep_dict = dict(zip(representative_df['Word'], representative_df['Representative']))\n",
    "    # map the 'source' column to its representative words, keeping orignial words where no mapping exists\n",
    "    GPT_SP_DF[f'source_rep_{threshold}'] = GPT_SP_DF['source'].map(rep_dict).fillna(GPT_SP_DF['source'])\n",
    "    # map the 'target' column to its representative words, keeping orignial words where no mapping exists\n",
    "    GPT_SP_DF[f'target_rep_{threshold}'] = GPT_SP_DF['target'].map(rep_dict).fillna(GPT_SP_DF['target'])\n",
    "\n",
    "    GPT_SP_DF2 = GPT_SP_DF.copy()\n",
    "    representative_df = create_representative_df(GPT_SP_DF2, gpt_sim_df, threshold)\n",
    "    # create a dictionary from the representative_df for mapping\n",
    "    rep_dict = dict(zip(representative_df['Word'], representative_df['Representative']))\n",
    "    # map the 'source' column to its representative words, keeping orignial words where no mapping exists\n",
    "    GPT_SP_DF2[f'source_rep_{threshold}'] = GPT_SP_DF2['source'].map(rep_dict).fillna(GPT_SP_DF2['source'])\n",
    "    # map the 'target' column to its representative words, keeping orignial words where no mapping exists\n",
    "    GPT_SP_DF2[f'target_rep_{threshold}'] = GPT_SP_DF2['target'].map(rep_dict).fillna(GPT_SP_DF2['target'])\n",
    "# 이제 SSAN_SP_DF 데이터프레임에는 각 임계값에 따른 source와 target의 대표 단어가 매핑된 컬럼들이 추가되어 있습니다.\n",
    "# 예를 들어, 'source_rep_95', 'target_rep_95', ..., 'source_rep_60', 'target_rep_60' 등의 컬럼들이 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fde69e",
   "metadata": {},
   "source": [
    "similarity threshold가 entity 그룹화에 어떤 영향을 미치는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "SSAN_SP_DF_without_column = SSAN_SP_DF.drop(['edge','company','source_ner_2','target_ner_2'], axis=1)\n",
    "# filtering rows with 'Unknown' values\n",
    "SSAN_SP_DF_without_column = SSAN_SP_DF_without_column[(SSAN_SP_DF_without_column['source_ner']!='Unknown')&(SSAN_SP_DF_without_column['target_ner']!='Unknown')]\n",
    "# dropping columns\n",
    "SSAN_SP_DF_without_column = SSAN_SP_DF_without_column.drop(['source_ner_3','target_ner_3','source_ner','target_ner'], axis=1)\n",
    "\n",
    "# dropping columns\n",
    "GPT_SP_DF_without_column = GPT_SP_DF.drop(['para','relation','company','source_ner_2','target_ner_2'], axis=1)\n",
    "# filtering rows with 'Unknown' values\n",
    "GPT_SP_DF_without_column = GPT_SP_DF_without_column[(GPT_SP_DF_without_column['source_ner_4']!='Unknown')&(GPT_SP_DF_without_column['target_ner_4']!='Unknown')]\n",
    "# dropping columns\n",
    "GPT_SP_DF_without_column = GPT_SP_DF_without_column.drop(['source_ner_3','target_ner_3','source_ner_4','target_ner_4','target_ner','source_ner'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER 태그를 카테고리에 매핑하는 함수\n",
    "def map_ner_to_category(ner_tag):\n",
    "    if ner_tag in ['GPE', 'LOC','FAC']:\n",
    "        return 'Country'\n",
    "    elif ner_tag in ['ORG']:\n",
    "        return 'Firm'\n",
    "    elif ner_tag in ['MATERIAL']: # 'MONEY',\n",
    "        return 'Resource'\n",
    "    elif ner_tag in [ 'WORK_OF_ART', 'PRODUCT']:#'DATE', 'TIME', 'EVENT'\n",
    "        return 'Technology'\n",
    "    else:\n",
    "        return 'Unknown'  # 혹은 다른 기본값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b65840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to SSAN_SP_DF and GPT_SP_DF2\n",
    "SSAN_SP_DF['source_ner'] = SSAN_SP_DF['source_ner_3'].apply(map_ner_to_category)\n",
    "SSAN_SP_DF['target_ner'] = SSAN_SP_DF['target_ner_3'].apply(map_ner_to_category)\n",
    "\n",
    "GPT_SP_DF2['source_ner'] = GPT_SP_DF2['source_ner_3'].apply(map_ner_to_category)\n",
    "GPT_SP_DF2['target_ner'] = GPT_SP_DF2['target_ner_3'].apply(map_ner_to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장\n",
    "SSAN_SP_DF[(SSAN_SP_DF['source_ner_3']!='Unknown')&(SSAN_SP_DF['target_ner_3']!='Unknown')].to_csv('SSAN_SP_DF_after_norm.csv', index=False)\n",
    "GPT_SP_DF[(GPT_SP_DF['source_ner_4']!='Unknown')&(GPT_SP_DF['target_ner_4']!='Unknown')].to_csv('GPT_SP_DF_after_norm.csv', index=False)\n",
    "GPT_SP_DF2[(GPT_SP_DF2['source_ner_4']!='Unknown')&(GPT_SP_DF2['target_ner_4']!='Unknown')].to_csv('GPT_SP_DF_after_norm2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e381dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter 'Unknown' values in the 'source_ner_4' and the 'target_ner_4' columns\n",
    "GPT_SP_DF2 = GPT_SP_DF2[(GPT_SP_DF2['source_ner_4']!='Unknown')&(GPT_SP_DF2['target_ner_4']!='Unknown')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8f13f",
   "metadata": {},
   "source": [
    "## 전처리된 DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAN_SP_DF = pd.read_csv('SSAN_SP_DF_after_norm.csv')\n",
    "GPT_SP_DF = pd.read_csv('GPT_SP_DF_after_norm.csv')\n",
    "GPT_SP_DF2 = pd.read_csv('GPT_SP_DF_after_norm2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642b4ec",
   "metadata": {},
   "source": [
    "전처리된 데이터프레임을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고유한 엔티티 쌍의 개수를 구합니다.\n",
    "unique_entity_pairs = SSAN_SP_DF.drop_duplicates(subset=['source', 'target'])\n",
    "number_of_unique_pairs = unique_entity_pairs.shape[0]\n",
    "\n",
    "print(f\"Number of unique entity pairs: {number_of_unique_pairs}\")\n",
    "\n",
    "# 고유한 엔티티 쌍의 개수를 구합니다.\n",
    "unique_entity_pairs = GPT_SP_DF.drop_duplicates(subset=['source', 'target'])\n",
    "number_of_unique_pairs = unique_entity_pairs.shape[0]\n",
    "\n",
    "print(f\"Number of unique entity pairs: {number_of_unique_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303542d8",
   "metadata": {},
   "source": [
    "'SSAN_SP_DF' 데이터프레임에는 10252개의 고유한 entity pair가 있고 'GPT_SP_DF' 데이터프레임에는 25326개의 고유한 entity pair가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of unique \"edge\" values in SSAN_SP_DF: {SSAN_SP_DF[\"edge\"].nunique()}')\n",
    "print(f'number of unique \"relation\" values in GPT_SP_DF: {GPT_SP_DF[\"relation\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423f9a6",
   "metadata": {},
   "source": [
    "* SSAN_SP_DF의 relation별 갯수를 막대 그래프로 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot relation type counts in descending order for SSAN_SP_DF\n",
    "# 'relation' 열에서 각 관계의 종류별로 개수를 세고, 내림차순으로 정렬합니다.\n",
    "relation_counts = SSAN_SP_DF['edge'].value_counts().sort_values()\n",
    "\n",
    "# 가로 막대 그래프를 그립니다.\n",
    "plt.figure(figsize=(15, 20))  # 그래프의 크기를 조정합니다.\n",
    "relation_counts.plot(kind='barh',color='slategray')  # 수평 막대 그래프\n",
    "plt.title('Relation Type Counts in Descending Order')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Relation Type')\n",
    "\n",
    "# 긴 레이블을 다루기 위해 x축 레이블의 각도를 조정할 수 있습니다.\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout()  # 레이아웃을 조정하여 레이블이 잘리지 않도록 합니다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52835c",
   "metadata": {},
   "source": [
    "* GPT_SP_DF의 relation별 갯수를 막대 그래프로 시각화 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot relation type counts in descending order for GPT_SP_DF\n",
    "# 'relation' 열에서 각 관계의 종류별로 개수를 세고, 내림차순으로 정렬합니다.\n",
    "relation_counts = GPT_SP_DF['relation'].value_counts().sort_values()\n",
    "\n",
    "# 가로 막대 그래프를 그립니다.\n",
    "plt.figure(figsize=(15, 20))  # 그래프의 크기를 조정합니다.\n",
    "relation_counts.plot(kind='barh',color='slategray')  # 수평 막대 그래프\n",
    "plt.title('Relation Type Counts in Descending Order')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Relation Type')\n",
    "\n",
    "# 긴 레이블을 다루기 위해 x축 레이블의 각도를 조정할 수 있습니다.\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout()  # 레이아웃을 조정하여 레이블이 잘리지 않도록 합니다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임계값에 따른 네트워크 정보를 계산하는 함수\n",
    "def network_info(df, source_col, target_col):\n",
    "    # 그래프 생성\n",
    "    # convert tabular data into a graph format rows = edges\n",
    "    G = nx.from_pandas_edgelist(df, source=source_col, target=target_col)\n",
    "    # 네트워크 정보 계산\n",
    "    # calculate number of nodes and edges\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    # computes the density of the graph\n",
    "    density = nx.density(G)\n",
    "    # calculate average degree of the graph\n",
    "    avg_degree = sum(dict(G.degree()).values()) / num_nodes\n",
    "    # determine the number of connected components in the graph\n",
    "    num_components = nx.number_connected_components(G)\n",
    "    \n",
    "    return {\n",
    "        'num_nodes': num_nodes,\n",
    "        'num_edges': num_edges,\n",
    "        'density': density,\n",
    "        'average_degree': avg_degree,\n",
    "        'connected_components': num_components\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605365f",
   "metadata": {},
   "source": [
    "* SSAN_SP_DF 데이터프레임의 네트워크 정보를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9114519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SSAN_SP_DF\n",
    "# 각 임계값에 따른 네트워크 정보 추출\n",
    "# initialize an empty dictionary\n",
    "network_stats = {}\n",
    "thresholds = ['95', '90', '85']\n",
    "for t in thresholds:\n",
    "    network_stats[f'{t}%'] = network_info(SSAN_SP_DF, f'source_rep_{t}', f'target_rep_{t}')\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "network_stats_df = pd.DataFrame(network_stats).T\n",
    "network_stats_df.reset_index(inplace=True)\n",
    "network_stats_df.rename(columns={'index': 'Raw'}, inplace=True)\n",
    "\n",
    "network_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576000f",
   "metadata": {},
   "source": [
    "* GPT_SP_DF 데이터프레임의 네트워크 정보를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPT_SP_DF\n",
    "# 각 임계값에 따른 네트워크 정보 추출\n",
    "network_stats = {}\n",
    "thresholds = ['95', '90', '85']\n",
    "for t in thresholds:\n",
    "    network_stats[f'{t}%'] = network_info(GPT_SP_DF, f'source_rep_{t}', f'target_rep_{t}')\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "network_stats_df = pd.DataFrame(network_stats).T\n",
    "network_stats_df.reset_index(inplace=True)\n",
    "network_stats_df.rename(columns={'index': 'Raw'}, inplace=True)\n",
    "\n",
    "network_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2e9ce",
   "metadata": {},
   "source": [
    "* GPT_SP_DF2 데이터프레임의 네트워크 정보를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GPT_SP_DF2\n",
    "# 각 임계값에 따른 네트워크 정보 추출\n",
    "network_stats = {}\n",
    "thresholds = ['95', '90', '85']\n",
    "for t in thresholds:\n",
    "    network_stats[f'{t}%'] = network_info(GPT_SP_DF2, f'source_rep_{t}', f'target_rep_{t}')\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "network_stats_df = pd.DataFrame(network_stats).T\n",
    "network_stats_df.reset_index(inplace=True)\n",
    "network_stats_df.rename(columns={'index': 'Raw'}, inplace=True)\n",
    "\n",
    "network_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62f777",
   "metadata": {},
   "source": [
    "* Similarity threshold별 representative words의 분포를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 임계값 리스트 (이 부분은 실제 임계값 리스트로 대체해야 함)\n",
    "thresholds = [95, 90, 85, 80, 75, 70, 65, 60]\n",
    "\n",
    "# 예시 데이터프레임 생성 (이 부분은 실제 데이터프레임으로 대체해야 함)\n",
    "# SSAN_SP_DF = ...\n",
    "\n",
    "# 그래프 생성\n",
    "fig, ax = plt.subplots(figsize=(15, 10))  # 전체 그래프 크기 조정\n",
    "# y축에 해당하는 고유 대표 단어들을 저장하는 리스트\n",
    "y_axis_words = []\n",
    "\n",
    "# 각 임계값에 대해 반복\n",
    "for threshold in thresholds:\n",
    "    # unique_representative 값 설정\n",
    "    unique_representatives = pd.unique(SSAN_SP_DF[f'source_rep_{threshold}'].tolist() + \n",
    "                                       SSAN_SP_DF[f'target_rep_{threshold}'].tolist())\n",
    "    # y축에 매핑\n",
    "    y_positions = pd.Series(range(len(unique_representatives)), index=unique_representatives)\n",
    "\n",
    "    # y축 단어 리스트에 추가\n",
    "    y_axis_words.extend(unique_representatives)\n",
    "    \n",
    "    # 데이터 프레임 생성\n",
    "    df_mapped = SSAN_SP_DF[[f'source_rep_{threshold}', f'target_rep_{threshold}']].melt(var_name='type', value_name='Representative')\n",
    "    df_mapped['y_position'] = df_mapped['Representative'].map(y_positions)\n",
    "    \n",
    "    # 데이터 프레임 sorting\n",
    "    sorted_df = df_mapped.sort_values(by=['y_position', 'Representative'])\n",
    "    sorted_df = sorted_df.reset_index(drop=True)\n",
    "    \n",
    "    # 그래프에 데이터 추가 (각 임계값에 대해 다른 색상 사용)\n",
    "    ax.scatter(sorted_df.index, sorted_df['y_position'], label=f'{threshold}% Similarity Threshold', s=1)\n",
    "\n",
    "# 축 레이블 및 제목 설정\n",
    "ax.set_xlabel('Words before Normalization')\n",
    "ax.set_ylabel('Representative Words (Groups)')\n",
    "ax.set_title('Representative Words at Various Similarity Thresholds [SSAN_SP_DF]')\n",
    "\n",
    "\n",
    "# 레전드 설정 (점 크기 증가)\n",
    "ax.legend(markerscale=5, scatterpoints=1, fontsize='medium')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"combined_similarity_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daeb7f5",
   "metadata": {},
   "source": [
    "* Similarity threshold별 representative words의 분포를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 임계값 리스트 (이 부분은 실제 임계값 리스트로 대체해야 함)\n",
    "thresholds = [100, 95, 90, 85, 80, 75, 70, 65]\n",
    "\n",
    "GPT_SP_DF['source_rep_100']= GPT_SP_DF['source']\n",
    "GPT_SP_DF['target_rep_100']= GPT_SP_DF['target']\n",
    "\n",
    "# 예시 데이터프레임 생성 (이 부분은 실제 데이터프레임으로 대체해야 함)\n",
    "# SSAN_SP_DF = ...\n",
    "\n",
    "# 그래프 생성\n",
    "fig, ax = plt.subplots(figsize=(70, 20))  # 전체 그래프 크기 조정\n",
    "# y축에 해당하는 고유 대표 단어들을 저장하는 리스트\n",
    "y_axis_words = []\n",
    "\n",
    "# 각 임계값에 대해 반복\n",
    "for threshold in thresholds:\n",
    "    unique_representatives = pd.unique(GPT_SP_DF['source'].tolist() + \n",
    "                                       GPT_SP_DF['target'].tolist())\n",
    "    y_positions = pd.Series(range(len(unique_representatives)), index=unique_representatives)\n",
    "\n",
    "    # y축 단어 리스트에 추가\n",
    "    y_axis_words.extend(unique_representatives)\n",
    "    \n",
    "    df_mapped = GPT_SP_DF[[f'source_rep_{threshold}', f'target_rep_{threshold}']].melt(var_name='type', value_name='Representative')\n",
    "    df_mapped['y_position'] = df_mapped['Representative'].map(y_positions)\n",
    "    \n",
    "    # sorted_df = df_mapped.sort_values(by=['y_position', 'Representative'])\n",
    "\n",
    "    sorted_df = df_mapped.reset_index(drop=True)\n",
    "    \n",
    "    # 그래프에 데이터 추가 (각 임계값에 대해 다른 색상 사용)\n",
    "    ax.scatter(sorted_df.index, sorted_df['y_position'], label=f'{threshold}% Similarity Threshold', s=1)\n",
    "\n",
    "# 축 레이블 및 제목 설정\n",
    "ax.set_xlabel('Words before Normalization')\n",
    "ax.set_ylabel('Representative Words (Groups)')\n",
    "ax.set_title('Representative Words at Various Similarity Thresholds')\n",
    "\n",
    "\n",
    "# 레전드 설정 (점 크기 증가)\n",
    "ax.legend(markerscale=5, scatterpoints=1, fontsize='medium')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"GPT_combined_similarity_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7486c1a",
   "metadata": {},
   "source": [
    "* Similarity threshold별 representative words의 변화를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 임계값 리스트 (실제 임계값 리스트로 대체해야 함)\n",
    "thresholds = [100, 95, 90, 85, 80, 75, 70, 65]\n",
    "\n",
    "# 예시 데이터프레임 생성 (실제 데이터프레임으로 대체해야 함)\n",
    "# GPT_SP_DF = ...\n",
    "\n",
    "# 그래프 생성\n",
    "fig, ax = plt.subplots(figsize=(20, 15))  # 전체 그래프 크기 조정\n",
    "\n",
    "# 각 임계값에 대해 반복\n",
    "for threshold in thresholds:\n",
    "    # 대표 단어와 원본 단어를 매핑하는 데이터프레임 생성\n",
    "    df_mapped = GPT_SP_DF[['source', f'source_rep_{threshold}', 'target', f'target_rep_{threshold}']].melt(var_name='type', value_name='Representative')\n",
    "\n",
    "    # 단어와 대표 단어의 관계를 x, y 값으로 사용\n",
    "    if threshold == 100:\n",
    "        # 임계값이 100일 경우, 단어는 자신을 대표하므로 y=x 선을 따라 표시\n",
    "        ax.plot(list(set(GPT_SP_DF['source'].to_list()+GPT_SP_DF['target'].to_list())), list(set(GPT_SP_DF['source'].to_list()+GPT_SP_DF['target'].to_list())), 'o', label=f'{threshold}% Similarity Threshold', markersize=1)\n",
    "    else:\n",
    "        # 그 외의 경우, 원본 단어의 인덱스와 대표 단어의 인덱스를 사용\n",
    "        df_mapped['Word_Index'] = df_mapped.groupby('Representative').ngroup()\n",
    "        ax.scatter(df_mapped['Word_Index'], df_mapped['Representative'], label=f'{threshold}% Similarity Threshold', s=1)\n",
    "\n",
    "# 레전드 및 제목 설정\n",
    "ax.legend(markerscale=5, scatterpoints=1, fontsize='medium')\n",
    "ax.set_title('Word Representation Change with Different Similarity Thresholds')\n",
    "ax.set_xlabel('Word Index')\n",
    "ax.set_ylabel('Representative Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fdd05",
   "metadata": {},
   "source": [
    "* threshold별 word transformation을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold별로 column 이름 리스트 저장\n",
    "source_cols = [f\"source_rep_{t}\" for t in thresholds]\n",
    "target_cols = [f\"target_rep_{t}\" for t in thresholds]\n",
    "\n",
    "transformation_data = []\n",
    "\n",
    "# threshold loop\n",
    "for t in thresholds:\n",
    "    for index, row in GPT_SP_DF.iterrows():\n",
    "        # Source transformations\n",
    "        transformation_data.append({\n",
    "            \"Word\": row[\"source\"],\n",
    "            \"Transformed_Word\": row[f\"source_rep_{t}\"],\n",
    "            \"Threshold\": t,\n",
    "            \"Type\": \"Source\"\n",
    "        })\n",
    "        \n",
    "        # Target transformations\n",
    "        transformation_data.append({\n",
    "            \"Word\": row[\"target\"],\n",
    "            \"Transformed_Word\": row[f\"target_rep_{t}\"],\n",
    "            \"Threshold\": t,\n",
    "            \"Type\": \"Target\"\n",
    "        })\n",
    "\n",
    "# Creating a DataFrame from the list of dictionaries\n",
    "transformations = pd.DataFrame(transformation_data)\n",
    "\n",
    "# Plotting the transformations\n",
    "# ... (plotting code remains the same)\n",
    "\n",
    "\n",
    "# Plotting the transformations\n",
    "plt.figure(figsize=(100, 100))\n",
    "sns.scatterplot(data=transformations, x=\"Word\", y=\"Transformed_Word\", hue=\"Threshold\", style=\"Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Word Transformations Across Different Thresholds\")\n",
    "plt.xlabel(\"Original Word\")\n",
    "plt.ylabel(\"Transformed Word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411d18e",
   "metadata": {},
   "source": [
    "* Similarity threshold별 word pair mapping의 변화를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data loading (replace this with your actual data loading)\n",
    "# SSAN_SP_DF = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Combine 'source' and 'target' columns for x-axis\n",
    "SSAN_SP_DF['words_before'] = SSAN_SP_DF['source'] + ' + ' + SSAN_SP_DF['target']\n",
    "\n",
    "# Create a figure for the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Iterate over each threshold and plot the data\n",
    "thresholds = [80, 75, 70, 65, 60]\n",
    "for threshold in thresholds:\n",
    "    # Combine the representative words for the current threshold\n",
    "    SSAN_SP_DF[f'words_after_{threshold}'] = SSAN_SP_DF[f'source_rep_{threshold}'] + ' + ' + SSAN_SP_DF[f'target_rep_{threshold}']\n",
    "    \n",
    "    # Create a mapping of words_before to words_after\n",
    "    mapping = SSAN_SP_DF.groupby('words_before')[f'words_after_{threshold}'].unique().apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    # Plot the data\n",
    "    ax.scatter(mapping.index, mapping, label=f'{threshold}% Similarity Threshold', s=1)\n",
    "\n",
    "ax.set_xlabel('Original Words (source + target)')\n",
    "ax.set_ylabel('Mapped Words (source_rep + target_rep)')\n",
    "ax.set_xticklabels(SSAN_SP_DF['words_before'], rotation=90, fontsize=5)\n",
    "ax.legend()\n",
    "ax.set_title('Change in Representative Words Across Different Similarity Thresholds')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"threshold_comparison_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bbeb0",
   "metadata": {},
   "source": [
    "* Similarity threshold별로 normalizing 후 단어들의 변화를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Extracting the thresholds and preparing the dataframe for plotting\n",
    "thresholds = [95, 90, 85, 80, 75, 70, 65, 60]\n",
    "\n",
    "# Merging source and target words before and after normalization\n",
    "df['words_before'] = df['source']  + df['target']\n",
    "for threshold in thresholds:\n",
    "    df[f'words_after_{threshold}'] = df[f'source_rep_{threshold}'] + ' ' + df[f'target_rep_{threshold}']\n",
    "\n",
    "# Preparing for plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plotting the data for each threshold\n",
    "for threshold in thresholds:\n",
    "    # Extracting words before and after for the current threshold\n",
    "    words_before = df['words_before'].str.split().explode().unique()\n",
    "    words_after = df[f'words_after_{threshold}'].str.split().explode().unique()\n",
    "    \n",
    "    # Mapping words_before to their index to align them on the x-axis\n",
    "    x_positions = pd.Series(range(len(words_before)), index=words_before)\n",
    "    \n",
    "    # Mapping words_after to the index of words_before to get their y-axis position\n",
    "    y_positions = pd.Series(x_positions.index, index=x_positions.values).loc[words_after].values\n",
    "    \n",
    "    # Plotting\n",
    "    ax.plot(x_positions, y_positions, marker='o', linestyle='', label=f'{threshold}% Similarity Threshold')\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Words Before Normalization')\n",
    "ax.set_ylabel('Words After Normalization (by similarity threshold)')\n",
    "ax.set_title('Word Changes Across Different Similarity Thresholds')\n",
    "\n",
    "# Adding a legend\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7f534",
   "metadata": {},
   "source": [
    "* Similarity threshold별 word normalization을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 생성\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# 각 임계값에 대해 반복\n",
    "for threshold in thresholds:\n",
    "    source_col = f'source_rep_{threshold}'\n",
    "    target_col = f'target_rep_{threshold}'\n",
    "\n",
    "    # source 및 target 정규화된 단어 그리기\n",
    "    ax.scatter(SSAN_SP_DF['source'], SSAN_SP_DF[source_col], label=f'Source {threshold}% Similarity', alpha=0.5)\n",
    "    ax.scatter(SSAN_SP_DF['target'], SSAN_SP_DF[target_col], label=f'Target {threshold}% Similarity', alpha=0.5)\n",
    "\n",
    "# 축 레이블 및 제목 설정\n",
    "ax.set_xlabel('Words before Normalization')\n",
    "ax.set_ylabel('Words after Normalization')\n",
    "ax.set_title('Word Normalization at Various Similarity Thresholds')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"word_normalization_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f99aaa",
   "metadata": {},
   "source": [
    "* Similarity threshold별 'SSAN_SP_DF' 데이터프레임의 representative grouping의 분포를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dcd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 겹치지 않게 모든 데이터프레임 시각화\n",
    "fig, axs = plt.subplots(len(thresholds), 1, figsize=(15, 40))  # 그래프 크기 조정\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    print(threshold)\n",
    "    # 임계값별로 대표 단어에 대한 y 위치를 조정합니다.\n",
    "    unique_representatives = pd.unique(SSAN_SP_DF[f'source_rep_{threshold}'].tolist() + \n",
    "                                       SSAN_SP_DF[f'target_rep_{threshold}'].tolist())\n",
    "    y_positions = pd.Series(range(len(unique_representatives)), index=unique_representatives)\n",
    "    \n",
    "    # 모든 source와 target의 대표 단어를 매핑합니다.\n",
    "    df_mapped = SSAN_SP_DF[[f'source_rep_{threshold}', f'target_rep_{threshold}']].melt(var_name='type', value_name='Representative')\n",
    "    df_mapped['y_position'] = df_mapped['Representative'].map(y_positions)\n",
    "\n",
    "\n",
    "     # 데이터를 그룹화하고, 각 그룹별로 정렬합니다.\n",
    "    sorted_df = df_mapped.sort_values(by=['y_position', 'Representative'])\n",
    "    sorted_df = sorted_df.reset_index(drop=True)\n",
    "    # 각 임계값에 대한 대표 단어의 분포를 시각화\n",
    "    axs[i].scatter(sorted_df.index, sorted_df['y_position'], label=f'{threshold}% Similarity Threshold', s=1)\n",
    "    axs[i].set_yticks(y_positions)\n",
    "    axs[i].set_yticklabels(y_positions.index, fontsize=5)\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(f'Representative Words at {threshold}% Similarity Threshold')\n",
    "    axs[i].set_ylabel('Representative Words (Groups)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sample_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06dd4b",
   "metadata": {},
   "source": [
    "* 'SSAN_SP_DF' 데이터프레임의 edge 수를 그래프로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad019fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'edge' 열의 각 종류별 개수를 계산\n",
    "edge_counts = SSAN_SP_DF['edge'].value_counts()\n",
    "\n",
    "# 개수가 많은 순서대로 정렬\n",
    "edge_counts = edge_counts.sort_values(ascending=True)\n",
    "# 그래프 크기 설정\n",
    "plt.figure(figsize=(14,10))\n",
    "# 막대 그래프 시각화\n",
    "plt.barh(edge_counts.index, edge_counts.values)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Edge Type')\n",
    "plt.title('Relation Type Counts in Descending Order')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fef187",
   "metadata": {},
   "source": [
    "* Similarity threshold별로 unique entity 개수를 세고 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c79e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임계값별 entity 개수\n",
    "unique_entities_counts =[]\n",
    "thresholds = [95, 90, 85, 80, 75, 70, 65, 60]\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    #print(threshold)\n",
    "    # 임계값별로 대표 단어에 대한 y 위치를 조정합니다.\n",
    "    unique_representatives = pd.unique(SSAN_SP_DF[f'source_rep_{threshold}'].tolist() + \n",
    "                                       SSAN_SP_DF[f'target_rep_{threshold}'].tolist())\n",
    "    unique_entities_counts.append(len(unique_representatives))\n",
    "    print(f\"Threshold {threshold}%: {len(unique_representatives)} unique entities\")\n",
    "    # 선 그래프 생성\n",
    "thresholds_with_raw = ['Raw'] + [str(t) + '%' for t in thresholds]\n",
    "unique_entities_counts_with_raw = [9079] + unique_entities_counts\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(thresholds_with_raw, unique_entities_counts_with_raw, marker = 'o')\n",
    "plt.title('Number of Unique Entities by Similarity Threshold')\n",
    "plt.xlabel('Similarity Threshold (%)')\n",
    "plt.ylabel('Number of Unique Entities')\n",
    "plt.grid(True)\n",
    "plt.xticks(thresholds_with_raw)\n",
    "\n",
    "#plt.savefig('unique_entities_by_similarity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3a940",
   "metadata": {},
   "source": [
    "* Similarity threshold별 Graph complexity(노드의 수) 변화를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a767bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터 (실제 데이터로 교체 필요)\n",
    "similarity_thresholds = ['Raw', '95%', '90%', '85%', '80%', '75%', '70%', '65%', '60%']\n",
    "thresholds = ['Raw', 95, 90, 85, 80, 75, 70, 65, 60]  # 'Raw' 추가\n",
    "\n",
    "# 각 임계값에 대해 노드와 엣지의 수를 저장할 리스트 초기화\n",
    "number_of_nodes = []\n",
    "number_of_edges = []\n",
    "\n",
    "# 데이터 프레임 (실제 사용 시 SSAN_SP_DF 데이터프레임으로 대체)\n",
    "# SSAN_SP_DF = ...\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # 'Raw' 데이터에 대한 처리\n",
    "    if threshold == 'Raw':\n",
    "        G = nx.DiGraph()\n",
    "        for s, t in zip(SSAN_SP_DF['source'], SSAN_SP_DF['target']):\n",
    "            G.add_edge(s, t)\n",
    "    else:\n",
    "        # 각 임계값에 대한 대표 노드 칼럼을 가져옵니다.\n",
    "        source = SSAN_SP_DF[f'source_rep_{threshold}']\n",
    "        target = SSAN_SP_DF[f'target_rep_{threshold}']\n",
    "        G = nx.DiGraph()\n",
    "        for s, t in zip(source, target):\n",
    "            G.add_edge(s, t)\n",
    "\n",
    "    # 노드와 엣지의 수를 리스트에 추가\n",
    "    number_of_nodes.append(G.number_of_nodes())\n",
    "    number_of_edges.append(G.number_of_edges())\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# 막대 그래프 생성 (노드의 수)\n",
    "bars = ax1.bar(similarity_thresholds, number_of_nodes, color='lightblue')\n",
    "ax1.set_xlabel('Similarity Threshold (%)')\n",
    "ax1.set_ylabel('Number of Nodes', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# 바 속에 각 막대의 값 표시\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, yval - (yval*0.05), yval, ha='center', va='top', color='blue')\n",
    "\n",
    "# 두 번째 y축 추가 (엣지의 수)\n",
    "ax2 = ax1.twinx()  \n",
    "lines = ax2.plot(similarity_thresholds, number_of_edges, color='grey', marker='o', linestyle='-', linewidth=2)\n",
    "ax2.set_ylabel('Number of Edges', color='grey')  \n",
    "ax2.tick_params(axis='y', labelcolor='grey')\n",
    "\n",
    "# 점 위에 각 점의 값 표시\n",
    "for i, (xy, txt) in enumerate(zip(lines[0].get_xydata(), number_of_edges)):\n",
    "    ax2.annotate(txt, xy=xy, textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# 제목 추가\n",
    "plt.title('Graph Size Change by Similarity Threshold')\n",
    "\n",
    "# 그래프 저장\n",
    "plt.savefig('graph_size_change.png')\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017fbf3",
   "metadata": {},
   "source": [
    "* Similarity threshold별 silhouette coefficient를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette coefficient vs threshold 그래프\n",
    "# silhouette score, thresholds 지정\n",
    "silhouette_scores= [0.7684047, 0.76462686, 0.71767795, 0.56971604, 0.5110331, 0.4427302, 0.36146897, 0.29856825]\n",
    "thresholds = ['95%', '90%', '85%', '80%', '75%', '70%', '65%', '60%']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, silhouette_scores, marker='o')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.title('Silhouette Coefficient vs Threshold')\n",
    "plt.grid(True)\n",
    "\n",
    "# 결과 저장\n",
    "plt.savefig('silhouette_coefficient_vs_threshold.png')  # 저장 경로\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f1225",
   "metadata": {},
   "source": [
    "# 4. SSAN & GPT Graph Visualization\n",
    "\n",
    "SSAN 모델을 통해 분석한 데이터와 GPT를 통해 획득한 데이터를 EDA를 통해 특성을 파악합니다. 각각의 데이터프레임에서 threshold별, node별로 출발지와 목적지 정보를 기반으로 그래프를 생성하고 시각화하는 기능을 수행합니다. 이를 통해 supply chain network를 분석하고 시각화하는 기능을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab5db8",
   "metadata": {},
   "source": [
    "### Data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf596a",
   "metadata": {},
   "source": [
    "데이터 파일을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAN_SP_DF = pd.read_csv('SSAN_SP_DF_after_norm.csv')\n",
    "GPT_SP_DF2 = pd.read_csv('GPT_SP_DF_after_norm2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239810e",
   "metadata": {},
   "source": [
    "필요 없는 열을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffabc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSAN_SP_DF = SSAN_SP_DF.drop(['source_ner_2','target_ner_2','source_ner_3','target_ner_3'],axis=1)\n",
    "GPT_SP_DF2 = GPT_SP_DF2.drop(['para','source_ner_2','target_ner_2','source_ner_3','target_ner_3','source_ner_4','target_ner_4','sorted_pair'],axis=1) #GPT_SP_DF2에서 'para','source_ner_2','target_ner_2','source_ner_3','target_ner_3','source_ner_4','target_ner_4','sorted_pair'열 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683e344",
   "metadata": {},
   "source": [
    "dc_check: 방향성이 있는 그래프를 생성하고 degree_centrality를 측정하여 중심성을 기준으로 내림차순으로 정리합니다.<br> 상위 30개 노드 선별 하고 각 카테고리에 대한 상위 30개 노드 정보 포함하는 dataframe을 생성하여 digraph와 생성한 dataframe를 return합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ccdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#방향성이 있는 그래프를 생성\n",
    "def dc_check(fin_df, edge_name):\n",
    "    # Creating a directed graph(방향성이 있는 그래프 생성)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Adding edges from the DataFrame\n",
    "    for _, row in fin_df.iterrows():\n",
    "        G.add_edge(row['source'], row['target'], edge=row[edge_name]) #edge 추가\n",
    "\n",
    "    # Calculate centrality measures (e.g., degree centrality) 노드에 각 degree centrality(각 노드가 다른 노드랑 얼마나 연결되어 있는가)측정 \n",
    "    centrality = nx.degree_centrality(G)\n",
    "\n",
    "    # Preparing to categorize nodes by their NER tags, \n",
    "    # #디폴트값이 list인 딕션너리 생성\n",
    "    node_categories = defaultdict(list)\n",
    "\n",
    "    # Categorizing nodes\n",
    "    for node in G.nodes:\n",
    "        # Finding the node in the DataFrame and categorizing it, \n",
    "        # #해당하는 변수에 배당\n",
    "        if node in fin_df['source'].values:\n",
    "            category = fin_df[fin_df['source'] == node]['source_ner'].values[0]\n",
    "        elif node in fin_df['target'].values:\n",
    "            category = fin_df[fin_df['target'] == node]['target_ner'].values[0]\n",
    "        else:\n",
    "            category = 'Unknown'\n",
    "        \n",
    "        node_categories[category].append(node)\n",
    "\n",
    "    # # Sorting nodes in each category by centrality, \n",
    "    # 중심성을 기준으로 내림차순으로 정리 -> 상위 30개 노드 선별\n",
    "    top_centrality_by_category = {category: sorted(nodes, key=lambda x: centrality[x], reverse=True)[:30]\n",
    "                                for category, nodes in node_categories.items()}\n",
    "\n",
    "    # top_centrality_by_category\n",
    "    # Converting the top_centrality_by_category dictionary to a DataFrame, \n",
    "    # 카테고리에 대한 상위 30개 노드 정보 포함하는 DF\n",
    "    df_top_centrality = pd.DataFrame([{\"Category\": category, \"Node\": node, \"Centrality\": centrality[node]}\n",
    "                                    for category, nodes in top_centrality_by_category.items()\n",
    "                                    for node in nodes])\n",
    "\n",
    "    return G, df_top_centrality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aee77c",
   "metadata": {},
   "source": [
    "import한 데이터프레임인 SSAN_SP_DF와 GPT_SP_DF2를 df_check함수에 대입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssan_G, ssan_dc_df = dc_check(SSAN_SP_DF, 'edge')\n",
    "gpt_G, gpt_dc_df = dc_check(GPT_SP_DF2, 'relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08448945",
   "metadata": {},
   "source": [
    "결과값인 ssan_dc_df와 gpt_dc_df를 엑셀로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea510422",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssan_dc_df.to_excel('ssan_raw_dc.xlsx',index = False)\n",
    "gpt_dc_df.to_excel('gpt_raw_dc.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f60fb",
   "metadata": {},
   "source": [
    "dc_check_threshold: dc_check과 동일한 매커니즘이지만 입력값으로 thre(:threshold 값)를 하나 더 입력 받아 thre별로 dc_check 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_check_threshold(fin_df, thre, edge_name):\n",
    "    # Creating a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Adding edges from the DataFrame\n",
    "    for _, row in fin_df.iterrows():\n",
    "        G.add_edge(row['source_rep_{}'.format(thre)], row['target_rep_{}'.format(thre)], edge=row[edge_name])\n",
    "\n",
    "    # Calculate centrality measures (e.g., degree centrality)\n",
    "    centrality = nx.degree_centrality(G)\n",
    "\n",
    "    # Preparing to categorize nodes by their NER tags\n",
    "    node_categories = defaultdict(list)\n",
    "\n",
    "    # Categorizing nodes\n",
    "    for node in G.nodes:\n",
    "        # Finding the node in the DataFrame and categorizing it\n",
    "        if node in fin_df['source_rep_{}'.format(thre)].values:\n",
    "            category = fin_df[fin_df['source_rep_{}'.format(thre)] == node]['source_ner'].values[0]\n",
    "        elif node in fin_df['target_rep_{}'.format(thre)].values:\n",
    "            category = fin_df[fin_df['target_rep_{}'.format(thre)] == node]['target_ner'].values[0]\n",
    "        else:\n",
    "            category = 'Unknown'\n",
    "        \n",
    "        node_categories[category].append(node)\n",
    "\n",
    "    # Sorting nodes in each category by centrality\n",
    "    top_centrality_by_category = {category: sorted(nodes, key=lambda x: centrality[x], reverse=True)[:30]\n",
    "                                for category, nodes in node_categories.items()}\n",
    "\n",
    "    # top_centrality_by_category\n",
    "    # Converting the top_centrality_by_category dictionary to a DataFrame\n",
    "    df_top_centrality = pd.DataFrame([{\"Category\": category, \"Node\": node, \"Centrality\": centrality[node]}\n",
    "                                    for category, nodes in top_centrality_by_category.items()\n",
    "                                    for node in nodes])\n",
    "\n",
    "    return G, df_top_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a6a5e",
   "metadata": {},
   "source": [
    "SSAN_SP_DF와 GPT_SP_DF2를 dc_check_threshold를 thre(threshold)별로 수행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssan_G_95, ssan_dc_df_95= dc_check_threshold(SSAN_SP_DF,95, 'edge')\n",
    "gpt_G_95, gpt_dc_df_95 = dc_check_threshold(GPT_SP_DF2,95, 'relation')\n",
    "\n",
    "ssan_G_90, ssan_dc_df_90= dc_check_threshold(SSAN_SP_DF,90, 'edge')\n",
    "gpt_G_90, gpt_dc_df_90 = dc_check_threshold(GPT_SP_DF2,90, 'relation')\n",
    "\n",
    "ssan_G_85, ssan_dc_df_85= dc_check_threshold(SSAN_SP_DF,85, 'edge')\n",
    "gpt_G_85, gpt_dc_df_85 = dc_check_threshold(GPT_SP_DF2,85, 'relation')\n",
    "\n",
    "ssan_G_80, ssan_dc_df_80= dc_check_threshold(SSAN_SP_DF,80, 'edge')\n",
    "gpt_G_80, gpt_dc_df_80 = dc_check_threshold(GPT_SP_DF2,80, 'relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a579e7e",
   "metadata": {},
   "source": [
    "statistic1: node_count: node수 측정, edge_count: edge 수 측정, density: 밀도 측정, connected_components: 그래프에서 약한 구성요소의 수를 계산하여 node_count, edge_count, density, average_degree를 각각 출력하고 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic1(filtered_G):\n",
    "# 노드 수와 엣지 수\n",
    "    node_count = filtered_G.number_of_nodes()\n",
    "    edge_count = filtered_G.number_of_edges()\n",
    "\n",
    "    # 그래프 밀도\n",
    "    density = nx.density(filtered_G)\n",
    "\n",
    "    # 평균 차수\n",
    "    average_degree = sum(dict(filtered_G.degree()).values()) / node_count\n",
    "\n",
    "    # 연결된 구성 요소 수\n",
    "    connected_components = nx.number_weakly_connected_components(filtered_G)\n",
    "\n",
    "    print('node_count: ',node_count,'\\nedge_count: ', edge_count,'\\ndensity: ', density, \n",
    "          '\\naverage degree: ',average_degree,'\\nconnected components: ', connected_components, \n",
    "          )\n",
    "    return node_count, edge_count, density, average_degree, connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a92fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic1(ssan_G)\n",
    "statistic1(ssan_G_95)\n",
    "statistic1(ssan_G_90)\n",
    "statistic1(ssan_G_85)\n",
    "\n",
    "statistic1(gpt_G)\n",
    "statistic1(gpt_G_95)\n",
    "statistic1(gpt_G_90)\n",
    "statistic1(gpt_G_85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0d498",
   "metadata": {},
   "source": [
    "dc_check_threshold의 각 출력값을 엑셀로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ae153",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssan_dc_df_95.to_excel('ssan_95_dc.xlsx',index = False)\n",
    "gpt_dc_df_95.to_excel('gpt_95_dc.xlsx',index = False)\n",
    "\n",
    "ssan_dc_df_90.to_excel('ssan_90_dc.xlsx',index = False)\n",
    "gpt_dc_df_90.to_excel('gpt_90_dc.xlsx',index = False)\n",
    "\n",
    "ssan_dc_df_85.to_excel('ssan_85_dc.xlsx',index = False)\n",
    "gpt_dc_df_85.to_excel('gpt_85_dc.xlsx',index = False)\n",
    "\n",
    "ssan_dc_df_80.to_excel('ssan_80_dc.xlsx',index = False)\n",
    "gpt_dc_df_80.to_excel('gpt_80_dc.xlsx',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b264bc9",
   "metadata": {},
   "source": [
    "SP_DF 데이터프레임 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35225f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#열 이름 변경: relation -> edge\n",
    "GPT_SP_DF2.rename(columns={'relation': 'edge'}, inplace=True)\n",
    "#SSAN_SP_DF와 GPT_SP_DF2를 행 방향으로 연결해서 새로운 DF 생성\n",
    "SP_DF = pd.concat([SSAN_SP_DF, GPT_SP_DF2], ignore_index=True)\n",
    "#SP_DF 중복행 제거 후 인덱스 재설정\n",
    "SP_DF = SP_DF.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26366584",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_DF = SP_DF.drop_duplicates(subset=['source', 'target'], keep='first').reset_index(drop=True) #SP_DF에서 'source' 및 'target' 열을 기준으로 중복된 행을 제거 -> 동일한 행이 있는 경우 첫 번째 발생만 유지되고 나머지 중복 행이 제거 -> 새로운 인덱스 생성\n",
    "SP_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5a0ef",
   "metadata": {},
   "source": [
    "SP_DF 데이터프레임을 thre(threshold) 별로 dc_check 함수를 통해 그래프 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_G, com_dc_df = dc_check(SP_DF, 'edge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_G_95, com_dc_df_95= dc_check_threshold(SP_DF,95, 'edge')\n",
    "\n",
    "com_G_90, com_dc_df_90= dc_check_threshold(SP_DF,90, 'edge')\n",
    "\n",
    "com_G_85, com_dc_df_85= dc_check_threshold(SP_DF,85, 'edge')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d64eee",
   "metadata": {},
   "source": [
    "dc_check 함수 출력값인 thre(threshold) 별 각 그래프를 statistic1 함수를 통해 데이터 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee261f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic1(com_G)\n",
    "statistic1(com_G_95)\n",
    "statistic1(com_G_90)\n",
    "statistic1(com_G_85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf982d",
   "metadata": {},
   "source": [
    "SSAN_SP_DF의 thre(threshold) 별 교집합을 벤다이어그램을 통해 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSAN_SP_DF에 'source'와 'target' 열에서 얻은 두 리스트의 합집합을 사용하여 집합을 생성\n",
    "setA = set(SSAN_SP_DF['source'].to_list()+SSAN_SP_DF['target'].to_list())\n",
    "#SSAN_SP_DF에 'source_rep_95'와 'target_rep_95' 열에서 얻은 두 리스트의 합집합을 사용하여 집합을 생성\n",
    "setB = set(SSAN_SP_DF['source_rep_95'].to_list()+SSAN_SP_DF['target_rep_95'].to_list())\n",
    "#SSAN_SP_DF에 'source_rep_05'와 'target_rep_90' 열에서 얻은 두 리스트의 합집합을 사용하여 집합을 생성\n",
    "setC = set(SSAN_SP_DF['source_rep_90'].to_list()+SSAN_SP_DF['target_rep_90'].to_list())\n",
    "\n",
    "# 벤 다이어그램 그리기\n",
    "#venn3 함수를 이용하여 벤다이어그램 생성 -> setA, setB, setC라는 세 개의 집합을 가정하고 이들의 교집합을 Venn 다이어그램으로 시각화\n",
    "venn3([setA, setB, setC], ('SSAN', 'ChatGPT-small spacy', 'Set C-large spacy'))\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "setA = set(SSAN_SP_DF['source_rep_90'].to_list()+SSAN_SP_DF['target_rep_90'].to_list())\n",
    "setB = set(SSAN_SP_DF['source_rep_85'].to_list()+SSAN_SP_DF['target_rep_85'].to_list())\n",
    "setC = set(SSAN_SP_DF['source_rep_80'].to_list()+SSAN_SP_DF['target_rep_80'].to_list())\n",
    "\n",
    "# 벤 다이어그램 그리기\n",
    "venn3([setA, setB, setC], ('SSAN', 'ChatGPT-small spacy', 'Set C-large spacy'))\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f6533",
   "metadata": {},
   "source": [
    "Graph_and_Visual: 각 노드간의 관계 그래프를 시각화하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a825c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_and_Visual(fin_df, source, target, isin_ner_li, Title, pos):\n",
    "    #Country' 또는 'Firm'으로 태그된 노드만 포함하는 행을 필터링(예시)\n",
    "    #fin_df에서 'source_ner' 및 'target_ner' 열의 값이 리스트 isin_ner_li에 있는 경우에 해당하는 행만 선택\n",
    "    filtered_df = fin_df[(fin_df['source_ner'].isin(isin_ner_li)) & (fin_df['target_ner'].isin(isin_ner_li))]\n",
    "\n",
    "    # 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "    #from_pandas_edgelist 함수를 사용하여 필터링된 DataFrame(filtered_df)을 기반으로 유향 그래프(DiGraph)를 생성\n",
    "    #그래프는 DataFrame의 'source' 및 'target' 열을 사용하여 구성되며, 'edge' 열은 간선 속성으로 할당\n",
    "    filtered_G = nx.from_pandas_edgelist(filtered_df, source=source, target=target, edge_attr='edge', create_using=nx.DiGraph())\n",
    "    \n",
    "    # 색상 매핑 (이전에 정의된 'Country'와 'Firm'에 대한 색상만 사용)\n",
    "    #제공된 color_map_filtered 사전은 그래프에서 다른 노드 유형에 대한 노드 색상을 지정하는 데 사용\n",
    "    color_map_filtered = {'Country': 'royalblue', 'Firm': 'lightgreen','Resource': 'orange','Technology': 'darkred'}\n",
    "\n",
    "    # 필터링된 그래프의 노드에 NER 태그 속성 추가\n",
    "    # #filtered_df DataFrame의 행을 반복하며, 각 행의 정보를 기반으로 filtered_G 그래프의 노드의 'ner' 속성을 업데이트\n",
    "\n",
    "    #filtered_df DataFrame의 각 행을 반복\n",
    "    for _, row in filtered_df.iterrows(): \n",
    "        #현재 행의 source 노드가 filtered_G 그래프의 노드 중에 있는지 확인\n",
    "        if row[source] in filtered_G.nodes:\n",
    "            #source 노드가 존재하는 경우 해당 노드의 'ner' 속성을 DataFrame에서 가져온 'source_ner' 값으로 업데이트\n",
    "            filtered_G.nodes[row[source]]['ner'] = row['source_ner']\n",
    "        if row[target] in filtered_G.nodes: \n",
    "            filtered_G.nodes[row[target]]['ner'] = row['target_ner']\n",
    "\n",
    "    # 노드 색상 할당 (필터링된 그래프에 대해)\n",
    "    #filtered_G 그래프의 각 노드에 대한 노드 색상 목록(filtered_node_colors)을 생성\n",
    "    #각 노드의 'ner' 속성을 기반으로 하며, 'ner' 속성을 특정 색상에 매핑하는 데 color_map_filtered 사전을 사용\n",
    "    filtered_node_colors = [color_map_filtered[filtered_G.nodes[node]['ner']] for node in filtered_G.nodes]\n",
    "\n",
    "    # 그래프 시각화\n",
    "    plt.figure(figsize=(60,60))\n",
    "    nx.draw(filtered_G, with_labels=True, node_color=filtered_node_colors, edge_color='gray', pos=pos, font_size=3, node_size=100)\n",
    "    plt.title(\"Supply Chain Graph ({})\".format(Title), fontdict={'fontsize': 100})\n",
    "    plt.savefig(\"SSAN_graph_{}.png\".format(source[-2:]))\n",
    "    plt.show()\n",
    "    return filtered_G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fe9c1",
   "metadata": {},
   "source": [
    "statistic: 앞서 선언한 statistic1 함수를 재구성(각 연결된 구성요소의 평균 최단 경로 길이 계산 추가)합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(filtered_G):\n",
    "    # 필터링된 그래프의 통계 정보 재계산 (지름 제외)\n",
    "\n",
    "    # 노드 수와 엣지 수\n",
    "    node_count = filtered_G.number_of_nodes()\n",
    "    edge_count = filtered_G.number_of_edges()\n",
    "\n",
    "    # 그래프 밀도\n",
    "    density = nx.density(filtered_G)\n",
    "\n",
    "    # 평균 차수\n",
    "    average_degree = sum(dict(filtered_G.degree()).values()) / node_count\n",
    "\n",
    "    # 연결된 구성 요소 수\n",
    "    connected_components = nx.number_weakly_connected_components(filtered_G)\n",
    "\n",
    "\n",
    "    # 각 연결된 구성 요소의 평균 최단 경로 길이 계산\n",
    "\n",
    "    average_path_lengths = []\n",
    "    #유향 그래프 filtered_G의 약하게 연결된 컴포넌트를 순회\n",
    "    for component in nx.weakly_connected_components(filtered_G):\n",
    "        #현재의 약하게 연결된 컴포넌트에 해당하는 부분 그래프를 추출\n",
    "        subgraph = filtered_G.subgraph(component)\n",
    "        # 서브그래프가 강하게 연결되어 있지 않더라도 계산 가능한지 확인\n",
    "        # 부분 그래프가 약하게 연결되어 있는지 확인\n",
    "        if nx.is_weakly_connected(subgraph):\n",
    "            # 모든 노드 쌍의 평균 최단 경로 길이 계산\n",
    "            try:\n",
    "                # 약하게 연결된 컴포넌트 내 모든 노드 쌍에 대한 최단 경로 길이를 계산\n",
    "                lengths = dict(nx.all_pairs_shortest_path_length(subgraph))\n",
    "                # 컴포넌트 내 모든 노드에 대한 최단 경로 길이의 총합을 계산\n",
    "                total_length = sum(sum(lengths[n].values()) for n in lengths)\n",
    "                # 약하게 연결된 컴포넌트에 대한 평균 경로 길이를 계산\n",
    "                avg_length = total_length / (len(subgraph) * (len(subgraph) - 1))\n",
    "                #평균 경로 길이를 average_path_lengths 리스트에 추가\n",
    "                average_path_lengths.append(avg_length) \n",
    "            except Exception as e:\n",
    "                # 오류 발생 시 0으로 설정\n",
    "                #계산 중 오류가 발생한 경우에 대비하여 0 값을 리스트에 추가하여 예외를 처리\n",
    "                average_path_lengths.append(0)\n",
    "\n",
    "    # 통계 정보 출력 (지름 제외)\n",
    "    print('node_count: ',node_count,'\\nedge_count: ', edge_count,'\\ndensity: ', density, \n",
    "          '\\naverage degree: ',average_degree,'\\nconnected components: ', connected_components, \n",
    "          '\\naverage of average_path_lengths: ',sum(average_path_lengths)/len(average_path_lengths))\n",
    "    return node_count, edge_count, density, average_degree, connected_components, sum(average_path_lengths)/len(average_path_lengths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8f5bc",
   "metadata": {},
   "source": [
    "원본를 데이터 복사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a99541",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = SSAN_SP_DF.copy()\n",
    "fin_df2 = GPT_SP_DF2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518f4f6",
   "metadata": {},
   "source": [
    "spring_layout()를 이용하여 노드의 위치를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#추가했음(방법2)\n",
    "pos = nx.spring_layout(ssan_G)\n",
    "pos_95 = nx.spring_layout(ssan_G_95)\n",
    "pos_90 = nx.spring_layout(ssan_G_90)\n",
    "pos_85 = nx.spring_layout(ssan_G_85)\n",
    "pos_80 = nx.spring_layout(ssan_G_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a13d29",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e9147",
   "metadata": {},
   "source": [
    "Graph_and_Visual 함수를 통해 thre(threshold) 별 노드 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21bd09",
   "metadata": {},
   "source": [
    "### Country-Firm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ce0bf",
   "metadata": {},
   "source": [
    "country와 firm 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15574928",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Country','Firm'], 'Country and Firm',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Country','Firm'], 'Country and Firm',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Country','Firm'], 'Country and Firm',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Country','Firm'], 'Country and Firm',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65654b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Country','Firm'], 'Country and Firm',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ad753",
   "metadata": {},
   "source": [
    "### Country-Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a1521",
   "metadata": {},
   "source": [
    "country 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Country'], 'Country',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b41250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Country'], 'Country',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4520fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Country'], 'Country',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01152d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Country'], 'Country',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a083ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Country'], 'Country',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02767fd",
   "metadata": {},
   "source": [
    "### Country-Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83de08",
   "metadata": {},
   "source": [
    "country와 resource 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Country','Resource'], 'Country and Resource',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f06199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Country','Resource'], 'Country and Resource',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Country','Resource'], 'Country and Resource',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06396dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Country','Resource'], 'Country and Resource',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546148b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Country','Resource'], 'Country and Resource',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b16a93",
   "metadata": {},
   "source": [
    "### Country-Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b85a7",
   "metadata": {},
   "source": [
    "country와 technology 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Country','Technology'], 'Country and Technology',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da059a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Country','Technology'], 'Country and Technology',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d870c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Country','Technology'], 'Country and Technology',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7674f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Country','Technology'], 'Country and Technology',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Country','Technology'], 'Country and Technology',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c346be",
   "metadata": {},
   "source": [
    "### Firm-Firm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716cae47",
   "metadata": {},
   "source": [
    "firm 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Firm'], 'Firm',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78310ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Firm'], 'Firm',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74149fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Firm'], 'Firm',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19436f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Firm'], 'Firm',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Firm'], 'Firm',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091256a",
   "metadata": {},
   "source": [
    "### Firm-Resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da04cdf",
   "metadata": {},
   "source": [
    "firm과 resource 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Firm','Resource'], 'Firm and Resource',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Resource','Firm'], 'Firm and Resource',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Resource','Firm'], 'Firm and Resource',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12153df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Resource','Firm'], 'Firm and Resource',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Resource','Firm'], 'Firm and Resource',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be16991",
   "metadata": {},
   "source": [
    "### Firm-Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cf3e3",
   "metadata": {},
   "source": [
    "firm과 technology 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df, 'source', 'target', ['Firm','Technology'], 'Firm and Technology',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', ['Technology','Firm'], 'Firm and Technology',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a02ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', ['Technology','Firm'], 'Firm and Technology',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', ['Technology','Firm'], 'Firm and Technology',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df, 'source_rep_80', 'target_rep_80', ['Technology','Firm'], 'Firm and Technology',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5781dd",
   "metadata": {},
   "source": [
    "### Resource-Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#추가했음(방법2)\n",
    "pos = nx.spring_layout(gpt_G)\n",
    "pos_95 = nx.spring_layout(gpt_G_95)\n",
    "pos_90 = nx.spring_layout(gpt_G_90)\n",
    "pos_85 = nx.spring_layout(gpt_G_85)\n",
    "pos_80 = nx.spring_layout(gpt_G_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890be270",
   "metadata": {},
   "source": [
    "resouce 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf12e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df2, 'source', 'target', ['Resource'], 'Resource',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_95', 'target_rep_95', ['Resource'], 'Resource',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_90', 'target_rep_90', ['Resource'], 'Resource',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_85', 'target_rep_85', ['Resource'], 'Resource',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_80', 'target_rep_80', ['Resource'], 'Resource',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61b6df",
   "metadata": {},
   "source": [
    "### Resource-Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef9265",
   "metadata": {},
   "source": [
    "resouce와 technology 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e430938",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df2, 'source', 'target', ['Resource','Technology'], 'Resource and Technology',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_95', 'target_rep_95', ['Resource','Technology'], 'Resource and Technology',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc179ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_90', 'target_rep_90', ['Resource','Technology'], 'Resource and Technology',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50657571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_85', 'target_rep_85', ['Resource','Technology'], 'Resource and Technology',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ec931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_80', 'target_rep_80', ['Resource','Technology'], 'Resource and Technology',pos_80)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d577104",
   "metadata": {},
   "source": [
    "### Technology-Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e451e4c",
   "metadata": {},
   "source": [
    "technology 사이 thre(threshold) 별 관계그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51197c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph_and_Visual(fin_df2, 'source', 'target', ['Technology'], 'Technology',pos) #수정\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_95', 'target_rep_95', ['Technology'], 'Technology',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_90', 'target_rep_90', ['Technology'], 'Technology',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_85', 'target_rep_85', ['Technology'], 'Technology',pos_85) #추가\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80\n",
    "G = Graph_and_Visual(fin_df2, 'source_rep_80', 'target_rep_80', ['Technology'], 'Technology',pos_80) #추가\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86ba57",
   "metadata": {},
   "source": [
    "Entire_Graph_and_Visual: 모든 요소 별 방향성이 있는 그래프를 생성하고 그리는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entire_Graph_and_Visual(fin_df, source, target, Title, pos):\n",
    "    # 특정 열만 필터링 할 필요X -> 전체 열을 고려하는 그래프를 생성하기 때문문\n",
    "    \n",
    "    # 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "    filtered_G = nx.from_pandas_edgelist(fin_df, source=source, target=target, edge_attr='edge', create_using=nx.DiGraph())\n",
    "    \n",
    "    # 색상 매핑 (이전에 정의된 'Country'와 'Firm'에 대한 색상만 사용)\n",
    "    color_map_filtered = {'Country': 'royalblue', 'Firm': 'lightgreen','Resource': 'orange','Technology': 'darkred', 'Unknown':'gray'}\n",
    "\n",
    "    # 필터링된 그래프의 노드에 NER 태그 속성 추가\n",
    "    for _, row in fin_df.iterrows():\n",
    "        if row[source] in filtered_G.nodes:\n",
    "            filtered_G.nodes[row[source]]['ner'] = row['source_ner']\n",
    "        if row[target] in filtered_G.nodes:\n",
    "            filtered_G.nodes[row[target]]['ner'] = row['target_ner']\n",
    "\n",
    "    # 노드 색상 할당 (필터링된 그래프에 대해)\n",
    "    filtered_node_colors = [color_map_filtered[filtered_G.nodes[node]['ner']] for node in filtered_G.nodes]\n",
    "\n",
    "    # 그래프 시각화\n",
    "    plt.figure(figsize=(60,60))\n",
    "    nx.draw(filtered_G, with_labels=True, node_color=filtered_node_colors, edge_color='gray', pos=pos, font_size=3, node_size=100)\n",
    "    plt.title(\"Supply Chain Graph ({})\".format(Title), fontdict={'fontsize': 100})\n",
    "    plt.legend(['node 1', 'node 2', 'edge'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.savefig('network_{}.png'.format(Title))\n",
    "    return filtered_G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62e75",
   "metadata": {},
   "source": [
    "Entire_Graph_and_Visual을 이용하여 thre(threshold) 별로 전체 그래프를 그립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_raw = nx.from_pandas_edgelist(fin_df, source='source', target='target', edge_attr='edge', create_using=nx.DiGraph())\n",
    "pos = nx.spring_layout(G_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Entire_Graph_and_Visual(fin_df, 'source', 'target', 'Raw', pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_95 = nx.from_pandas_edgelist(fin_df, source='source_rep_95', target='target_rep_95', edge_attr='edge', create_using=nx.DiGraph()) #수정\n",
    "pos_95 = nx.spring_layout(G_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Entire_Graph_and_Visual(fin_df, 'source_rep_95', 'target_rep_95', '95%', pos_95) #수정\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_90 = nx.from_pandas_edgelist(fin_df, source='source_rep_90', target='target_rep_90', edge_attr='edge', create_using=nx.DiGraph()) #수정\n",
    "pos_90 = nx.spring_layout(G_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Entire_Graph_and_Visual(fin_df, 'source_rep_90', 'target_rep_90', '90%', pos_90) #수정\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f75036",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_85 = nx.from_pandas_edgelist(fin_df, source='source_rep_85', target='target_rep_85', edge_attr='edge', create_using=nx.DiGraph()) #수정\n",
    "pos_85 = nx.spring_layout(G_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Entire_Graph_and_Visual(fin_df, 'source_rep_85', 'target_rep_85', '85%', pos_85) #수정\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad866367",
   "metadata": {},
   "source": [
    "# 5. Finding the Shortest Path\n",
    "\n",
    "pickle 파일을 통해 각각의 그래프를 로드합니다. 각 그래프의 타겟으로 하는 노드마다의 도달가능한 노드와 최단경로를 계산하고 시각화합니다. 이후 소스노드와 타겟노드간의 네트워크 분석과 네트워크 시뮬레이션을 수행합니다. 시뮬레이션은 노드간의 새로운 엣지를 추가하여 새로운 최단경로를 계산하는 과정입니다. 마지막으로 네트워크 내에서 중요한 역할을 하는 노드(영향력있는 노드) 식별합니다. 영향력 있는 노드는 도달성 분석을 통해 특정 노드에서 다른 노드들에게 얼마나 많은 영향을 미칠 수 있는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5487b",
   "metadata": {},
   "source": [
    "SSAN_SP_DF와 GPT_SP_DF2의 교집합을 벤다이어그램을 통해 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9814a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "setA = set(SSAN_SP_DF['source'].to_list()+SSAN_SP_DF['target'].to_list())\n",
    "setB = set(GPT_SP_DF2['source'].to_list()+GPT_SP_DF2['target'].to_list())\n",
    "\n",
    "# 벤 다이어그램 그리기\n",
    "venn2([setA, setB], ('SSAN', 'RE ChatGPT'))\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1351098",
   "metadata": {},
   "source": [
    " 'SSAN_SP_DF'와 'GPT_SP_DF2'가 각각 'source' 및 'target' 열을 가진 데이터프레임입니다.<br>각 데이터프레임에서 'source'와 'target'의 쌍을 기반으로 벤 다이어그램을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f67045",
   "metadata": {},
   "outputs": [],
   "source": [
    "setA = set(list(zip(SSAN_SP_DF['source'], SSAN_SP_DF['target'])))\n",
    "setB = set(list(zip(GPT_SP_DF2['source'], GPT_SP_DF2['target'])))\n",
    "\n",
    "# 벤 다이어그램 그리기\n",
    "venn2([setA, setB], ('SSAN', 'RE ChatGPT'))\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70701862",
   "metadata": {},
   "source": [
    " 'SSAN_SP_DF' 및 'GPT_SP_DF2'의 'source', 'target', 'edge' 열을 조합하여 튜플을 만든 후 이를 기반으로 집합을 생성합니다. <br> 이후 이러한 집합 간의 교집합을 시각화하기 위해 벤 다이어그램을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setA = set(list(zip(SSAN_SP_DF['source'], SSAN_SP_DF['target'], SSAN_SP_DF['edge'])))\n",
    "setB = set(list(zip(GPT_SP_DF2['source'], GPT_SP_DF2['target'], GPT_SP_DF2['edge'])))\n",
    "\n",
    "# 벤 다이어그램 그리기\n",
    "venn2([setA, setB], ('SSAN', 'RE ChatGPT'))\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad457d5d",
   "metadata": {},
   "source": [
    "SP_DF를 기반으로 thre(threshold) 기반으로 필터링된 데이터프레임(raw_df, norm_95_df, norm_90_df, norm_85_df, norm_80_df)을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thre 별로 source 열과 target 열이 다르면 source와 target 열 rename, 'source_ner'과 'target_ner'이 'unknown'이 아닌 것 중에 중복인 행 제거\n",
    "raw_df = SP_DF[SP_DF['source'] != SP_DF['target']][['source','target','edge','company','source_ner','target_ner']][(SP_DF['source_ner']!='Unknown')&(SP_DF['target_ner']!='Unknown')].drop_duplicates(subset=['source', 'target'], keep='first').reset_index(drop=True)\n",
    "norm_95_df = SP_DF[SP_DF['source_normalized_95'] != SP_DF['target_normalized_95']][['source_normalized_95','target_normalized_95','edge','company','source_ner','target_ner']].rename(columns={'source_normalized_95': 'source','target_normalized_95': 'target'})[(SP_DF['source_ner']!='Unknown')&(SP_DF['target_ner']!='Unknown')].drop_duplicates(subset=['source', 'target'], keep='first').reset_index(drop=True)\n",
    "norm_90_df = SP_DF[SP_DF['source_normalized_90'] != SP_DF['target_normalized_90']][['source_normalized_90','target_normalized_90','edge','company','source_ner','target_ner']].rename(columns={'source_normalized_90': 'source','target_normalized_90': 'target'})[(SP_DF['source_ner']!='Unknown')&(SP_DF['target_ner']!='Unknown')].drop_duplicates(subset=['source', 'target'], keep='first').reset_index(drop=True)\n",
    "norm_85_df = SP_DF[SP_DF['source_normalized_85'] != SP_DF['target_normalized_85']][['source_normalized_85','target_normalized_85','edge','company','source_ner','target_ner']].rename(columns={'source_normalized_85': 'source','target_normalized_85': 'target'})[(SP_DF['source_ner']!='Unknown')&(SP_DF['target_ner']!='Unknown')].drop_duplicates(subset=['source', 'target'], keep='first').reset_index(drop=True)\n",
    "norm_80_df = SP_DF[SP_DF['source_normalized_80'] != SP_DF['target_normalized_80']][['source_normalized_80','target_normalized_80','edge','company','source_ner','target_ner']].rename(columns={'source_normalized_80': 'source','target_normalized_80': 'target'})[(SP_DF['source_ner']!='Unknown')&(SP_DF['target_ner']!='Unknown')].drop_duplicates(subset=['source', 'target'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70392d21",
   "metadata": {},
   "source": [
    "처리한 데이터를 thre(threshold) 별로 csv파일로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e96884",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.to_csv('ALL_df.csv',index = False)\n",
    "norm_95_df.to_csv('ALL_df_95.csv',index = False)\n",
    "norm_90_df.to_csv('ALL_df_90.csv',index = False)\n",
    "norm_85_df.to_csv('ALL_df_85.csv',index = False)\n",
    "norm_80_df.to_csv('ALL_df_80.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277909f4",
   "metadata": {},
   "source": [
    "thre(threshold) 별로 필터링한 데이터를 dc_check 함수를 통해 그래프 생성하고 degree_centrality를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, dc_df = dc_check(raw_df, 'edge')\n",
    "\n",
    "G_95, dc_df_95= dc_check(norm_95_df, 'edge')\n",
    "\n",
    "G_90, dc_df_90= dc_check(norm_90_df, 'edge')\n",
    "\n",
    "G_85, dc_df_85= dc_check(norm_85_df, 'edge')\n",
    "\n",
    "G_80, dc_df_80= dc_check(norm_80_df, 'edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7f4bd",
   "metadata": {},
   "source": [
    "dc_check return 값 중 하나인 dc_df를 csv 파일로 extract합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dc_df.to_csv('raw_dc',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dac148",
   "metadata": {},
   "source": [
    "statistic1 함수를 이용하여 node_count: node수 측정, edge_count: edge 수 측정, density: 밀도 측정, connected_components: 그래프에서 약한 구성요소의 수를 측정하여 각각을 출력하고 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08959e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic1(G)\n",
    "statistic1(G_95)\n",
    "statistic1(G_90)\n",
    "statistic1(G_85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079d557",
   "metadata": {},
   "source": [
    "dc_df_85에서 필요한 열만 추출합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Category' 열이 'Resource'인 dc_df_85을 필터링\n",
    "# 'Node' 및 'Centrality' 열을 선택하고 데이터프레임을 표시할 때 인덱스를 숨기는 스타일을 적용\n",
    "try:\n",
    "    dc_df_hide = dc_df_85[dc_df_85['Category']=='Resource'][['Node','Centrality']].style.hide_index()\n",
    "except:\n",
    "    dc_df_hide = dc_df_85[dc_df_85['Category']=='Resource'][['Node','Centrality']].style.hide(axis='index')\n",
    "\n",
    "dc_df_hide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7e08b",
   "metadata": {},
   "source": [
    "pickle 파일을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle 파일로부터 그래프 로드\n",
    "with open('graph.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "with open('graph_95.pickle', 'rb') as f:\n",
    "    G_95 = pickle.load(f)\n",
    "\n",
    "with open('graph_90.pickle', 'rb') as f:\n",
    "    G_90 = pickle.load(f)\n",
    "\n",
    "with open('graph_85.pickle', 'rb') as f:\n",
    "    G_85 = pickle.load(f)\n",
    "\n",
    "with open('graph_80.pickle', 'rb') as f:\n",
    "    G_80 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece59083",
   "metadata": {},
   "source": [
    "norm_80_df에서 필요한 열만 select합니다.(확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53674bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_80_df[(norm_80_df['source']=='January 15 and July 15 of each year')|(norm_80_df['target']=='January 15 and July 15 of each year')]\n",
    "norm_80_df[(norm_80_df['source']=='2,821')|(norm_80_df['target']=='2,821')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd9cd5",
   "metadata": {},
   "source": [
    "그래프 G에서 특정 소스 노드로부터 도달 가능한 노드 및 최단 경로를 찾는 코드입니다.<br>\n",
    " (*전체 데이터를 출력하시려면 밑에 주석 처리된 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19703a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_node = 'Tesla, Inc.'\n",
    "# 특정 노드에서 도달할 수 있는 모든 노드 찾기\n",
    "reachable_nodes = nx.descendants(G, source_node)\n",
    "\n",
    "# 특정 노드에서 각 노드까지의 최단 경로 찾기\n",
    "shortest_paths = {node: nx.shortest_path(G, source_node, node) for node in reachable_nodes}\n",
    "\n",
    "# 결과 출력\n",
    "#print(\"Reachable Nodes:\", reachable_nodes)\n",
    "#print(\"Shortest Paths:\", shortest_paths)   \n",
    "print(\"Reachable Nodes:\", list(reachable_nodes)[:10])  # 처음 10개 노드만 출력\n",
    "print(\"Shortest Paths:\", {node: shortest_paths[node] for node in list(reachable_nodes)[:10]})  # 처음 10개 노드의 최단 경로만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91342402",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''실제 코드 입니다.\n",
    "source_node = 'Tesla, Inc.'\n",
    "# 특정 노드에서 도달할 수 있는 모든 노드 찾기\n",
    "reachable_nodes = nx.descendants(G, source_node)\n",
    "\n",
    "# 특정 노드에서 각 노드까지의 최단 경로 찾기\n",
    "shortest_paths = {node: nx.shortest_path(G, source_node, node) for node in reachable_nodes}\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Reachable Nodes:\", reachable_nodes)\n",
    "print(\"Shortest Paths:\", shortest_paths)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7866cc2",
   "metadata": {},
   "source": [
    "find_path: 그래프 G와 가능한 소스노드의 목록을 받아 각 소스에 대해 실제 소스노드를 찾은 다음 해당 소스에서 child까지의 최단 경로를 계산하는 함수입니다.\n",
    "<br>결과는 shortest_paths라는 딕셔너리에 저장되며 함수에 의해 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(G, possible_sources):\n",
    "    shortest_paths = {}\n",
    "    # 그래프 노드 중에서 실제 존재하는 source_node 찾기\n",
    "    for source in possible_sources:\n",
    "        actual_source = next((node for node in G.nodes() if node == source), None)\n",
    "        if actual_source:\n",
    "            reachable_nodes = nx.descendants(G, actual_source)\n",
    "            tmp_shortest_paths = {node: nx.shortest_path(G, actual_source, node) for node in reachable_nodes}\n",
    "            \n",
    "            print(\"Exist\", actual_source)\n",
    "            shortest_paths = {**shortest_paths, **tmp_shortest_paths}\n",
    "        # print(source)\n",
    "        \n",
    "    return shortest_paths\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f660f30",
   "metadata": {},
   "source": [
    "material_li: 다양한 소재의 이름을 포함하는 리스트를 정의합니다.\n",
    "<br> materials는 material_li를 이용하여 각 소재 이름을 소문자로 변환하여 새로운 리스트 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e537ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "material_li = ['Lead', 'Zinc Oxide', 'Stoneware', 'Diabase', 'Bone China', 'Sulfuric Acid', 'Bauxite Alumina', 'Diesel', \n",
    "               'Diopside', 'Benzene', 'Selenium', 'Boric Acid', 'Pearl', 'Peat', 'Onyx', 'Perlite', 'Fiberglass Resin', \n",
    "               'Iceland Spar', 'Fluorine', 'Spinel', 'Thorianite', 'Cement', 'Turpentine', 'Apatite', 'Petalite', 'Steatite', \n",
    "               'Synthetic Rubber', 'Beryllium', 'Coal Tar', 'Slate', 'Marcasite', 'Gelatin', 'Expanded Polystyrene', 'Glycerol',\n",
    "                'Beryllium Copper', 'Spinel Gemstone', 'Rubber', 'Vanadinite', 'Vivianite', 'Chloroform', 'Pyrex', 'Pyrolusite',\n",
    "                'Baddeleyite', 'Fullerene', 'Iron Pyrites', 'Peridot', 'Bromargyrite', 'Potassium Hydroxide', 'Limonite', 'Zinc',\n",
    "                'Niobium', 'Osmium', 'Pumice', 'Scolecite', 'Taconite', 'Siderite', 'Liquid Nitrogen', 'Galena Lead', 'Sodium Chloride',\n",
    "                'Vanadium', 'Silica Gel', 'Isopropanol', 'Lacquer', 'Aluminium Oxide', 'Cuprite', 'Dolomitic Limestone', 'Bornite', 'Biofuels', \n",
    "                'Cassiterite', 'Biotite Mica', 'Cobaltite', 'Tremolite', 'Mendelevium', 'Thorium', 'Fermium', 'Phlogopite Mica', 'Halite', \n",
    "                'Jasper', 'Roentgenium', 'Flerovium', 'Propane', 'Pyrolusite Manganese', 'Iolite', 'Dysprosium', 'Nephrite', 'Quartz', 'Neodymium', \n",
    "                'Nanomaterials', 'Praseodymium', 'Granite', 'Chromite', 'Vermiculite', 'Hydrogen', 'Hematite', 'PVA (Polyvinyl Alcohol)', 'Aquamarine', \n",
    "                'Thallium', 'Darmstadtium', 'Flint', 'Ceramics', 'Cryolite Aluminum', 'Corundum', 'Rosin', 'Actinium', 'Leucite', 'Fluorapatite', \n",
    "                'Muscovite', 'Magnetite Iron', 'Meitnerium', 'Carbon Black', 'Hydrogen Peroxide', 'Iron', 'Tungstate', 'Cryogenite', \n",
    "                'Oganesson', 'Oxygen', 'Cryolite', 'Citrine', 'Polybutadiene', 'Sodium Hydroxide', 'Chlorine', 'Basalt', 'Amber', 'Mica Schist', \n",
    "                'Quartzite', 'Siderite Iron', 'Chromite Ore', 'Soap', 'Brick', 'Silimanite', 'Methyl Isobutyl Ketone', 'Magnesia', 'Talc Mineral', \n",
    "                'Sulfur', 'Polyester', 'Plaster', 'Sand', 'Lapis Lazuli', 'Dye', 'Aragonite', 'Methanol', 'Magnetite Sand', 'Hydrochloric Acid', \n",
    "                'Chrysocolla', 'Calcite', 'Dubnium', 'Hydraulic Fluid', 'Periclase', 'Cerium', 'Phlogopite', 'Paraffin Wax', 'Trona', 'Epidote', \n",
    "                'Azurite', 'Livermorium', 'Diatomaceous Earth', 'Jet', 'Nitric Acid', 'Xenotime', 'Gold', 'Titanite', 'Graphite', 'Monazite', \n",
    "                'Sphalerite Zinc', 'Celestite', 'Cerussite', 'Uranium', 'Californium', 'Vesuvianite', 'Isobutane', 'Boron Nitride', 'Ruby', \n",
    "                'Helium', 'Thorite', 'Fatty Acids', 'Iron Slag', 'Bakelite', 'Topaz Gemstone', 'Columbite', 'Lepidolite', 'Prehnite', 'Zircon', \n",
    "                'Emerald', 'Hassium', 'Rhodonite', 'Ink', 'Rubidium', 'Neon', 'Realgar', 'Gabbro', 'Lawrencium', 'Steel', 'Fiberglass', 'Butane', \n",
    "                'Polystyrene', 'Orthoclase', 'Marble', 'Carbon', 'Promethium', 'Earthenware', 'Tetrafluoroethylene', 'Quartz Crystal', 'Propylene', \n",
    "                'Plagioclase', 'Coking Coal', 'Cinnabar Mercury', 'Chalcopyrite', 'Rayon', 'Astatine', 'Spandex', 'Opal', 'Bismaleimide', 'Acrylic', \n",
    "                'Sapphire', 'Moonstone Gem', 'Asphalt', 'Carbon Fiber', 'Triethanolamine', 'Glauberite', 'Coal Slag', 'Plutonium', 'Gypsum Alabaster', \n",
    "                'Almandine', 'Gallium', 'Moonstone', 'Spessartine', 'Pyrophyllite', 'Diamond', 'Quartz Sand', 'Bohrium', 'Turquoise', 'Copernicium', \n",
    "                'Technetium', 'Bromine', 'Wax', 'Nylon', 'Formaldehyde', 'Silver', 'Magnesium Oxide', 'Platinum', 'Polyurethane Foam', 'Fuel Oil', \n",
    "                'Toluene', 'Ethylene Glycol', 'Copper Sulfate', 'Limestone', 'Labradorite', 'Salt', 'Glycerine', 'Methanol Fuel', 'Mica', 'Naphtha', \n",
    "                'Bitumen', 'Iodine', 'Sulfur Crystal', 'Magnesite', 'Curium', 'Concrete', 'Radon', 'Petroleum', 'Antifreeze', 'Nickel', 'Greenockite', \n",
    "                'Scandium', 'Magnesite Magnesium', 'Uraninite', 'Sillimanite', 'Transformer Oil', 'Gypsum', 'Tanzanite Gem', 'Ytterbium', 'Goethite', 'Talcum', \n",
    "                'Fluorspar', 'Liquid Oxygen', 'Solder', 'Clay', 'Wollastonite', 'Terbium', 'Halloysite', 'Zeolite', 'Rhyolite', 'Ammonia', 'Cadmium', 'Jade', \n",
    "                'Molybdenite', 'Carbon Dioxide', 'Methyl Ethyl Ketone', 'Zirconium Dioxide', 'Fluorite', 'Cuprite Copper', 'Polyisoprene', 'Caprolactam', \n",
    "                'Soybean Oil', 'Feldspathoids', 'Germanium', 'Xenon', 'Malachite', 'Amino Resins', 'Tellurium', 'Smithsonite', 'Protactinium', 'Sphalerite', \n",
    "                'Iridium', 'Chalcocite', 'Ethylene', 'Serpentine', 'Tantalite', 'Barite', 'Phosphoric Acid', 'Orthoclase Feldspar', 'Celestine', 'Scheelite', \n",
    "                'Zirconium', 'Gadolinium', 'Alabaster', 'Limonite Iron', 'Vinyl Acetate', 'Styrene', 'Alumina', 'Rhodochrosite', 'Paint', 'Feldspar', 'Europium', \n",
    "                'Zinc Blende', 'Bentonite', 'Aniline', 'Polyethylene', 'Compressed Air', 'Nobelium', 'Kyanite', 'Titanium', 'Rutile Titanium', 'Palladium', 'Galena', \n",
    "                'Obsidian', 'Pisolite', 'Boron', 'Psilomelane', 'Neoprene', 'Sodium Carbonate', 'Riebeckite', 'Yttrium Oxide', 'Olivine', 'Hornblende', 'Sylvite', \n",
    "                'Malachite Green', 'Glucose', 'Kaolinite', 'Phenol', 'Topaz', 'Talc', 'Obsidian Glass', 'Tin', 'Borax', 'Cyclohexane', 'Glass Wool', \n",
    "                'Amethyst', 'Yttrium', 'Ferrite', 'Cryogenic Liquids', 'Magnetite', 'Nitrogen', 'Kerosene', 'Einsteinium', 'Kevlar', 'Aluminum', \n",
    "                'Cellulose', 'Cinnabar', 'Seaborgium', 'Sodium Silicate', 'Silicon', 'Staurolite', 'Strontianite', 'Sodalite', 'Bismuth', 'Erbium', \n",
    "                'Hematine', 'Opal Gemstone', 'Enamel', 'Phenacite', 'Arsenic', 'Natural Gas', 'Plasticizers', 'Holmium', 'Samarium', 'Ethane', 'Lithium', \n",
    "                'PVC', 'Bauxite', 'Polypropylene', 'Tungsten', 'Hydroboracite', 'Acetone', 'Vinyl Chloride', 'Detergent', 'Zincite', 'Montmorillonite', \n",
    "                'Silicone', 'Garnierite', 'Hauyne', 'Ethanol', 'Tennessine', 'Lactic Acid', 'Pitchblende', 'Andalusite', 'Oleum', 'Glass Fiber', 'Glass', \n",
    "                'Phosphorus', 'Furfural', 'Varnish', 'Cobalt', 'Lanthanum', 'Unakite', 'Nepheline', 'Staurolite Gem', 'Stibnite', 'Garnet', 'Urea', \n",
    "                'Porcelain', 'Octane', 'Neptunium', 'Microcline', 'Sorbitol', 'Mineral Spirits', 'Lutetium', 'Biotite', 'Monazite Rare Earths', \n",
    "                'Nihonium', 'Muscovite Mica', 'Thulium', 'Nitrogen Gas', 'Moscovium', 'Blue John', 'Solvents', 'Calcium Oxide', 'Chalk', 'Silica', \n",
    "                'Olivine Peridot', 'Orpiment', 'PVC Resin', 'Chromium', 'Schorl', 'Travertine', 'Dolostone', 'Coal', 'Wulfenite', 'Xylene', 'Hexane', \n",
    "                'Americium', 'Idocrase', 'Berkelium', 'Argon', 'Hafnium', 'Krypton', 'Pyrite', 'Strontium', 'Clinochlore', 'Detergent Chemicals', 'Methane', \n",
    "                'Manganese', 'Azomite', 'Ruthenium', 'Melamine', 'Agate', 'Molybdenum', 'Indium', 'Soapstone', 'Lazurite', 'Shale', 'Jet Fuel', 'Ceramic Fiber', \n",
    "                'Diatomite', 'Aspartame', 'Copper', 'Ulexite', 'Rutherfordium', 'Calcium Carbonate', 'Chert', 'Antimony', 'Gravel', 'Tourmaline', 'Ilmenite', \n",
    "                'Asbestos', 'Radium', 'Rhodium', 'Bioplastics', 'Mercury', 'Barium', 'Sodalite Group', 'Resorcinol', 'Dolomite', 'Milky Quartz', \n",
    "                'Terephthalic Acid', 'White Spirit', 'White Gold', 'Synthetic Sapphire', 'Polycarbonate','Silicone Rubber']\n",
    "materials = [x.lower() for x in material_li]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e11a0",
   "metadata": {},
   "source": [
    "find_path 함수를 사용하여 그래프에서 특정 출처로부터의 가장 짧은 경로를 찾고, 그 경로들을 필터링하여 특정 길이의 경로에 대한 정보를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Tesla, Inc.', 'Tesla','TSLA']\n",
    "\n",
    "#find_path 함수를 사용하여 각 출처에서의 가장 짧은 경로를 찾고, 결과를 shortest_paths 변수에 저장\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "# shortest_paths에서 각 재료가 1홉으로 이어진 경로만 필터링, filtered_paths: shortest_paths에서 특정 길이의 경로를 필터링\n",
    "# 먼저, 길이가 2인 경로를 필터링하여 path[1]에 해당하는 노드가 materials에 속하는지 확인\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 2 and any(material == path[1].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "\n",
    "#길이가 3인 경로를 필터링하여 path[2]에 해당하는 노드가 materials에 속하는지 확인\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b220cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Dow Inc.', 'Dow Jones Indices', 'The Dow Chemical Dow Inc.', 'Siam Polyethylene Dow Inc. Limited', 'Sadara Chemical Dow Inc.','The Kuwait Styrene Dow Inc.', 'Saudi Arabian Oil Dow Inc.', 'Dow Inc.' ]\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "# shortest_paths에서 각 재료가 1홉으로 이어진 경로만 필터링\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 2 and any(material == path[1].lower() for material in materials)\n",
    "    \n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f43fc3",
   "metadata": {},
   "source": [
    "데이터프레임에서 특정 조건을 만족하는 'source'와 'target' 열의 고유한 값을 추출합니다.(확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'source' 열에서 'dow'를 포함하는 경우(대소문자 무시) 해당되는 행들을 선택하고, 이들 중에서 'source' 열의 고유한 값들을 추출\n",
    "unique_values_source = raw_df[raw_df['source'].str.contains('dow', case=False)]['source'].unique()\n",
    "\n",
    "# 'target' 열에서 'dow'를 포함하는 경우(대소문자 무시) 해당되는 행들을 선택하고, 이들 중에서 'target' 열의 고유한 값들을 추출\n",
    "unique_values_target = raw_df[raw_df['target'].str.contains('dow', case=False)]['target'].unique()\n",
    "\n",
    "# 결과 출력\n",
    "print(set(list(unique_values_source) + list(unique_values_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf955bb",
   "metadata": {},
   "source": [
    "shortest_paths 딕셔너리에서 길이가 3이며 특정 조건을 만족하는 경로들을 필터링하고 그 결과를 출력합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a240d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ab3ba",
   "metadata": {},
   "source": [
    "그래프G에서 특정 출처로부터의 가장 짧은 경로를 찾고, 해당 경로를 필터링합니다.<br>\n",
    "exclude_nodes 리스트에 포함된 노드는 제외하고 경로에 포함된 노드 중에 materials 리스트에 속하는 재료가 있는 경우만을 필터링합니다.<br>\n",
    "이 과정을 source, thre(threshold) 별로 반복합니다.\n",
    "<br>(*모든 출력값을 원하시면 주석처리된 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_filtered_paths(filtered_paths):\n",
    "    count = 0\n",
    "    for key, path in filtered_paths.items():\n",
    "        if count >= 10:  # 처음 10개 항목까지만 출력\n",
    "            break\n",
    "        print(key, \":\", path)\n",
    "        count += 1\n",
    "\n",
    "possible_sources = ['Tesla, Inc.', 'Tesla','TSLA']\n",
    "exclude_nodes = ['sec']\n",
    "\n",
    "# G에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "filtered_paths = {\n",
    "    key: path for key, path in filtered_paths.items()\n",
    "    if not any(node.lower() in exclude_nodes for node in path)\n",
    "}\n",
    "print(\"Filtered Paths for G:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_95에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_95, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_95:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_90에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_90, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_90:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_85에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_85:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_80에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_80, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_80:\")\n",
    "print_filtered_paths(filtered_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "possible_sources = ['Tesla, Inc.', 'Tesla','TSLA']\n",
    "# 제거하고자 하는 노드 목록\n",
    "exclude_nodes = [ 'sec']\n",
    "\n",
    "\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "# shortest_paths에서 각 재료가 포함된 경로만 필터링\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "# filtered_paths에서 특정 노드를 포함하지 않는 경로만 필터링\n",
    "filtered_paths = {\n",
    "    key: path for key, path in filtered_paths.items()\n",
    "    if not any(node.lower() in exclude_nodes for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths = find_path(G_95, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths = find_path(G_90, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths =find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths =find_path(G_80, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b928bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_filtered_paths(filtered_paths):\n",
    "    count = 0\n",
    "    for key, path in filtered_paths.items():\n",
    "        if count >= 10:  # 처음 10개 항목까지만 출력\n",
    "            break\n",
    "        print(key, \":\", path)\n",
    "        count += 1\n",
    "\n",
    "possible_sources = ['Apple, Inc.','Apple', 'AAPL']\n",
    "\n",
    "# G에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"Filtered Paths for G:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_95에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_95, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_95:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_90에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_90, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_90:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_85에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_85:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_80에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_80, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_80:\")\n",
    "print_filtered_paths(filtered_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "possible_sources = ['Apple, Inc.','Apple', 'AAPL']\n",
    "\n",
    "\n",
    "shortest_paths = find_path(G, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "# shortest_paths에서 각 재료가 포함된 경로만 필터링\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths = find_path(G_95, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths =find_path(G_90, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths =find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths =find_path(G_80, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37da4e",
   "metadata": {},
   "source": [
    "특정 출처(여기서는 'Apple, Inc.', 'Apple', 'AAPL')로부터의 가장 짧은 경로를 찾고, 해당 경로를 특정 조건에 따라 필터링하여 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c224ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Apple, Inc.','Apple', 'AAPL']\n",
    "\n",
    "\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "# shortest_paths에서 각 재료가 1홉으로 이어진 경로만 필터링\n",
    "#길이가 2인 경로를 필터링하고, 해당 경로의 두 번째 노드가 materials 리스트에 속하는지 확인하여 filtered_paths에 저장\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 2 and any(material == path[1].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "\n",
    "#길이가 3인 경로를 필터링하고, 해당 경로의 세 번째 노드가 materials 리스트에 속하는지 확인하여 filtered_paths에 저장\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "\n",
    "#길이가 4인 경로를 필터링하고, 해당 경로의 네 번째 노드가 materials 리스트에 속하는지 확인하여 filtered_paths에 저장\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 4 and any(material == path[3].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589046c2",
   "metadata": {},
   "source": [
    "데이터프레임에서 'samsung'을 포함하는 'source' 또는 'target' 값을 고유하게 추출하는 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4045de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'source' 열에서 'samsung' 포함하는 고유값 추출\n",
    "#'source' 열에서 'samsung'을 포함하는 행을 선택하고, 해당 행들 중에서 'source' 열의 고유한 값들을 추출\n",
    "unique_values_source = raw_df[raw_df['source'].str.contains('samsung', case=False)]['source'].unique()\n",
    "\n",
    "#'target' 열에서 'samsung' 포함하는 고유값 추출\n",
    "#'target' 열에서 'samsung'을 포함하는 행을 선택하고, 해당 행들 중에서 'target' 열의 고유한 값들을 추출\n",
    "unique_values_target = raw_df[raw_df['target'].str.contains('samsung', case=False)]['target'].unique()\n",
    "\n",
    "# 결과 출력\n",
    "#'source'와 'target'에서 추출한 고유한 값들을 리스트로 만들고, 이를 합쳐서 중복을 제거한 후 출력\n",
    "print(list(unique_values_source) + list(unique_values_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79221c",
   "metadata": {},
   "source": [
    "'Samsung' 및 'Samsung Electronics Co., Ltd.'로부터의 가장 짧은 경로를 찾고, 해당 경로를 특정 길이에 따라 필터링하여 출력하는 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Samsung', 'Samsung Electronics Co., Ltd.']\n",
    "\n",
    "\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "# shortest_paths에서 각 재료가 1홉으로 이어진 경로만 필터링\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 2 # and any(material == path[1].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 3 # and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 4 #and any(material == path[3].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ba71d",
   "metadata": {},
   "source": [
    "DataFrame에서 'source' 또는 'target' 열이 'Samsung'인 행을 필터링합니다.(확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[(raw_df['source']=='Samsung')|(raw_df['target']=='Samsung')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333410b6",
   "metadata": {},
   "source": [
    "thre(threshold) 별로 find_path 함수를 사용하여 'Samsung' 및 'Samsung Electronics Co., Ltd.' 출처로부터의 최단 경로를 찾고, 해당 경로를 재료에 따라 필터링하여 결과를 출력합니다.<br>\n",
    "(*모든 출력값을 원하시면 밑에 주석 처리된 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_filtered_paths(filtered_paths):\n",
    "    count = 0\n",
    "    for key, path in filtered_paths.items():\n",
    "        if count >= 10:  # 처음 10개 항목까지만 출력\n",
    "            break\n",
    "        print(key, \":\", path)\n",
    "        count += 1\n",
    "\n",
    "possible_sources = ['Samsung', 'Samsung Electronics Co., Ltd.']\n",
    "\n",
    "# G에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"Filtered Paths for G:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_95에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_95, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_95:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_90에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_90, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_90:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_85에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_85:\")\n",
    "print_filtered_paths(filtered_paths)\n",
    "\n",
    "# G_80에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_80, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"\\nFiltered Paths for G_80:\")\n",
    "print_filtered_paths(filtered_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "possible_sources = ['Samsung', 'Samsung Electronics Co., Ltd.']\n",
    "\n",
    "shortest_paths=find_path(G, possible_sources) #최단경로 확보\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "# shortest_paths에서 각 재료가 포함된 경로만 필터링\n",
    "# 각 경로에 대해 해당 경로의 노드 중에서 materials 리스트에 속하는 재료가 포함되어 있는지 확인\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths=find_path(G_95, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths=find_path(G_90, possible_sources)\n",
    "# 'lithium' 또는 'Lithium'이 포함된 경로만 추출\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths=find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "\n",
    "shortest_paths=find_path(G_80, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6c406",
   "metadata": {},
   "source": [
    "최단 경로들 중에서 경로의 길이가 4인 경우(노드 수가 3인 경우)에 해당하는 노드 및 경로들을 선택하여 딕셔너리로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaa187",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_hop_3 = {node: path for node, path in shortest_paths.items() if len(path) - 1 == 3}\n",
    "#len(path)는 경로의 길이를 나타내며, 경로의 길이에서 1을 뺀 값이 3이 되는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46057fe",
   "metadata": {},
   "source": [
    "최단 경로들 중에서 경로의 길이가 4이며 (노드 수가 3인 경우), 해당 경로의 마지막 노드가 materials 리스트에 속하는 재료인 경우에 해당하는 노드 및 경로들을 선택하여 딕셔너리로 저장합니다. 이후 각 노드와 해당 노드에 이르는 최단 경로를 filtered_paths 딕셔너리에 저장합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#연습코드\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if len(path) == 4 #and any(material == path[3].lower() for material in materials)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57c7e2",
   "metadata": {},
   "source": [
    "방향성 그래프를 생성하고 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e517fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 노드 유형에 대한 색상을 지정합니다., 각 노드와 색상을 매핑하는 사전 정의\n",
    "node_color_map = {\n",
    "    'Company': 'green',\n",
    "    'Country': 'blue',\n",
    "    'Resource': 'orange',\n",
    "    'Technology': 'red'\n",
    "}\n",
    "\n",
    "# Extracting unique nodes and their types\n",
    "#'norm_85_df'에서 'source' 및 'target' 열을 사용하여 두 개의 데이터프레임(sources 및 targets)을 생성 열 이름은 각각 'node' 및 'type'으로 변경\n",
    "sources = norm_85_df[['source', 'source_ner']].rename(columns={'source': 'node', 'source_ner': 'type'}) \n",
    "targets = norm_85_df[['target', 'target_ner']].rename(columns={'target': 'node', 'target_ner': 'type'}) \n",
    "#'sources' 및 'targets' 데이터프레임을 연결하고 중복을 제거한 후 'node' 열을 인덱스로 설정\n",
    "all_nodes = pd.concat([sources, targets]).drop_duplicates().set_index('node') \n",
    "# Mapping of nodes to types\n",
    "#'all_nodes' DataFrame의 'type' 열을 사용하여 딕셔너리(node_types)를 생성\n",
    "node_types = all_nodes['type'].to_dict() \n",
    "\n",
    "# Initialize a directed graph\n",
    "H = nx.DiGraph() #방향성 그래프 생성, 초기화\n",
    "\n",
    "# Add edges to the graph\n",
    "#딕셔너리를 반복하며, 각 경로에 대해 그래프 H에 간선을 추가\n",
    "for source, targets in filtered_paths.items():\n",
    "    for i in range(len(targets) - 1):\n",
    "        H.add_edge(targets[i], targets[i + 1])\n",
    "\n",
    "# Set the layout for the graph\n",
    "# pos = nx.spring_layout(H, k=0.5, iterations=20)  # Adjust 'k' for distance between nodes\n",
    "\n",
    "\n",
    "# Set graph layout\n",
    "pos = nx.kamada_kawai_layout(H)\n",
    "\n",
    "# Draw nodes with colors based on their types\n",
    "#각 노드의 유형에 기반하여 노드 색상 목록(node_colors)을 작성\n",
    "node_colors = [node_color_map.get(node_types.get(node, 'gray'), 'gray') for node in H.nodes()]\n",
    "nx.draw_networkx_nodes(H, pos, node_color=node_colors, node_size=50)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(H, pos)\n",
    "\n",
    "# Draw labels below nodes\n",
    "#labels_pos = {node: (coords[0], coords[1] - 0.1) for node, coords in pos.items()}\n",
    "nx.draw_networkx_labels(H, pos, font_size=8)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f4d72",
   "metadata": {},
   "source": [
    "데이터프레임에서 'source' 또는 'target' 열에 'Shell'이라는 문자열을 포함하는 고유한 값을 찾고, 해당 값을 출력하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'source' 열에 'Shell'을 포함하는 행을 필터링하고, 그 중에서 고유한 값 찾아냄\n",
    "unique_values_source = raw_df[raw_df['source'].str.contains('Shell', case=False)]['source'].unique()\n",
    "\n",
    "#'target' 열에서 'Shell' 포함하는 고유값 추출\n",
    "unique_values_target = raw_df[raw_df['target'].str.contains('Shell', case=False)]['target'].unique()\n",
    "\n",
    "# 결과 출력, 집합으로 변환하여 중복제거 후\n",
    "print(set(list(unique_values_source) + list(unique_values_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468ae00",
   "metadata": {},
   "source": [
    "그래프 (G_85)에 가능한 소스 노드 목록에서 다른 노드로의 가장 짧은 경로를 찾은 다음 경로는 특정 조건과 일치하는 경우에만 필터링합니다.<br> (*실제 데이터를 원하시면 밑에 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07762e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_filtered_paths(filtered_paths):\n",
    "    count = 0\n",
    "    for key, path in filtered_paths.items():\n",
    "        if count >= 10:  # 처음 10개 항목까지만 출력\n",
    "            break\n",
    "        print(key, \":\", path)\n",
    "        count += 1\n",
    "\n",
    "possible_sources = ['Royal Dutch Shell', 'Shell Downstream Inc.', 'Shell New Energies US', 'Shell']\n",
    "\n",
    "# G_85에 대한 filtered_paths 출력\n",
    "shortest_paths = find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(\"Filtered Paths for G_85:\")\n",
    "print_filtered_paths(filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#possible_sources = ['Samsung', 'Samsung Electronics Co., Ltd.']\n",
    "\n",
    "possible_sources = ['Royal Dutch Shell', 'Shell Downstream Inc.', 'Shell New Energies US', 'Shell']\n",
    "\n",
    "#각 가능한 소스에서 그래프의 다른 노드로의 최단 경로를 찾기 위해 함수 (find_path)를 호출\n",
    "shortest_paths = find_path(G_85, possible_sources)\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any(any(material == node.lower() for material in materials) for node in path)#node == 'Cisco' for node in path) #any(material == node.lower() for material in materials) for node in path)\n",
    "}\n",
    "print(filtered_paths)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f5879",
   "metadata": {},
   "source": [
    "### Check whether a key path exists for the final network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764837c",
   "metadata": {},
   "source": [
    "plot_individual_shortest_path(): 최단 경로들을 시각화하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88296f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터: 실제 데이터는 paths_hop_3 변수에 저장되어 있다고 가정합니다.\n",
    "\n",
    "# 각 경로를 별도의 figure로 시각화하기 위한 함수 정의\n",
    "def plot_individual_shortest_paths(paths):\n",
    "    fig_width = 30  # 가로 길이 설정\n",
    "    \n",
    "    for key, path in paths.items():\n",
    "        # if key == 'the U.S.':\n",
    "            # fig_height = 0.5  # 각 figure의 세로 길이는 고정\n",
    "            fig_width = 20  # 가로 길이 설정\n",
    "            fig_height = 0.7\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "            \n",
    "            # 경로 길이에 따라 x 위치를 등간격으로 설정, 노드의 x위치 및 y위치 설정\n",
    "            x_positions = np.linspace(0, fig_width, len(path))\n",
    "            y_positions = np.zeros(len(path))  # y 위치는 0으로 고정\n",
    "            \n",
    "            # 노드 색상 결정\n",
    "            node_colors = ['orange' if i in [0, len(path) - 1] else 'gray' for i in range(len(path))]\n",
    "            \n",
    "            # 노드와 경로 그리기\n",
    "            ax.plot(x_positions, y_positions, '-o', color='black', markersize=10, markerfacecolor='gray')\n",
    "            ax.scatter(x_positions[[0, -1]], y_positions[[0, -1]], color='orange', zorder=5)  # 첫 번째와 마지막 노드 강조\n",
    "            \n",
    "            # 노드 이름 텍스트로 표기\n",
    "            for x, label in zip(x_positions, path):\n",
    "                ax.text(x, -0.1, label, ha='center', va='top', fontsize=15)\n",
    "            \n",
    "            # 축 및 테두리 제거\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # 그래프 표시\n",
    "            plt.show()\n",
    "\n",
    "# 최단 경로 시각화 함수 호출\n",
    "plot_individual_shortest_paths(filtered_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd25440",
   "metadata": {},
   "source": [
    "plot_individual_shortest_paths(): 주어진 최단 경로들을 시각화하는 함수를 정의합니다. <br>위 함수 내용을 동일하지만 그래프 크기에 있어서 차이가 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터: 실제 데이터는 paths_hop_3 변수에 저장되어 있다고 가정합니다.\n",
    "\n",
    "# 각 경로를 별도의 figure로 시각화하기 위한 함수 정의\n",
    "def plot_individual_shortest_paths(paths):\n",
    "    fig_width = 30  # 가로 길이 설정\n",
    "    for key, path in paths.items():\n",
    "        fig_height = 0.5  # 각 figure의 세로 길이는 고정\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        \n",
    "        # 경로 길이에 따라 x 위치를 등간격으로 설정\n",
    "        x_positions = np.linspace(0, fig_width, len(path))\n",
    "        y_positions = np.zeros(len(path))  # y 위치는 0으로 고정\n",
    "        \n",
    "        # 노드 색상 결정\n",
    "        node_colors = ['orange' if i in [0, len(path) - 1] else 'gray' for i in range(len(path))]\n",
    "        \n",
    "        # 노드와 경로 그리기\n",
    "        ax.plot(x_positions, y_positions, '-o', color='black', markersize=15, markerfacecolor='gray')\n",
    "        ax.scatter(x_positions[[0, -1]], y_positions[[0, -1]], color='orange', zorder=5)  # 첫 번째와 마지막 노드 강조\n",
    "        \n",
    "        # 노드 이름 텍스트로 표기\n",
    "        for x, label in zip(x_positions, path):\n",
    "            ax.text(x, -0.1, label, ha='center', va='top', fontsize=8)\n",
    "        \n",
    "        # 축 및 테두리 제거\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # 그래프 표시\n",
    "        plt.show()\n",
    "\n",
    "# 최단 경로 시각화 함수 호출\n",
    "plot_individual_shortest_paths(paths_hop_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13b03a",
   "metadata": {},
   "source": [
    "plot_shortest_paths(): 주어진 여러 최단 경로들을 시각화하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#수정\n",
    "# 경로를 시각화하기 위한 함수 정의\n",
    "def plot_shortest_paths(paths):\n",
    "    # 시각화 설정\n",
    "    fig_width = 30  # 가로 길이 설정\n",
    "    fig_height = len(paths) * 0.5  # 세로 길이는 경로 수에 비례\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    \n",
    "    # 각 경로에 대해 반복하여 시각화\n",
    "    for i, (key, path) in enumerate(paths.items()):\n",
    "        # 경로 길이에 따라 x 위치를 균등하게 분할\n",
    "        x_positions = np.arange(0, fig_width, fig_width / (len(path) - 1))\n",
    "        y_positions = [i] * len(path)\n",
    "        \n",
    "        # 노드와 경로 그리기\n",
    "        ax.plot(x_positions, y_positions[:-1], '-o', color='black', markersize=10)\n",
    "        \n",
    "        # 노드 이름 텍스트로 표기\n",
    "        for x, label in zip(x_positions, path):\n",
    "            ax.text(x, i, label, ha='center', va='center', fontsize=8)\n",
    "\n",
    "    # 축 및 테두리 제거\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 그래프 표시\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 최단 경로 시각화 함수 호출\n",
    "plot_shortest_paths(paths_hop_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641bca4d",
   "metadata": {},
   "source": [
    "plot_timeline(): 여러 최단 경로들에 대한 타임라인을 그리는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bf0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 경로를 시각화하기 위한 함수 정의\n",
    "def plot_timeline(paths, ax, y_height, node_color, start_end_color):\n",
    "    # 각 경로에 대한 타임라인 그리기\n",
    "    for i, (node, path) in enumerate(paths.items()):\n",
    "        # 노드의 x 위치 설정\n",
    "        x_positions = range(len(path))\n",
    "        y_positions = [y_height] * len(path)\n",
    "        \n",
    "        # 타임라인과 노드 그리기\n",
    "        ax.plot(x_positions, y_positions, color='grey', linestyle='-', marker='o', markersize=10)\n",
    "        ax.scatter([x_positions[0], x_positions[-1]], [y_height, y_height], color=start_end_color, zorder=5)\n",
    "        \n",
    "        # 노드 레이블 추가\n",
    "        for x, label in zip(x_positions, path):\n",
    "            color = node_color if x not in (0, len(path) - 1) else start_end_color\n",
    "            ax.text(x, y_height, label, ha='center', va='center', bbox=dict(facecolor=color, edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        # 다음 경로를 위해 y 위치 조정\n",
    "        y_height -= 1\n",
    "\n",
    "# 전체 플롯 설정\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.set_ylim(-len(paths_hop_3 ), 1)  # y축의 범위 설정\n",
    "ax.set_xlim(-1, max(len(path) for path in shortest_paths.values()))  # x축의 범위 설정\n",
    "\n",
    "# 경로 타임라인 그리기\n",
    "plot_timeline(paths_hop_3 , ax, 0, 'grey', 'orange')\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "ax.axis('off')  # 축 제거\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정된 경로 시각화 함수 정의\n",
    "def plot_timeline2(paths, y_height, node_color, start_end_color):\n",
    "    # 시각화의 가로 길이 설정\n",
    "    fig_width = 20\n",
    "    # 전체 플롯 설정\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, len(paths)))\n",
    "\n",
    "    # 각 경로에 대한 타임라인 그리기\n",
    "    for i, (node, path) in enumerate(paths.items()):\n",
    "        # 노드의 x 위치 설정\n",
    "        x_positions = np.linspace(0, fig_width, len(path))\n",
    "        y_positions = [y_height] * len(path)\n",
    "        \n",
    "        # 타임라인과 노드 그리기\n",
    "        ax.plot(x_positions, y_positions, linestyle='-', marker='o', markersize=10, color=node_color)\n",
    "        ax.scatter([x_positions[0], x_positions[-1]], [y_height, y_height], color=start_end_color, zorder=5)\n",
    "        \n",
    "        # 노드 레이블 추가\n",
    "        for x, label in zip(x_positions, path):\n",
    "            ax.text(x, y_height - 0.1, label, ha='center', va='top', fontsize=9)\n",
    "\n",
    "        # 다음 경로를 위해 y 위치 조정\n",
    "        y_height -= 1\n",
    "\n",
    "    # 그래프 스타일 설정\n",
    "    ax.axis('off')  # 축 제거\n",
    "    ax.set_ylim(-len(paths) - 1, 1)  # y축의 범위 설정\n",
    "    ax.set_xlim(-1, fig_width + 1)  # x축의 범위 설정\n",
    "\n",
    "    # 그래프 표시\n",
    "    plt.tight_layout()  # 레이아웃 조정\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 경로 타임라인 그리기 함수 호출\n",
    "plot_timeline2(paths_hop_3, 0, 'grey', 'orange')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03dd99",
   "metadata": {},
   "source": [
    "주어진 그래프에서 특정 출발 노드로부터 도달 가능한 노드들과 해당 노드들 간의 최단 경로를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_node에서 도달 가능한 모든 노드 찾기\n",
    "reachable_nodes = nx.descendants(G_80, source_node) | {source_node}\n",
    "\n",
    "# source_node에서 각 노드까지의 최단 경로 찾기\n",
    "shortest_paths = {node: nx.shortest_path(G_80, source_node, node) for node in reachable_nodes}\n",
    "\n",
    "# 그래프 시각화\n",
    "plt.figure(figsize=(10, 5))  # 그래프 크기 설정\n",
    "\n",
    "# 모든 노드의 위치를 동일한 간격으로 설정\n",
    "node_positions = {node: (i, 0) for i, node in enumerate(G_80.nodes())}\n",
    "\n",
    "# 모든 노드와 엣지를 회색으로 그리기\n",
    "nx.draw_networkx_nodes(G_80, node_positions, node_color='grey', alpha=0.5)\n",
    "nx.draw_networkx_edges(G_80, node_positions, edge_color='grey', alpha=0.5)\n",
    "\n",
    "# 각 최단 경로를 그리기\n",
    "for path in shortest_paths.values():\n",
    "    # 경로상의 노드 위치 지정\n",
    "    path_positions = {node: node_positions[node] for node in path}\n",
    "    # 시작, 종료 노드만 주황색으로 표시\n",
    "    colors = ['orange' if node == source_node or node == path[-1] else 'grey' for node in path]\n",
    "    nx.draw_networkx_nodes(G_80, path_positions, nodelist=path, node_color=colors)\n",
    "    # 경로상의 엣지를 그리기\n",
    "    nx.draw_networkx_edges(G_80, node_positions, edgelist=list(zip(path[:-1], path[1:])), edge_color='orange', width=2)\n",
    "\n",
    "# 노드 레이블 추가\n",
    "nx.draw_networkx_labels(G_80, node_positions)\n",
    "\n",
    "# 축 제거\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea6364",
   "metadata": {},
   "source": [
    "## Heavy computation\n",
    "\n",
    "주어진 그래프에서 특정 출발 노드로부터 도달 가능한 노드들과 해당 노드들 간의 최단 경로를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정: G는 이미 생성된 네트워크X의 DiGraph() 객체입니다.\n",
    "# source_node는 시작 노드입니다.\n",
    "\n",
    "\n",
    "# 특정 노드에서 도달할 수 있는 모든 노드 찾기\n",
    "reachable_nodes = nx.descendants(G_80, source_node)\n",
    "\n",
    "# 최단 경로 찾기\n",
    "shortest_paths = {node: nx.shortest_path(G_80, source_node, node) for node in reachable_nodes}\n",
    "\n",
    "# 새로운 그래프 생성 및 최단 경로의 노드와 엣지만 추가\n",
    "H = nx.DiGraph()\n",
    "for path in shortest_paths.values():\n",
    "    nx.add_path(H, path)\n",
    "\n",
    "# 최단 경로를 그래프에 표시\n",
    "pos = nx.kamada_kawai_layout(G)  # 새로운 그래프의 노드 위치 결정\n",
    "plt.figure(figsize=(60, 60))\n",
    "\n",
    "# 노드와 엣지를 그립니다.\n",
    "nx.draw_networkx_nodes(H, pos, node_color='lightblue')\n",
    "nx.draw_networkx_edges(H, pos, edge_color='lightblue', width=2)\n",
    "nx.draw_networkx_labels(H, pos)\n",
    "\n",
    "# 시작 노드를 강조합니다.\n",
    "nx.draw_networkx_nodes(H, pos, nodelist=[source_node], node_color='green', node_size=300)\n",
    "\n",
    "plt.title('Shortest Paths from Source Node')\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbaeaf",
   "metadata": {},
   "source": [
    "주어진 그래프에서 특정 출발 노드로부터 도달 가능한 노드들과 해당 노드들 간의 최단 경로를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최단 경로를 그래프에 표시\n",
    "pos = nx.spring_layout(G_80) #노드 위치결정\n",
    "plt.figure(figsize=(60, 60))\n",
    "\n",
    "# 모든 노드와 엣지를 그립니다.\n",
    "nx.draw_networkx_nodes(G_80, pos, node_color='lightgray')\n",
    "nx.draw_networkx_edges(G_80, pos, alpha=0.3)\n",
    "\n",
    "# 각 최단 경로를 진하게 그립니다.\n",
    "for path in shortest_paths.values():\n",
    "    nx.draw_networkx_nodes(G_80, pos, nodelist=path, node_color='blue')\n",
    "    nx.draw_networkx_edges(G_80, pos, edgelist=list(zip(path, path[1:])), edge_color='blue', width=2)\n",
    "    nx.draw_networkx_labels(G_80, pos)\n",
    "\n",
    "# 시작 노드를 강조합니다.\n",
    "nx.draw_networkx_nodes(G_80, pos, nodelist=[source_node], node_color='green', node_size=300)\n",
    "\n",
    "plt.title('Shortest Paths from Source Node')\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dceabf",
   "metadata": {},
   "source": [
    "주어진 그래프에서 특정 출발 노드로부터 도달 가능한 노드 및 엣지를 강조하여 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f752051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가정: G는 이미 생성된 네트워크X의 DiGraph() 객체입니다.\n",
    "# source_node는 시작 노드입니다.\n",
    "\n",
    "\n",
    "# source_node에서 도달할 수 있는 모든 노드와 엣지를 구합니다.\n",
    "reachable_nodes = nx.descendants(G, source_node) | {source_node}\n",
    "reachable_edges = [(u, v) for u, v in G.edges() if u == source_node or v == source_node or u in reachable_nodes or v in reachable_nodes]\n",
    "\n",
    "# 모든 노드와 엣지에 대한 색상과 투명도 설정\n",
    "node_color = []\n",
    "node_alpha = []\n",
    "for node in G.nodes():\n",
    "    if node in reachable_nodes:\n",
    "        node_color.append('lightblue')  # 도달 가능한 노드는 진한 색\n",
    "        node_alpha.append(1.0)\n",
    "    else:\n",
    "        node_color.append('gray')  # 나머지 노드는 다른 색과 투명도 0.7\n",
    "        node_alpha.append(0.7)\n",
    "\n",
    "edge_color = []\n",
    "edge_alpha = []\n",
    "for edge in G.edges():\n",
    "    if edge in reachable_edges:\n",
    "        edge_color.append('lightblue')  # 도달 가능한 엣지는 진한 색\n",
    "        edge_alpha.append(1.0)\n",
    "    else:\n",
    "        edge_color.append('gray')  # 나머지 엣지는 다른 색과 투명도 0.7\n",
    "        edge_alpha.append(0.7)\n",
    "\n",
    "# 그래프 시각화\n",
    "pos = nx.spring_layout(G)  # 노드 위치 결정\n",
    "\n",
    "# 노드와 엣지 그리기\n",
    "plt.figure(figsize=(60,60))\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_color, alpha=node_alpha)\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_color, alpha=edge_alpha)\n",
    "nx.draw_networkx_labels(G, pos)  # 노드 레이블 그리기\n",
    "\n",
    "plt.title('Graph Visualization with Reachable Nodes and Edges Highlighted')\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e4ec3",
   "metadata": {},
   "source": [
    "주어진 그래프에서 노드와 엣지를 강조하여 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드와 엣지 그리기\n",
    "plt.figure(figsize=(60,60)) #그림의 크기 설정\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_color, alpha=node_alpha) #노드의 색상과 투명도 결정\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_color, alpha=edge_alpha) #엣지의 색상과 투명도 결정\n",
    "nx.draw_networkx_labels(G, pos)  # 노드 레이블 그리기\n",
    "\n",
    "plt.title('Graph Visualization with Reachable Nodes and Edges Highlighted')\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07e0a2",
   "metadata": {},
   "source": [
    "### 네트워크 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f68bfe",
   "metadata": {},
   "source": [
    "네트워크 분석 및 시뮬레이션을 수행합니다.\n",
    "<br>1. 최단경로분석\n",
    "<br>2. 탄력성 및 견고성 분석\n",
    "<br>3. 네트워크 시뮬레이션\n",
    "\n",
    "(*전체 출력을 원하시면 아래에 주석 처리된 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Analysis: Find the shortest path between two nodes.\n",
    "# For this example, let's find the shortest path from 'FDA' to 'Belgium'\n",
    "#최단경로 분석: 그래프 G에서 'FDA'에서 'Belgium'까지의 최단 경로\n",
    "shortest_path = nx.shortest_path(G, source='FDA', target='Belgium')\n",
    "\n",
    "# Robustness & Resilience Analysis: Remove a node and check the effect on the network.\n",
    "# For this example, let's remove 'U.S.' and see how it affects the connectivity.\n",
    "# Robustness & Resilience Analysis: U.S.' 노드를 제거한 후 그래프에서 연결된 구성 요소를 식별\n",
    "G.remove_node('U.S.')\n",
    "connected_components_after_removal = list(nx.connected_components(G.to_undirected()))\n",
    "\n",
    "# Network Simulation: Simulate scenarios by adding or removing nodes or edges.\n",
    "# For example, let's see how adding a new connection affects the shortest path.\n",
    "# Network Simulation: 'Health Care'와 'Belgium' 노드 간에 새로운 엣지를 추가하여 새로운 연결을 시뮬레이션 -> 새로운 최단경로\n",
    "G.add_edge('Health Care', 'Belgium')\n",
    "new_shortest_path = nx.shortest_path(G, source='Health Care', target='Belgium')\n",
    "\n",
    "# Display the results\n",
    "#shortest_path, connected_components_after_removal, new_shortest_path\n",
    "print(shortest_path)\n",
    "\n",
    "# Print the first 10 connected components\n",
    "for i, component in enumerate(connected_components_after_removal[1:11], start=1):\n",
    "    print(f\"Connected Component {i}: {component}\")\n",
    "    \n",
    "print(new_shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Path Analysis: Find the shortest path between two nodes.\n",
    "# For this example, let's find the shortest path from 'FDA' to 'Belgium'\n",
    "#최단경로 분석: 그래프 G에서 'FDA'에서 'Belgium'까지의 최단 경로\n",
    "shortest_path = nx.shortest_path(G, source='FDA', target='Belgium')\n",
    "\n",
    "# Robustness & Resilience Analysis: Remove a node and check the effect on the network.\n",
    "# For this example, let's remove 'U.S.' and see how it affects the connectivity.\n",
    "# Robustness & Resilience Analysis: U.S.' 노드를 제거한 후 그래프에서 연결된 구성 요소를 식별\n",
    "G.remove_node('U.S.')\n",
    "connected_components_after_removal = list(nx.connected_components(G.to_undirected()))\n",
    "\n",
    "# Network Simulation: Simulate scenarios by adding or removing nodes or edges.\n",
    "# For example, let's see how adding a new connection affects the shortest path.\n",
    "# Network Simulation: 'Health Care'와 'Belgium' 노드 간에 새로운 엣지를 추가하여 새로운 연결을 시뮬레이션 -> 새로운 최단경로\n",
    "G.add_edge('Health Care', 'Belgium')\n",
    "new_shortest_path = nx.shortest_path(G, source='Health Care', target='Belgium')\n",
    "\n",
    "# Display the results\n",
    "shortest_path, connected_components_after_removal, new_shortest_path\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cfb15",
   "metadata": {},
   "source": [
    "주어진 데이터프레임을 기반으로 필터링된 그래프를 생성하고, 특정 노드를 선택한 후 해당 노드로부터 도달 가능한 모든 노드를 찾는 과정을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = SSAN_SP_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019142a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'source_ner' 및 'target_ner' 열이 'Country'가 아닌 경우에 해당하는 행만을 포함하는 데이터프레임을 생성\n",
    "filtered_df = df[~df['source_ner'].isin(['Country']) & \n",
    "                 ~df['target_ner'].isin(['Country'])]\n",
    "\n",
    "# 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "filtered_G = nx.from_pandas_edgelist(filtered_df, source='source_rep_85', target='target_rep_85', edge_attr='edge', create_using=nx.DiGraph())\n",
    "\n",
    "# Now let's choose a node. For this example, we'll use 'the U.S. Food and Drug Administration' as the source node.\n",
    "# Now let's choose a node. For this example, we'll use 'Tesla, Inc.' as the source node.\n",
    "#출발노드 선택\n",
    "source_node = 'Tesla, Inc.'\n",
    "\n",
    "# Get all nodes reachable from the source node\n",
    "#도달가능한 노드 및 거리 계산\n",
    "reachable_nodes = nx.single_source_shortest_path_length(filtered_G, source_node)\n",
    "\n",
    "#도달가능한 모든 노드 목록 출력\n",
    "reachable_nodes.keys()  # This will list all the nodes that can be reached from the source node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896d795",
   "metadata": {},
   "source": [
    "'Country'를 제외한 노드에 대한 그래프를 생성하고, 특정 노드로부터 도달 가능한 모든 노드 중에서 각(Tesla, Apple, ...) 회사에 속하지 않는 노드를 찾는 과정을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country를 제외한 노드에 대한 그래프 생성\n",
    "filtered_df = df[~df['source_ner'].isin(['Country']) & ~df['target_ner'].isin(['Country'])]\n",
    "\n",
    "# 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "filtered_G = nx.from_pandas_edgelist(filtered_df, source='source_rep_85', target='target_rep_85', edge_attr='edge', create_using=nx.DiGraph())\n",
    "\n",
    "# Now let's choose a node. For this example, we'll use 'Tesla, Inc.' as the source node.\n",
    "#출발 노드 선택\n",
    "source_node = 'Tesla, Inc.'\n",
    "\n",
    "\n",
    "# 도달 가능한 모든 노드 구하기\n",
    "reachable_nodes = nx.single_source_shortest_path_length(filtered_G, source_node)\n",
    "\n",
    "# 'TSLA' 회사에 속하지 않는 노드 구하기\n",
    "non_tsla_nodes = [] #'TSLA' 회사에 속하지 않는 노드를 저장할 빈 리스트를 생성\n",
    "for node in reachable_nodes: #도달가능한 각 노드에 대해\n",
    "\n",
    "    # 현재 노드에 해당하는 데이터프레임을 추출합니다.\n",
    "    node_df = df[(df['source'] == node) | (df['target'] == node)]\n",
    "\n",
    "    # 데이터프레임이 비어있지 않고, 'TSLA'가 아닌 회사인지 확인합니다.\n",
    "    if not node_df.empty and all(node_df['company'] != 'TSLA'):\n",
    "        non_tsla_nodes.append(node)\n",
    "    else:\n",
    "        print(node)\n",
    "\n",
    "print(non_tsla_nodes)\n",
    "#결론:  'Tesla, Inc.' 노드로부터 도달 가능하면서 'TSLA' 회사에 속하지 않는 모든 노드 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b28cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country를 제외한 노드에 대한 그래프 생성\n",
    "filtered_df = df[~df['source_ner'].isin(['Country']) & ~df['target_ner'].isin(['Country'])]\n",
    "\n",
    "# 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "filtered_G = nx.from_pandas_edgelist(filtered_df, source='source_rep_85', target='target_rep_85', edge_attr='edge', create_using=nx.DiGraph())\n",
    "\n",
    "# Now let's choose a node. For this example, we'll use 'Apple Inc.' as the source node.\n",
    "#출발 노드 선택: Apple Inc.\n",
    "source_node = 'Apple Inc.'\n",
    "\n",
    "\n",
    "# 도달 가능한 모든 노드 구하기\n",
    "reachable_nodes = nx.single_source_shortest_path_length(filtered_G, source_node)\n",
    "\n",
    "# 'AAPL' 회사에 속하지 않는 노드 구하기\n",
    "non_AAPL_nodes = []\n",
    "for node in reachable_nodes:\n",
    "\n",
    "    # 현재 노드에 해당하는 데이터프레임을 추출합니다.\n",
    "    node_df = df[(df['source'] == node) | (df['target'] == node)]\n",
    "\n",
    "    # 데이터프레임이 비어있지 않고, 'AAPL'가 아닌 회사인지 확인합니다.\n",
    "    if not node_df.empty and all(node_df['company'] != 'AAPL'):\n",
    "        non_AAPL_nodes.append(node)\n",
    "    else:\n",
    "        print(node)\n",
    "\n",
    "print(non_AAPL_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808e7fe",
   "metadata": {},
   "source": [
    "각 열에서 해당 조건을 만족하는 열의 빈도 출력합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7714c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'source_rep_85' 열의 소문자 형태가 'the european union'인 행을 선택하고, 해당 행들 중 'source_ner' 열의 값들을 계산하여 카운트\n",
    "#source_ner 값의 빈도 출력\n",
    "df[(df['source_rep_85'].str.lower().isin(['the european union']))]['source_ner'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'target_rep_85' 열의 소문자 형태가 'the european union'인 행을 선택하고, 해당 행들 중 'target_ner' 열의 값들을 계산하여 카운트\n",
    "#target_ner의 빈도 출력\n",
    "df[(df['target_rep_85'].str.lower().isin(['the european union']))]['target_ner'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624bb015",
   "metadata": {},
   "source": [
    "각 열의 조건을 만족하는 행을 출력합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'source_rep_85' 열의 소문자 형태가 'european'인 행이거나 'target_rep_85' 열의 소문자 형태가 'the european union'인 행들을 선택\n",
    "df[(df['source_rep_85'].str.lower().isin(['european'])) | (df['target_rep_85'].str.lower().isin(['the european union']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'source_rep_85' 열의 소문자 형태가 'ge'인 행이거나 'target_rep_85' 열의 소문자 형태가 'ge'인 행들을 선택\n",
    "df[(df['source_rep_85'].str.lower().isin(['ge'])) | (df['target_rep_85'].str.lower().isin(['ge']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b860645",
   "metadata": {},
   "source": [
    "주어진 데이터프레임을 기반으로 네트워크 그래프를 만듭니다. <br> 노드 중심성을 계산한 다음, 노드를 NER 태그에 따라 카테고리화합니다. <br> 각 카테고리에서 중심성이 높은 상위 10개의 노드를 선택하여 이 정보들을 DataFrame에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Adding edges from the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(row['source_rep_85'], row['target_rep_85'], edge=row['edge'])\n",
    "\n",
    "# Calculate centrality measures (e.g., degree centrality)\n",
    "centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Preparing to categorize nodes by their NER tags\n",
    "node_categories = defaultdict(list)\n",
    "\n",
    "# Categorizing nodes\n",
    "for node in G.nodes:\n",
    "    # Finding the node in the DataFrame and categorizing it\n",
    "    if node in df['source_rep_85'].values:\n",
    "        category = df[df['source_rep_85'] == node]['source_ner'].values[0]\n",
    "    elif node in df['target_rep_85'].values:\n",
    "        category = df[df['target_rep_85'] == node]['target_ner'].values[0]\n",
    "    else:\n",
    "        category = 'Unknown'\n",
    "    \n",
    "    node_categories[category].append(node)\n",
    "\n",
    "# # Sorting nodes in each category by centrality\n",
    "# 중심성이 높은 상위 10개 노드 top_centrality_by_category 딕션너리에 저장\n",
    "top_centrality_by_category = {category: sorted(nodes, key=lambda x: centrality[x], reverse=True)[:10]\n",
    "                              for category, nodes in node_categories.items()}\n",
    "\n",
    "# top_centrality_by_category\n",
    "# Converting the top_centrality_by_category dictionary to a DataFrame\n",
    "# top_centrality_by_category기반으로 데이터프레임 생성\n",
    "df_top_centrality = pd.DataFrame([{\"Category\": category, \"Node\": node, \"Centrality\": centrality[node]}\n",
    "                                  for category, nodes in top_centrality_by_category.items()\n",
    "                                  for node in nodes])\n",
    "\n",
    "df_top_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114546a1",
   "metadata": {},
   "source": [
    "'Country'를 제외한 노드를 기반으로 방향성 있는 그래프를 생성하고, 지정된 출발 노드에서 도달 가능한 모든 노드를 찾은 후에는 'AAPL' 회사에 속하지 않는 노드들을 확인하고 해당 노드들의 목록을 출력하는 과정을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7ff20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Country를 제외한 노드에 대한 그래프 생성\n",
    "filtered_df = df[~df['source_ner'].isin(['Country']) & ~df['target_ner'].isin(['Country'])]\n",
    "\n",
    "# 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "filtered_G = nx.from_pandas_edgelist(filtered_df, source='source_rep_85', target='target_rep_85', edge_attr='edge', create_using=nx.DiGraph())\n",
    "\n",
    "# Now let's choose a node. For this example, we'll use 'the U.S. Food and Drug Administration' as the source node.\n",
    "# 출발노드 지정\n",
    "#source_node = 'the U.S. Food and Drug Administration'\n",
    "#print(filtered_G.nodes)\n",
    "source_node = 'iPad'\n",
    "\n",
    "\n",
    "\n",
    "# 도달 가능한 모든 노드 구하기\n",
    "# 출발노드에서 도달가능한 모든 노드 찾아 그 거리 계산\n",
    "reachable_nodes = nx.single_source_shortest_path_length(filtered_G, source_node)\n",
    "\n",
    "# 'AAPL' 회사에 속하지 않는 노드 구하기\n",
    "non_AAPL_nodes = []\n",
    "for node in reachable_nodes:\n",
    "\n",
    "    # 현재 노드에 해당하는 데이터프레임을 추출합니다.\n",
    "    node_df = df[(df['source'] == node) | (df['target'] == node)]\n",
    "\n",
    "    # 데이터프레임이 비어있지 않고, 'AAPL'이 아닌 회사인지 확인합니다.\n",
    "    if not node_df.empty and all(node_df['company'] != 'AAPL'):\n",
    "        non_AAPL_nodes.append(node)\n",
    "    else:\n",
    "        print(node)\n",
    "\n",
    "print(non_AAPL_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aceea",
   "metadata": {},
   "source": [
    "find_single_points_of_failure(): 방향성 있는 그래프에서 단일 실패점을 찾는 함수를 정의합니다.\n",
    "<br>(*상위 10개 데이터에 대해서만 출력하였습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44035981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 실패점을 찾는 함수 정의\n",
    "def find_single_points_of_failure(graph):\n",
    "    spofs = []  # 단일 실패점 목록을 저장할 리스트\n",
    "    for node in graph.nodes():\n",
    "        H = graph.copy()  # 네트워크의 복사본을 생성\n",
    "        H.remove_node(node)  # 현재 노드를 복사본에서 제거\n",
    "        \n",
    "        # 네트워크가 여전히 강하게 연결되어 있는지 확인\n",
    "        # (방향성이 있는 그래프에 대한 강한 연결성 체크)\n",
    "        if not nx.is_strongly_connected(H):\n",
    "            spofs.append(node)  # 그래프가 여전히 강하게 연결되어 있지 않다면, 현재 노드는 단일 실패점\n",
    "            \n",
    "    return spofs\n",
    "\n",
    "# 단일 실패점 찾기\n",
    "# 단일 실패점을 찾아 single_points_of_failure에 저장\n",
    "single_points_of_failure = find_single_points_of_failure(G)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"First 10 elements in Single Points of Failure:\", single_points_of_failure[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b17aab",
   "metadata": {},
   "source": [
    "thre(threshold) 별로 주어진 그래프에서 노드의 도달성 분석을 통해 영향력을 계산하고, 그 영향력에 따라 상위 30개 노드를 출력합니다.\n",
    "\n",
    "네트워크 내에서 중요한 역할을 하는 노드를 식별합니다.\n",
    "\n",
    "영향력 있는 노드 찾기: 도달성 분석을 통해 특정 노드에서 다른 노드들에게 얼마나 많은 영향을 미칠 수 있는지 확인할 수 있습니다. <br> 예를 들어, 노드 A에서 다른 모든 노드까지 도달할 수 있다면, 노드 A는 네트워크에서 중요한 역할을 하는 것으로 간주될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터프레임 df에서 방향성 그래프 생성\n",
    "G=nx.from_pandas_edgelist(df, source='source_rep_85', target='target_rep_85', edge_attr='edge', create_using=nx.DiGraph())\n",
    "# 여기에 G에 노드와 엣지를 추가하는 코드를 넣어주세요.\n",
    "# 추가했음\n",
    "for _, row in df.iterrows():\n",
    "    G.add_node(row['source_rep_85'])\n",
    "    G.add_node(row['target_rep_85'])\n",
    "    G.add_edge(row['source_rep_85'], row['target_rep_85'], edge=row['edge'])\n",
    "\n",
    "# 노드별 영향력 점수 계산\n",
    "influence_scores = {} #노드별 영향력 점수를 저장할 빈 딕셔너리를 생성\n",
    "for node in G.nodes:\n",
    "    reachable_nodes = nx.descendants(G, node) #현재 노드에서 도달가능한 모든 하위 노드 찾기\n",
    "    influence_score = len(reachable_nodes) / (len(G.nodes) - 1) #도달 가능한 노드의 수를 전체 노드 수로 나누어 영향력 점수를 계산\n",
    "    influence_scores[node] = influence_score #딕션너리에 점수 저장\n",
    "\n",
    "# 영향력 점수를 기준으로 내림차순으로 상위 30개의 노드 추출\n",
    "top_30_nodes = sorted(influence_scores, key=influence_scores.get, reverse=True)[:30]\n",
    "\n",
    "# 결과 출력\n",
    "for i, node in enumerate(top_30_nodes, start=1):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1efd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 내에서 중요한 역할을 하는 노드 식별\n",
    "# 데이터프레임을 기반으로 방향성 있는 그래프 G를 생성, source: 출발노드, target: 도착노드\n",
    "G=nx.from_pandas_edgelist(df, source='source', target='target', edge_attr='edge', create_using=nx.DiGraph())\n",
    "# 여기에 G에 노드와 엣지를 추가하는 코드를 넣어주세요.\n",
    "# 추가했음\n",
    "for _, row in df.iterrows():\n",
    "    G.add_node(row['source'])\n",
    "    G.add_node(row['target'])\n",
    "    G.add_edge(row['source'], row['target'], edge=row['edge'])\n",
    "\n",
    "# 노드별 영향력 점수 계산\n",
    "influence_scores = {}\n",
    "for node in G.nodes:\n",
    "    reachable_nodes = nx.descendants(G, node) \n",
    "    influence_score = len(reachable_nodes) / (len(G.nodes) - 1)\n",
    "    influence_scores[node] = influence_score\n",
    "\n",
    "# 영향력 점수를 기준으로 상위 30개의 노드 추출\n",
    "top_30_nodes = sorted(influence_scores, key=influence_scores.get, reverse=True)[:30]\n",
    "\n",
    "# 결과 출력\n",
    "for i, node in enumerate(top_30_nodes, start=1):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e5da5",
   "metadata": {},
   "source": [
    "상위 30개 노드와 각 노드의 영향력 점수를 함께 출력합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a34440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 출력\n",
    "for i, node in enumerate(top_30_nodes, start=1):\n",
    "    print(node, influence_scores[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c33eff",
   "metadata": {},
   "source": [
    "'eOne'이라는 값이 'source_rep_85' 또는 'target_rep_85' 열에 포함된 행들을 필터링합니다. (확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9942286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['source_rep_85'].isin(['eOne'])) | (df['target_rep_85'].isin(['eOne']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baecf3d",
   "metadata": {},
   "source": [
    "그래프 (filtered_G)의 노드에 대한 영향력 점수를 계산하는 과정입니다. <br> 특정 조건에 따라 도달 가능한 노드를 출력하고, 영향력 점수를 기준으로 상위 30개 노드를 식별한 후 이를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800683b",
   "metadata": {},
   "source": [
    "네트워크 내에서 중요한 역할을 하는 노드(영향력있는 노드) 식별합니다. <br>\n",
    "영향력 있는 노드 찾기: 도달성 분석을 통해 특정 노드에서 다른 노드들에게 얼마나 많은 영향을 미칠 수 있는지 확인할 수 있습니다. <br>\n",
    "예를 들어, 노드 A에서 다른 모든 노드까지 도달할 수 있다면, 노드 A는 네트워크에서 중요한 역할을 하는 것으로 간주될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6da0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 G에 노드와 엣지를 추가하는 코드를 넣어주세요.\n",
    "\n",
    "# 노드별 영향력 점수 계산\n",
    "influence_scores = {}\n",
    "for node in filtered_G.nodes:\n",
    "    reachable_nodes = nx.descendants(filtered_G, node)\n",
    "    influence_score = len(reachable_nodes) / (len(filtered_G.nodes) - 1)\n",
    "    # if influence_score>0.002:\n",
    "    #     print(node, reachable_nodes)\n",
    "    if node == 'U.S' or node == 'U.S':\n",
    "        print(node, reachable_nodes)\n",
    "    influence_scores[node] = influence_score\n",
    "\n",
    "# 영향력 점수를 기준으로 상위 30개의 노드 추출\n",
    "top_30_nodes = sorted(influence_scores, key=influence_scores.get, reverse=True)[:30]\n",
    "\n",
    "# 결과 출력\n",
    "for i, node in enumerate(top_30_nodes, start=1):\n",
    "    print(f\"Rank {i}: Node {node}, Influence Score: {influence_scores[node]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cab2f",
   "metadata": {},
   "source": [
    "그래프 G에서 노드를 하나씩 제거하고, 제거된 노드로 인해 그래프의 강한 연결성이 깨지는지를 확인하는 과정입니다.\n",
    "<br> (*출력값은 주석처리 하였습니다. 데이터를 원하시면 주석처리한 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33084d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잠재적인 위험 요소 또는 취약점 찾기:\n",
    "# 단일 실패점 찾기: 네트워크 내에서 특정 노드가 실패하거나 제거되었을 때, 다른 노드들 간의 연결성이 손실되는 경우가 있을 수 있습니다. \n",
    "# 이를 \"단일 실패점\"이라고 합니다. 도달성 분석을 통해 이러한 단일 실패점을 찾을 수 있습니다.\n",
    "\n",
    "for node in G.nodes:\n",
    "    H = G.copy()\n",
    "    H.remove_node(node)\n",
    "    #만약 H가 더 이상 강하게 연결되어 있지 않다면, '현재 노드를 제거하면 그래프의 연결성이 깨진다'는 메시지를 출력\n",
    "    \n",
    "    # 전체 데이터를 원하시면 아래의 코멘트 처리한 코드를 돌리시면 됩니다.\n",
    "    #if not nx.is_strongly_connected(H):  # 노드 제거 후 네트워크가 더 이상 강하게 연결되어 있지 않음\n",
    "        #print(f\"Removing {node} breaks the network connectivity\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694758a",
   "metadata": {},
   "source": [
    "Louvain 알고리즘을 사용하여 네트워크를 여러 번 실행하고 최적의 커뮤니티 분할을 찾는 과정을 수행합니다. <br>\n",
    "각 실행에서 모듈러리티를 계산하고, 최종적으로 최고의 모듈러리티를 갖는 파티션을 선택합니다.\n",
    "<br>(*전체 데이터 출력을 원하시면 밑에 주석처리된 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf417fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_first_10_elements(result):\n",
    "    for item in list(result)[:10]:\n",
    "        print(item)\n",
    "\n",
    "partitions=[]\n",
    "modularities=[]\n",
    "\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "best_modularity = -1\n",
    "best_partition = None\n",
    "\n",
    "# 여러 번 실행하여 최적의 커뮤니티 분할 찾기\n",
    "for seed in range(10):\n",
    "    partition = community_louvain.best_partition(G_undirected, random_state=seed)\n",
    "    modularity = community_louvain.modularity(partition, G_undirected)\n",
    "    \n",
    "    partitions.append(partition)\n",
    "    modularities.append(modularity)\n",
    "    \n",
    "    print(f\"Run {seed}: Modularity = {modularity:.4f}\")\n",
    "    \n",
    "    if seed >= 9:\n",
    "        break\n",
    "\n",
    "# 가장 높은 모듈라리티를 가진 파티션 찾기\n",
    "best_partition_index = np.argmax(modularities)\n",
    "best_partition = partitions[best_partition_index]\n",
    "best_modularity = modularities[best_partition_index]\n",
    "\n",
    "print(\"\\nBest Partition:\")\n",
    "print_first_10_elements(best_partition.items())\n",
    "print(f\"Best Modularity: {best_modularity:.4f}\")\n",
    "\n",
    "# 노드의 실제 이름을 사용하여 결과 출력\n",
    "best_partition_named = {node: community for node, community in best_partition.items()}\n",
    "print(\"\\nBest Partition:\")\n",
    "print_first_10_elements(best_partition_named.items())\n",
    "print(\"Best Modularity:\", best_modularity)\n",
    "\n",
    "# 커뮤니티 크기 계산\n",
    "community_sizes = {}\n",
    "for node, comm in best_partition_named.items():\n",
    "    if comm not in community_sizes:\n",
    "        community_sizes[comm] = 0\n",
    "    community_sizes[comm] += 1\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nCommunity Sizes:\")\n",
    "print_first_10_elements(community_sizes.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 커뮤니티 검출 (Community Detection):\n",
    "# Louvain 메소드를 사용합니다.\n",
    "\n",
    "#modularities와 partitions 리스트 선언\n",
    "partitions=[]\n",
    "modularities=[]\n",
    "\n",
    "# community_louvain only takes undirected graphs\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# 여러 번 실행하여 최적의 커뮤니티 분할 찾기\n",
    "best_modularity = -1 #최적의 모듈러리티를 저장하는 변수 초기화\n",
    "best_partition = None #최적의 분할을 저장하는 변수 초기화\n",
    "# 여러 번 실행\n",
    "for seed in range(10):\n",
    "    # Louvain 커뮤니티 검출 실행\n",
    "    # Louvain 알고리즘을 실행하고 현재의 시드값으로 커뮤니티 파티션을 얻음\n",
    "    partition = community_louvain.best_partition(G_undirected, random_state=seed)\n",
    "    #얻은 파티션에 대한 모듈러리티 계산\n",
    "    modularity = community_louvain.modularity(partition, G_undirected)\n",
    "    \n",
    "    # 결과 저장\n",
    "    partitions.append(partition)\n",
    "    modularities.append(modularity)\n",
    "    \n",
    "    print(f\"Run {seed}: Modularity = {modularity:.4f}\")\n",
    "\n",
    "# 가장 높은 모듈라리티를 가진 파티션 찾기\n",
    "best_partition_index = np.argmax(modularities)\n",
    "best_partition = partitions[best_partition_index]\n",
    "best_modularity = modularities[best_partition_index]\n",
    "\n",
    "print(\"\\nBest Partition:\")\n",
    "print(best_partition)\n",
    "print(f\"Best Modularity: {best_modularity:.4f}\")\n",
    "\n",
    "#G_undirected에 대해서도 반복\n",
    "for i in range(10):\n",
    "    partition = community_louvain.best_partition(G_undirected, random_state=i)\n",
    "    modularity = community_louvain.modularity(partition, G_undirected)\n",
    "    print(f\"Run {i}: Modularity = {modularity:.4f}\")\n",
    "    \n",
    "    if modularity > best_modularity:\n",
    "        best_modularity = modularity\n",
    "        best_partition = partition\n",
    "\n",
    "# 노드의 실제 이름을 사용하여 결과 출력\n",
    "best_partition_named = {node: community for node, community in best_partition.items()}\n",
    "print(\"\\nBest Partition:\")\n",
    "print(best_partition_named)\n",
    "print(\"Best Modularity:\", best_modularity)\n",
    "\n",
    "# 커뮤니티 크기 계산\n",
    "community_sizes = {}\n",
    "for node, comm in best_partition_named.items():\n",
    "    if comm not in community_sizes:\n",
    "        community_sizes[comm] = 0\n",
    "    community_sizes[comm] += 1\n",
    "\n",
    "# 결과 출력\n",
    "for comm, size in community_sizes.items():\n",
    "    print(f\"Community {comm}: {size} nodes\")\n",
    "    \n",
    "'''    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db98e1f",
   "metadata": {},
   "source": [
    "# 6. SSAN+GPT Graph Visualization\n",
    "\n",
    "SSAN 모델을 통해 분석한 데이터와 GPT를 통해 획득한 데이터를 전처리하여 merge한 파일을 import합니다. 각 소스노드에서 타겟노드에 도달하는 최단거리를 파악합니다. 마지막으로 4. analysis와 마찬가지로 데이터프레임에서 threshold별, node별로 출발지와 목적지 정보를 기반으로 그래프를 생성하고 시각화하는 기능을 수행합니다. 이를 통해 supply chain network 데이터를 분석하고 시각화하는 기능을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c6113",
   "metadata": {},
   "source": [
    "파일을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87657a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ALL_df.csv')\n",
    "df_95 = pd.read_csv('ALL_df_95.csv')\n",
    "df_90 = pd.read_csv('ALL_df_90.csv')\n",
    "df_85 = pd.read_csv('ALL_df_85.csv')\n",
    "df_80 = pd.read_csv('ALL_df_80.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81745441",
   "metadata": {},
   "source": [
    "pickle 파일 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f08a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle 파일로부터 그래프 로드\n",
    "with open('graph.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "with open('graph_95.pickle', 'rb') as f:\n",
    "    G_95 = pickle.load(f)\n",
    "\n",
    "with open('graph_90.pickle', 'rb') as f:\n",
    "    G_90 = pickle.load(f)\n",
    "\n",
    "with open('graph_85.pickle', 'rb') as f:\n",
    "    G_85 = pickle.load(f)\n",
    "\n",
    "with open('graph_80.pickle', 'rb') as f:\n",
    "    G_80 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d8099",
   "metadata": {},
   "source": [
    "find_paths_to_targets: 방향성이 있는 그래프 G에서 주어진 대상 노드 목록으로부터 들어오는 모든 경로를 찾아주는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_to_targets(G, target_nodes):\n",
    "    all_paths = {} #모든 경로를 저장할 빈 딕셔너리를 초기화\n",
    "    for target in target_nodes: #대상 노드 목록에 대해 반복\n",
    "        # 해당 대상 노드로 들어오는 모든 경로 찾기\n",
    "        paths_to_target = [] #현재 대상 노드로 들어오는 경로를 저장할 빈 리스트를 초기화\n",
    "        for source in G.nodes(): #모든 노드에 대해 반복\n",
    "            if source != target: #출발 노드와 대상 노드가 같지 않은 경우에 실행\n",
    "                paths = list(nx.all_simple_paths(G, source, target)) #현재 출발 노드에서 대상 노드까지의 모든 단순 경로 탐색\n",
    "                if paths:\n",
    "                    paths_to_target.extend(paths) #찾은 경로가 있다면 찾은 경로를 paths_to_target 리스트에 추가\n",
    "        \n",
    "        # 찾은 경로를 결과 딕셔너리에 추가\n",
    "        if paths_to_target:\n",
    "            all_paths[target] = paths_to_target #현재 대상 노드를 key로, 대상 노드로 들어오는 모든 경로를 value로 저장\n",
    "    \n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255610f1",
   "metadata": {},
   "source": [
    "기존의 피클파일이 너무 커서 결과값 출력 간소화를 위해 원본 데이터에서 샘플링을 수행하였습니다.<br> \n",
    "전체 데이터로 처리한 결과값을 보려면 아래의 주석처리된 셀을 실행하시면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling due to the file size\n",
    "def G_sampling(pickle, sample_size):\n",
    "    random.seed(123)\n",
    "    G = pickle\n",
    "    sample_size = sample_size\n",
    "    num_nodes_to_sample = int(sample_size * G.number_of_nodes())\n",
    "    sampled_nodes = random.sample(list(G.nodes()), num_nodes_to_sample)\n",
    "    G_sample = G.subgraph(sampled_nodes)\n",
    "    return G_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47361d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled pickle files\n",
    "\n",
    "G_sample = G_sampling(G, 0.1)\n",
    "G_95_sample = G_sampling(G_95, 0.1)\n",
    "G_90_sample = G_sampling(G_90, 0.1)\n",
    "G_85_sample = G_sampling(G_85, 0.1)\n",
    "G_80_sample = G_sampling(G_80, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e407045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified version with the sampled data\n",
    "# 예시 대상 노드 목록 (실제 대상 노드로 변경 필요)\n",
    "target_nodes = ['U.S.', 'FDA']\n",
    "#target_nodes = ['TargetNode1', 'TargetNode2']\n",
    "\n",
    "\n",
    "found_paths = find_paths_to_targets(G_sample, target_nodes)\n",
    "\n",
    "# 결과 출력\n",
    "print(found_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavy computation\n",
    "\"\"\"\n",
    "# 예시 대상 노드 목록 (실제 대상 노드로 변경 필요)\n",
    "target_nodes = ['U.S.', 'FDA']\n",
    "#target_nodes = ['TargetNode1', 'TargetNode2']\n",
    "\n",
    "\n",
    "found_paths = find_paths_to_targets(G, target_nodes)\n",
    "\n",
    "# 결과 출력\n",
    "print(found_paths)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a5d0b",
   "metadata": {},
   "source": [
    "find_path는 그래프 G와 가능한 출발 노드 목록 possible_sources로부터 각 출발 노드에서 다른 노드로의 최단 경로를 찾아주는 역할을 하는 함수입니다.(중복)\n",
    "find_node는 df에서 source 또는 target 열에 주어진 name을 포함하는 특정 노드를 찾아주는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(G, possible_sources):\n",
    "    shortest_paths = {}\n",
    "    # 그래프 노드 중에서 실제 존재하는 source_node 찾기\n",
    "    for source in possible_sources: #가능한 출발노드 목록에 대해 반복\n",
    "        #현재 주어진 source가 실제 그래프 노드에 존재하는지 확인하고, 존재한다면 actual_source에 해당 노드를 할당\n",
    "        actual_source = next((node for node in G.nodes() if node == source), None)\n",
    "        if actual_source:\n",
    "            reachable_nodes = nx.descendants(G, actual_source)\n",
    "            tmp_shortest_paths = {node: nx.shortest_path(G, actual_source, node) for node in reachable_nodes}\n",
    "            \n",
    "            print(\"Exist\", actual_source)\n",
    "            shortest_paths = {**shortest_paths, **tmp_shortest_paths}\n",
    "        # print(source)\n",
    "        \n",
    "    return shortest_paths\n",
    "\n",
    "def find_node(name):\n",
    "    # 데이터프레임에서 source 열에 주어진 name을 포함하는 행을 필터링하고, 해당하는 source 값을 고유하게 추출\n",
    "    unique_values_source = df[df['source'].str.contains(name, case=False)]['source'].unique()\n",
    "    # 데이터프레임에서 target 열에 주어진 name을 포함하는 행을 필터링하고, 해당하는 target 값을 고유하게 추출\n",
    "    unique_values_target = df[df['target'].str.contains(name, case=False)]['target'].unique()\n",
    "    # 결과 출력\n",
    "    \n",
    "    return (set(list(unique_values_source) + list(unique_values_target))) \n",
    "\n",
    "# shortest_path = nx.shortest_path(G, source='Newmont Corporation', target=5)\n",
    "\n",
    "print(find_node('DuPont'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be89531",
   "metadata": {},
   "source": [
    " possible_sources 리스트의 노드에서 시작하여, 해당 노드로 가는 모든 최단 경로를 찾고, 특정 조건에 따라 필터링합니다.<br>\n",
    " (전체데이터를 출력하시려면 밑에 코드를 실행하시면 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_filtered_paths(filtered_paths):\n",
    "    count = 0\n",
    "    for key, path in filtered_paths.items():\n",
    "        if count >= 10:  # 처음 10개 항목까지만 출력\n",
    "            break\n",
    "        print(key, \":\", path)\n",
    "        count += 1\n",
    "\n",
    "possible_sources = ['DuPont Asian Group', 'DuPont Teijin Films', 'The Dow Chemical DuPont','DuPont']\n",
    "exclude_words = ['sec', 'covid','board of directors','results of operations','esg','consolidated financial statements',\n",
    "                 'pcaob']\n",
    "\n",
    "# find_path 함수를 사용하여 주어진 possible_sources에서 시작하는 모든 최단 경로 계산\n",
    "shortest_paths = find_path(G, possible_sources)\n",
    "\n",
    "# 최단 경로 중에서 exclude_words에 해당하는 단어를 포함하는 노드를 제외하고, 나머지 경로들을 filtered_paths에 저장\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if all(not any(exclude_word in node.lower() for exclude_word in exclude_words) for node in path)\n",
    "}\n",
    "\n",
    "print(\"Filtered Paths:\")\n",
    "print_filtered_paths(filtered_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''실제코드 입니다.\n",
    "#최단 경로를 찾을 시작 노드들로 구성된 리스트\n",
    "possible_sources = ['DuPont Asian Group', 'DuPont Teijin Films', 'The Dow Chemical DuPont','DuPont']\n",
    "#경로에서 제외할 단어들로 구성된 리스트\n",
    "exclude_words = ['sec', 'covid','board of directors','results of operations','esg','consolidated financial statements',\n",
    "                 'pcaob']\n",
    "\n",
    "#find_path 함수를 사용하여 주어진 possible_sources에서 시작하는 모든 최단 경로 계산\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "\n",
    "#최단 경로 중에서 exclude_words에 해당하는 단어를 포함하는 노드를 제외하고, 나머지 경로들을 filtered_paths에 저장\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if all(not any(exclude_word in node.lower() for exclude_word in exclude_words) for node in path)\n",
    "    # if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198af8a",
   "metadata": {},
   "source": [
    "possible_sources 리스트에 있는 노드에서 시작하여, 해당 노드로 가는 모든 최단 경로를 찾은 다음 특정 조건을 만족하는 경로들을 필터링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#최단 경로를 찾을 시작 노드들로 구성된 리스트\n",
    "possible_sources = ['BHP Group', 'BHP Copper, Inc']\n",
    "\n",
    "#find_path 함수를 사용하여 주어진 possible_sources에서 시작하는 모든 최단 경로 탐색\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "\n",
    "#최단 경로 중에서 해당 경로에 포함된 노드 중 적어도 하나가 'Tesla'를 포함하면 해당 경로를 filtered_paths에 저장\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any('Tesla' in node for node in path) #.lower()\n",
    "    # if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Qualcomm', 'Qualcomm Incorporated', 'QUALCOMM Incorporated', 'Qualcomm Technologies, Inc.']\n",
    "\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any('Samsung' in node for node in path) #.lower()\n",
    "    # if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Foxconn Technology Group']\n",
    "\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any('Best Buy' in node for node in path) #.lower()\n",
    "    # if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sources = ['Rio Tinto plc']\n",
    "\n",
    "shortest_paths =find_path(G, possible_sources)\n",
    "\n",
    "filtered_paths = {\n",
    "    key: path for key, path in shortest_paths.items()\n",
    "    if any('Apple' in node for node in path) #.lower()\n",
    "    # if len(path) == 3 and any(material == path[2].lower() for material in materials)\n",
    "}\n",
    "\n",
    "print(filtered_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd824e9a",
   "metadata": {},
   "source": [
    "Graph_and_Visual(): 각 노드간의 관계그래프를 시각화하는 함수를 정의합니다.<br>(앞서 정의한 Graph_and_Visual과 파일을 저장하는 과정에 있어 약간의 차이가 존재합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edbbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_and_Visual(fin_df, source, target, isin_ner_li, Title, thre, pos):\n",
    "    # 'Country' 또는 'Firm'으로 태그된 노드만 포함하는 행을 필터링\n",
    "    filtered_df = fin_df[(fin_df['source_ner'].isin(isin_ner_li)) & (fin_df['target_ner'].isin(isin_ner_li))]\n",
    "\n",
    "    # 필터링된 데이터프레임을 기반으로 그래프 다시 생성\n",
    "    filtered_G = nx.from_pandas_edgelist(filtered_df, source=source, target=target, edge_attr='edge', create_using=nx.DiGraph())\n",
    "    \n",
    "    # 색상 매핑 (이전에 정의된 'Country'와 'Firm'에 대한 색상만 사용)\n",
    "    color_map_filtered = {'Country': 'royalblue', 'Firm': 'lightgreen','Resource': 'orange','Technology': 'darkred'}\n",
    "\n",
    "    # 필터링된 그래프의 노드에 NER 태그 속성 추가\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        if row[source] in filtered_G.nodes:\n",
    "            filtered_G.nodes[row[source]]['ner'] = row['source_ner']\n",
    "        if row[target] in filtered_G.nodes:\n",
    "            filtered_G.nodes[row[target]]['ner'] = row['target_ner']\n",
    "\n",
    "    # 노드 색상 할당 (필터링된 그래프에 대해)\n",
    "    filtered_node_colors = [color_map_filtered[filtered_G.nodes[node]['ner']] for node in filtered_G.nodes]\n",
    "\n",
    "    # 그래프 시각화\n",
    "    plt.figure(figsize=(60,60))\n",
    "    nx.draw(filtered_G, with_labels=True, node_color=filtered_node_colors, edge_color='gray', pos=pos, font_size=3, node_size=100)\n",
    "    plt.title(\"Supply Chain Graph ({})\".format(Title), fontdict={'fontsize': 100})\n",
    "    plt.savefig(\"./ALL_graph_{}_{}.png\".format(Title,thre))\n",
    "    # plt.show()\n",
    "    return filtered_G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5aec53",
   "metadata": {},
   "source": [
    "df_95, df_90, df_85, df_80 데이터프레임을 기존의 df 데이터프레임에 추가한 후 중복된 행을 제거하는 과정을 수행합니다.(확인용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffe635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#수정했음\n",
    "tmp_df = pd.concat([df, df_95], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tmp_df = pd.concat([tmp_df, df_90], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tmp_df = pd.concat([tmp_df, df_85], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tmp_df = pd.concat([tmp_df, df_80], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#수정했음(빠른 컴파일, 원래 코드)\n",
    "tmp_df = pd.concat([df, df_95], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tmp_df = pd.concat([df, df_90], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tmp_df = pd.concat([df, df_85], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tmp_df = pd.concat([df, df_80], ignore_index=True)\n",
    "tmp_df = tmp_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a7f318",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe355a8",
   "metadata": {},
   "source": [
    "thre(threshold) 별로 Graph_and_Visual 함수를 호출하여 데이터프레임 df를 기반으로 그래프를 생성하고 시각화합니다. <br>이후 statistic 함수를 호출하여 생성된 그래프의 통계적 정보를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2eda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터프레임을 이용하여 그래프를 생성, source와 target: 엣지의 출발 노드와 도착 노드, edge_attr: 엣지의 속성, create_using: 사용할 그래프 유형을 지정\n",
    "G_raw = nx.from_pandas_edgelist(df, source='source', target='target', edge_attr='edge', create_using=nx.DiGraph())\n",
    "#그래프를 시각화하기 위한 레이아웃을 생성하는 함수 -> 힘 기반의 레이아웃 알고리즘을 사용하여 그래프의 노드들을 배치\n",
    "#pos 변수에 spring_layout로 얻은 레이아웃이 저장 -> 레이아웃은 그래프를 시각화할 때 각 노드의 위치를 지정\n",
    "pos = nx.spring_layout(G_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-R-T\n",
    "#데이터프레임 df를 이용하여 그래프를 생성하고 시각화\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Firm','Resource','Technology'], 'Firm and Resource and Technology','raw',pos)\n",
    "#그래프 G의 통계적 정보를 출력\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-C\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Country'], 'Country','raw',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c143134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-F\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Country','Firm'], 'Country and Firm','raw',pos)\n",
    "statistic(G)\n",
    "# C-R\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Country','Resource'], 'Country and Resource','raw',pos)\n",
    "statistic(G)\n",
    "# C-T\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Country','Technology'], 'Country and Technology','raw',pos)\n",
    "statistic(G)\n",
    "# F-F\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Firm'], 'Firm','raw',pos)\n",
    "statistic(G)\n",
    "# F-R\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Firm','Resource'], 'Firm and Resource','raw',pos)\n",
    "statistic(G)\n",
    "# F-T\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Firm','Technology'], 'Firm and Technology','raw',pos)\n",
    "statistic(G)\n",
    "# R-R\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Resource'], 'Resource','raw',pos)\n",
    "statistic(G)\n",
    "# R-T\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Resource','Technology'], 'Resource and Technology','raw',pos)\n",
    "statistic(G)\n",
    "# T-T\n",
    "G = Graph_and_Visual(df, 'source', 'target', ['Technology'], 'Technology','raw',pos)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_95 = nx.from_pandas_edgelist(df_95, source='source', target='target', edge_attr='edge', create_using=nx.DiGraph())\n",
    "pos_95 = nx.spring_layout(G_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ece867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-C\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Country'], 'Country','95',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-F\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Country','Firm'], 'Country and Firm','95',pos_95)\n",
    "statistic(G)\n",
    "# C-R\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Country','Resource'], 'Country and Resource','95',pos_95)\n",
    "statistic(G)\n",
    "# C-T\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Country','Technology'], 'Country and Technology','95',pos_95)\n",
    "statistic(G)\n",
    "# F-F\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Firm'], 'Firm','95',pos_95)\n",
    "statistic(G)\n",
    "# F-R\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Firm','Resource'], 'Firm and Resource','95',pos_95)\n",
    "statistic(G)\n",
    "# F-T\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Firm','Technology'], 'Firm and Technology','95',pos_95)\n",
    "statistic(G)\n",
    "# R-R\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Resource'], 'Resource','95',pos_95)\n",
    "statistic(G)\n",
    "# R-T\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Resource','Technology'], 'Resource and Technology','95',pos_95)\n",
    "statistic(G)\n",
    "# T-T\n",
    "G = Graph_and_Visual(df_95, 'source', 'target', ['Technology'], 'Technology','95',pos_95)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f96e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_90 = nx.from_pandas_edgelist(df_90, source='source', target='target', edge_attr='edge', create_using=nx.DiGraph())\n",
    "pos_90 = nx.spring_layout(G_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-C\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Country'], 'Country','90',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-F\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Country','Firm'], 'Country and Firm','90',pos_90)\n",
    "statistic(G)\n",
    "# C-R\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Country','Resource'], 'Country and Resource','90',pos_90)\n",
    "statistic(G)\n",
    "# C-T\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Country','Technology'], 'Country and Technology','90',pos_90)\n",
    "statistic(G)\n",
    "# F-F\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Firm'], 'Firm','90',pos_90)\n",
    "statistic(G)\n",
    "# F-R\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Firm','Resource'], 'Firm and Resource','90',pos_90)\n",
    "statistic(G)\n",
    "# F-T\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Firm','Technology'], 'Firm and Technology','90',pos_90)\n",
    "statistic(G)\n",
    "# R-R\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Resource'], 'Resource','90',pos_90)\n",
    "statistic(G)\n",
    "# R-T\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Resource','Technology'], 'Resource and Technology','90',pos_90)\n",
    "statistic(G)\n",
    "# T-T\n",
    "G = Graph_and_Visual(df_90, 'source', 'target', ['Technology'], 'Technology','90',pos_90)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13719308",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_85 = nx.from_pandas_edgelist(df_85, source='source', target='target', edge_attr='edge', create_using=nx.DiGraph())\n",
    "pos_85 = nx.spring_layout(G_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a325c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-C\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Country'], 'Country','85',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-F\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Country','Firm'], 'Country and Firm','85',pos_85)\n",
    "statistic(G)\n",
    "# C-R\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Country','Resource'], 'Country and Resource','85',pos_85)\n",
    "statistic(G)\n",
    "# C-T\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Country','Technology'], 'Country and Technology','85',pos_85)\n",
    "statistic(G)\n",
    "# F-F\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Firm'], 'Firm','85',pos_85)\n",
    "statistic(G)\n",
    "# F-R\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Firm','Resource'], 'Firm and Resource','85',pos_85)\n",
    "statistic(G)\n",
    "# F-T\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Firm','Technology'], 'Firm and Technology','85',pos_85)\n",
    "statistic(G)\n",
    "# R-R\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Resource'], 'Resource','85',pos_85)\n",
    "statistic(G)\n",
    "# R-T\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Resource','Technology'], 'Resource and Technology','85',pos_85)\n",
    "statistic(G)\n",
    "# T-T\n",
    "G = Graph_and_Visual(df_85, 'source', 'target', ['Technology'], 'Technology','85',pos_85)\n",
    "statistic(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b95dd",
   "metadata": {},
   "source": [
    "## 결과해석부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3c21c",
   "metadata": {},
   "source": [
    "결과적으로 그래프의 형태를 보면 임계값이 낮아짐에 따라 시각적으로 노드 간의 연결이 더 많아지는 것을 볼 수 있습니다. 또한, 그래프의 형태를 살펴보면 허브와 스포크 구조의 특징이 나타납니다. 이는 여러 중심 노드가 많은 다른 노드와 연결되어 있으며, 이러한 중심 노드들이 전체 네트워크를 구성하는 데 상대적으로 중요한 역할을 한다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html final_notebook.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
